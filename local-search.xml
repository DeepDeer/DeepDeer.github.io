<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>龙虾教授《人格及其转变》笔记（上）</title>
    <link href="/2021/05/07/lobster/"/>
    <url>/2021/05/07/lobster/</url>
    
    <content type="html"><![CDATA[<p>乔治*皮特森是加拿大多伦多大学教授，因其在《人生十二法则》一书中提出人与人的交往关系与龙虾间有相似之处，而被网友们亲切地称呼为“龙虾教授”。</p><p>我在2020年下半年学习了《人格及其转变》课程，收获颇丰。比如，我们应生活在阴（混沌）与阳（秩序）的交界、各种神话故事中包含的“英雄出走”的内核、人与父母间的关系、大五人格等。</p><p><strong>请把自己想象成人类灵魂工程师，我们在同时塑造自己和他人的人格，我们一同努力想要达成的是什么？</strong></p><p><strong>Be what you can be; Be yourself !</strong></p><p>另外，作为土生土长的中国人，我认为在了解西方哲学思想的同时还需要学习我国传统思想文化。理性至上无法提供安身立命之精神，如<strong>王德峰</strong>教授所说，即“感性大我之重建”。</p><h2 id="英雄-神话与萨满启蒙"><a class="markdownIt-Anchor" href="#英雄-神话与萨满启蒙"></a> 英雄、神话与萨满启蒙</h2><h3 id="一-元故事-meta-story"><a class="markdownIt-Anchor" href="#一-元故事-meta-story"></a> 一. 元故事 meta-story</h3><p>各种圣经故事、神话、小说、电影等作品其实包含了统一的精神内核，即“元故事”，它们在告诉人们如何在这个世界上生存。</p><p>人类的主旨故事之一，是走出去探索前人未知的领域，直面未知带来的恐惧，获得宝物凯旋而归。比如约翰从鲸鱼肚子里出来、哈利波特与密室、狮子王、漫威系列等，总是经过<strong>乐园、失乐园、复乐园</strong>（paradise, paradise lost, paradise regained）的过程。这个过程大多不顺利，因为life is hard，我们需要有意义的东西支撑下去，履行自己的使命。</p><div align="center">  <img src="/2021/05/07/lobster/whale.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p><strong>一旦掉入“地下城”该怎么办？</strong></p><ul><li>意识到也许自己正是一切不幸的源头；</li><li>审视自己的假设和行为方式，看清楚它们是怎样阻拦着你，搞清楚要怎么改变。你在担心什么？逃避什么？你无法发展出的是什么？你要怎么重塑自己？</li><li>摊开分析，弄懂它们，然后彻底放下；</li><li>决定做该做的事，重新塑造自己，从而以一种更正确的方式生活</li></ul><p>以“哈利波特与密室”为例具体说明：要直面未知的恐惧，虽然可能会死掉，但这是最好的选择和出路；其中有一段邓布利多的凤凰重生（transformation），学习是<strong>寻找自己正在做错的事，同时失去自己的这一部分</strong>（即使是错误的），这会很痛苦，你需要让自己很大一部人人格死去，像凤凰一样。另外，哈利波特包括很多其它故事也都体现出“拥有<strong>至善人格</strong>的人被施以残酷的惩罚”的原型。</p><div align="center">  <img src="/2021/05/07/lobster/hp.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>扩展开来，人类之间也有很多共同点。</p><ul><li><p>人类的自我意识也和自我局限有关，大猩猩视频实验表示，人类无法关注到世界上的所有事。人类其实是目光狭隘的，只是通过快速移动点亮每个区域，所能看到的世界其实很小。我们活在某种感知框架里，这个框架由我们的认知决定。正如佛家思想所讲，你的价值观决定了你的认知方向。我们的注意力很大程度上是自主的，即使你认为可以掌控自己，但并不一定（比如潜意识）；</p></li><li><p>人类天生有共同的恐惧和脆弱，比如死亡焦虑，害怕蛇、黑暗、混沌等。致力于一些有意义的事可以在一定程度上抵挡脆弱；</p></li><li><p>使用了致幻剂之后，被试们描绘着相同的超体验或濒死体验；</p></li><li><p>各地古老的关于世界的描述十分相似；</p></li><li><p>我们处于社群之中是会受到血清素系统影响的（与龙虾们相似），一个理想的人是爬到某个优势等级顶端的人。我们的环境并不是自然，而是文化，是其他人。人有本性，根深蒂固，我们只能依据本性构建社会（荣格），否则这类文化就会瓦解。</p></li></ul><h3 id="二-隐喻"><a class="markdownIt-Anchor" href="#二-隐喻"></a> 二. 隐喻</h3><h4 id="1已知隐喻"><a class="markdownIt-Anchor" href="#1已知隐喻"></a> 1.已知隐喻</h4><p>一般以父亲、太阳等形象隐喻已知，它代表了文化与社会秩序等，它具体、明确、确定，会给人一定的安全感。但同样也会以为这极权、暴君，比如斯大林、纳粹党等。</p><p>彼得潘的故事告诉我们，我们无法一直缩在已知的舒适区中。如果你不在成熟的过程（自然岁月）中使用潜能的话，潜能也会自我消耗。你会自然长大，所以最好将潜能塑造成某个东西。</p><p>让某人做某事的最好方法，是禁止他做这件事而且不告诉他原因。这里也体现出人类会打破秩序、探索未知的本能。</p><h4 id="2未知隐喻"><a class="markdownIt-Anchor" href="#2未知隐喻"></a> 2.未知隐喻</h4><p>通常以黑暗之地、禁果、潘多拉魔盒、蛇等形式展现，<strong>表现为女性或阴性</strong>。它代表了潜意识、带着酒神力量的本我（弗洛伊德）、潜伏的怪兽、万物的来源与归宿。还包括伟大母亲、女王、基体（the matrix）、容器、深处、山谷、月亮等符号，是<strong>超越性的</strong>。</p><p>以迦利神像为例，它是毁灭或恐惧的具象符号。我们需要给“社会/自然/母亲”她想要的东西，与未来做交易，做出正确的牺牲，以今天的愉悦为筹码，交换明日的优势。</p><div align="center">  <img src="/2021/05/07/lobster/jl.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>我们需要主动探索未知，如果知道外面正酝酿着极大的危险，主动前往比不小心掉入要好很多。因为主动挑战威胁，身体会被激活，否则你会进入猎物模式，所以<strong>最好睁大双眼，警惕萌生的威胁，及早行动</strong>。而每次你学到人生中的一课，学习到的瞬间，你的世界都地动山摇。</p><h4 id="3-二者结合"><a class="markdownIt-Anchor" href="#3-二者结合"></a> 3. 二者结合</h4><p>我们存在其中的世界充满了动机和感情，充斥着恐怖、痛苦、喜悦与挫败，以及其他人，包含一切已知和未知，你希望在可预料当中有一些不可预料。<strong>未知中蕴含着宇宙的混沌与未来的潜能，我参与其中，将可能转变为现实</strong>。</p><p>借助<strong>太极图</strong>，我们要理解到，已知的部分可能在一瞬间变为未知，一生中可能经历无数次这样的转折。我们注定会经历混沌（chaos），也许会在某次混沌中一蹶不振，但<strong>一定要坚信混沌中蕴含着向秩序的转变</strong>。</p><div align="center">  <img src="/2021/05/07/lobster/taiji.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>你要尽力活在<strong>秩序和混沌的边界（meta-place）</strong>，主动探索未知，强化秩序。现实中，有些人可以容忍大范围的混沌，比如自由派，但有些人更喜欢现存结构的稳定性，如保守派，环境瞬息万变，没有永远的对错，这就需要<strong>双方的沟通交流</strong>。当你处于混沌与秩序的最佳边界时，你的大脑会告诉你，它会制造出一种全情投入、富含意义的感受；<strong>你足够稳定，也有充足的兴趣，达到这种完美的平衡</strong>。</p><p><strong>要如何在混沌与秩序之间斡旋呢？</strong></p><p>1）Heaven is in the unknown。有些人带着幻想面对未知，未来，梦境，创造力从这里萌发，具有很高的经验开放性。但很多患者也这样，他们具有强烈的自我批判，他们聪明，具有创造力，通过梦境与艺术探索未知。许多19、20世纪被普遍认为很伟大的人（如尼采、达尔文、陀思妥耶夫斯基、托尔斯泰、弗洛伊德、荣格等）都经历过这种创造类疾病，那是长期、深刻的、心理上的不安与不确定。<strong>通往更高智慧的道路便是要经过恐怖的地域之门。</strong></p><p>2）文化、艺术、幻想、音乐、戏剧、故事，会起到缓冲作用。我们被文化和幻想包围着，也被保卫着，我们应当尊重秩序。对历史了解的太少才会更偏爱混沌，而不是秩序。应心存感激，感谢明君，但同时也要足够聪明，知道他的另一面是邪恶暴君，这才是<strong>对世界的完整认知</strong>。必须对人性有了解，并不是作恶者在一边而受害者在另一边，<strong>人性之邪恶，人性之崇高都是你的中心元素</strong>。</p><p>3）在发展初期（比如事不过三，你要记得人们会狡辩，你要坚持住自己的观点），你最好把自己从压迫中释放出来，<strong>把真实想法讲出来</strong>（不一定正确），你要与敌人或者不同观点的人<strong>真诚交流，探索未知</strong>。</p><p>4）<strong>怨恨是一种有毒的情绪</strong>。如果你怨恨某事这意味着你应该有所成长，承担责任，不再四处抱怨，哭哭啼啼；或者这意味着正有人压迫你、欺辱你，你有东西要说出来或做出来，但你没有（因为那可能在短期内让你惹上危险）。日复一日，虽然逃避会在短期内可以保护你，但这会把你挤压变形，随着怨恨积累，疯狂滋长，成为复仇的欲望。你会成为压迫者，你在背后说坏话，他们让你做什么你会很马虎或者很勉强地去做。如同陀思妥耶夫斯基在《地下室手记》中描写的主人公一样，住在<strong>心理地下室</strong>里，想着这个世界多么邪恶，别人如何针对你、拒绝你，那么你会觉得活着本身就是有毒的，你会想要走出去做尽极恶之事。</p><p>在此也可以引出<strong>科学技术与人文艺术的关系</strong>：</p><ul><li><p>科学用来描述“世界是什么”，而对于【真实】的定义，即使是科学也只是一种工具，是一种人类用来探索世界的工具；</p></li><li><p>神话、戏剧、梦境、潜意识以及其他美学的、艺术的、幻想方面的文学作品是在告诉我们事情应该是怎样的，告诉我们如何行动，传授为人处事之道。经过多少个世纪，提炼出<strong>至善、至恶</strong>让我们理解，它们就<strong>像圣经故事中的两兄弟，也在我们体内时常斗争</strong>；</p></li><li><p>人文类学科像魔法，让人意识到，我不只是父母的孩子，还是自然的孩子，人类文化的孩子；由于人生艰难，你必须做一些真的有意义的事，你肩负着道德责任，做正确的事；<strong>真正意义上的好人，单纯遵守纪律是不够的，需要理解恶，并经受住恶</strong>；我们必须要理解自己身上的恶和阴暗面，这些可以通过读史来实现，比如《古拉格群岛》、《南京大屠杀》、《奥斯维辛》等，读的时候想象自己是集中营的守卫、是卡波而不是所谓解放人民的大英雄。<strong>只有认识到自己是个魔鬼之后，你才能对自己有足够的尊重，会对自己的行为有所控制</strong>。</p></li></ul><h3 id="三-狮子王中的隐喻"><a class="markdownIt-Anchor" href="#三-狮子王中的隐喻"></a> 三. 《狮子王》中的隐喻</h3><h4 id="1-性欲与侵犯欲"><a class="markdownIt-Anchor" href="#1-性欲与侵犯欲"></a> 1. 性欲与侵犯欲</h4><p>与饥饿和口渴不同，性欲与侵犯欲这两种驱动力通常被人类社会排除在外，需要个体有意识地自我整合。</p><p><strong>整合好自身的攻击性</strong>是一件非常重要的成长课题。</p><p>早期的辛巴像一只瞪大了眼睛的无辜的鹿，它软弱而幼稚，具有天真的脸庞。它接受一切信息，但没有反应和产出，牺牲了自己而成为环境的出气筒。</p><p>牺牲自己迎合别人，永远不制造冲突，这并不会让你更受欢迎或成为一个好人。因为没有能力伤害别人不代表你就是道德的，<strong>你需要长出獠牙，这样会保证你更少用到它们</strong>。背后的愤恨或怨恨其实变相地展示了你的不成熟。另外，如果无法拒绝加入群众病态的（某些情况下）行动中，恶意的企图会把你的邪恶（你一直逃避、摒弃、压抑的邪恶）勾引出来。</p><p>荣格认为，<strong>应该将恐怖的一面整合进自身而不是一直摒弃它</strong>，鄙视它，妄图彻底清除它，因为你做不到。而且即使做到了，你只能得到一个软弱的自己。圣人不是纯白的，是阴阳统一的。整合好你的阴影和侵犯性，你的面庞会变得更加坚定，如同成年辛巴。</p><p>通过<strong>查验是否还存在比较持久的（18个月以上）活跃的负面记忆</strong>，可以判断自己的感知价值结构中是否还存在漏洞。具体可以通过<strong>自我写作</strong>的形式来做。在你的自传中，仔细考虑过去发生在你身上的那些不好的事，弄清楚到底发生了什么，如何避免未来再次发生，梳理并表述自己的负面情绪。</p><h4 id="2-自性"><a class="markdownIt-Anchor" href="#2-自性"></a> 2. 自性</h4><p>《狮子王》中的狒狒象征了<strong>自性，也即潜能，未来可能成为的你</strong>，如果你以获取最大化信息的方式与世界交互，那个你能成为的你。辛巴受到召唤，在地下深水泉（潜意识）中看到自己未来的模样。</p><p>你会受到睿智的召唤，重新发现孩童时期与太阳相连的部分，并且相信它。当然这会需要一个长期的过程：</p><ul><li>第一阶段，开始意识到自己的无用，伴随着对自己的辩解。生气而毫无力量，爱发牢骚，满怀愤恨，拥有可悲而欠揍的面容。刚刚唤醒自我意识，伴随着非常痛苦的自我反思；</li><li>第二阶段，意识到自身价值结构的不足，希望找到“父亲”，也即权威。但“父亲”已经死了，我该如何转化自己，我该如何变好？但是没有人能够给出答案。</li><li>第三阶段，真诚希望改变。一旦你意识到自己错了，并且打开了潜在改变的大门，你的一部分自我会做出回应。你的深层自我还保存着潜在的可能性，它会改变你对事情的看法，改变你的回应方式。</li></ul><div align="center">  <img src="/2021/05/07/lobster/feifei.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>如果你真的想让情况好转，让自己振作，并且你承认现阶段的不足，你考虑当前的问题，想弄明白下一步究竟应该怎么办，<strong>那么你会找到内心中有某种东西在你发展的过程中，指引着你，那就是自性，是更高阶的自我，是转变中的不变</strong>。<strong>心理治疗是可以被高境界的道德努力取代的</strong>，一定要在内心深处，深层潜意识中，看到自己可能成为的人。</p><p><strong>如何解决心理问题？</strong></p><p>1）陷入混乱，如何改善当下？即<strong>自我认知模型的重建</strong>。你如今的生活不如意，那么你期待一年（或三五年）以后的生活（或者只是某些方面）是什么样的？确定下来，作为目标，筹划一下（诚实地），考虑会遇到哪些阻碍（实际的，心理的）。制定策略，试着朝向理想状态迈步。<strong>一定要对自己真诚，协调自我，记住，自己和自己是一伙的</strong>。</p><p>2）<strong>在多个维度照顾好自己</strong>，避免陷入混沌。每个人都需要<strong>日常惯例（routine）<strong>比如固定的作息，在物理上照顾好自己；你需要</strong>一份事业（career）</strong>，生产你认为有意义的东西，制定职业计划；需要家庭、朋友、<strong>亲密关系</strong>；<strong>学会如何谈判，学会表达需求</strong>；学会<strong>讲真话</strong>；学会<strong>倾听</strong>；<strong>运动是最好的阻止智力下滑的方法</strong>。</p><p>3）其实，很多去找心理治疗的来访并不是心理问题，而是生活真的出了问题亟待解决。心理治疗是一段真诚的关系。而本质上，<strong>自愿接触你不愿面对的东西是有疗愈性的</strong>，注意这必须是自愿的面对的。</p><p>4）<strong>存在式焦虑</strong>；正常人在安全的地方是冷静的，是把自己整合的很好的。但是现实中常见的地方并不安全，所以<strong>感到焦虑、抑郁是正常的，并不需要理由</strong>，我们需要关注的是你如何达到安全协调。我们可以通过待在自己的领地，维持稳定与安全感。但我们<strong>必须在意外界</strong>。你确实需要<strong>整合好自己的思想、精神，将每一种构成要素转换成功能性的存在</strong>，并且让这个<strong>功能性存在与外界图景具有一致性，即融入社会</strong>。内在、外在整合兼备才能真正调整好你的情绪。</p><h4 id="3-其它隐喻元素"><a class="markdownIt-Anchor" href="#3-其它隐喻元素"></a> 3. 其它隐喻元素</h4><ul><li><p><strong>阳光照耀</strong>；如果真正想在某处感到舒适，想主导并融入这个地方，你需要<strong>花心思让每个角落都被（你的）光照耀到</strong>，主动探索而非恐惧地缩在一角。这里的“某处”包括自身、亲密关系、陌生环境等；</p></li><li><p><strong>将特权与能力混为一谈</strong>；在大学里、在和平国度里应当心存感激，而不是认为这一切理所当然；</p></li><li><p>要<strong>允许自己是傻瓜</strong>；如果不允许自己时这种状态，而且一味完美主义，你会觉得自己是个“冒名顶替者”。尝试新事物的时候，你确实会像个傻瓜，但如果因此不去尝试，你就是个更糟糕的傻瓜。<strong>允许自己犯错才能进步</strong>。</p></li><li><p><strong>刀疤的帝国</strong>；二战时期的德国，极具条理性和极权主义，是过度的文明。人们信奉一套理论体系，甚至不惜欺骗自己，整个系统中充满了谎言（比如德国、苏联、1984）。</p></li><li><p><strong>辛巴逃离到了一篇沙漠</strong>；当你离开一个国度（不管那里是怎样的残暴），你都会<strong>陷入混乱中</strong>。如戒烟戒酒的时候，你抛弃了旧的价值体系（也许是因为它不够好），但<strong>并不是马上迎来提升</strong>。</p></li><li><p><strong>国王在国外培养</strong>；哈利波特、亚瑟王、狮子王等都会包含这类元素。你会在一定程度上疏离于你的文化，可能是因为原本的文化滞后腐朽，也可能由于你并没有发挥出你的潜力践行价值，在现有的评价体系中，你并不成功。比如在电影中辛巴被陷害了，但它并不是完全无辜的。</p></li><li><p>睿智的部分<strong>没有对邪恶的部分有足够的戒心</strong>；辛巴的父亲被弟弟打败的场景表明，明君有一个邪恶的兄弟，而没有足够留意，它选择逃避或视而不见。人生的洪水也是如此，这些灾难有上天的随机因素，也有一些部分是你会责备自己过去的，<strong>因为当时的你短视或选择视而不见</strong>。</p></li></ul><h2 id="皮亚杰与构成主义"><a class="markdownIt-Anchor" href="#皮亚杰与构成主义"></a> 皮亚杰与构成主义</h2><p>皮亚杰，知识巨匠，一位杰出的发展心理学家，知识渊博。</p><p>经典经典科学观会假设知识本身是事实而不是过程，但当代科学更多认识到<strong>知识是过程而不是静态事实</strong>。</p><h4 id="1-同化与顺应"><a class="markdownIt-Anchor" href="#1-同化与顺应"></a> 1. 同化与顺应</h4><p>皮亚杰理论中重点有<strong>同化与顺应</strong>的概念。我们需要将知识视为工具，而不是客观独立的现实，<strong>世界不是所有等待发现的客观事实的合集</strong>，世界是更为复杂的。不随时间改变的才可以称为事实，比如人们产出事实的方式、知识（世界观）的获取与转换过程等，而不是事实本身。<strong>终极现实，是经历这些阶段的过程</strong>。</p><p>皮亚杰理论与萨满启蒙有共通之处，也即起初存在有序状态，之后意外发生，陷入混沌，原有体系瓦解，最终重组新生，形成一个更加完整的状态。根据这个理论可以解释<strong>孩子在商场中与家长分开会恐慌</strong>的现象。因为，原本家长将外界的复杂性与孩子隔绝，而家长离开，混沌和不确定性向孩子涌来，在新世界中学习速度过快，会带来巨大痛苦。而如果学习速度适中的话，你会从可能性中获益，而不是被不确定因素淹没。</p><div align="center">  <img src="/2021/05/07/lobster/pyj.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><h4 id="2-程序记忆与表现记忆"><a class="markdownIt-Anchor" href="#2-程序记忆与表现记忆"></a> 2. 程序记忆与表现记忆</h4><p>皮亚杰理论整体在研究人们如何表现世界并学习。他在<strong>试图填补科学与价值之间的裂缝</strong>。</p><blockquote><p>大卫休谟提出，你无法从“是什么”中推理出“应该是什么”，你知道这件事不代表你可以毫无差错地处理这件事；</p><p>哈里斯相信价值可以嵌于科学中；</p></blockquote><p>提出<strong>程序记忆与表现记忆</strong>，人类在婴儿时期还没有表现记忆，而社会结构隐形地嵌于程序记忆的系统结构中，你所出生于的这个社会结构会被编入你的行为，而你并不知道规则。</p><h4 id="3人生游戏"><a class="markdownIt-Anchor" href="#3人生游戏"></a> 3.人生游戏</h4><p>人类间的社会互动来源于一个有限制的空间，我们永远都在玩游戏。而通过研究哺乳动物的脑回路研究，我们发现公平游戏的感觉是生物天生的。</p><p>小孩子的游戏嵌在大人游戏中（所以，在2-4岁时，要让孩子学会如何和其他人，尤其是孩子，玩耍）；之后，人们会随着发展，更加有意识地玩游戏，并开始在行为上表现游戏，开始学习显示游戏规则；最终，在道德发展的最高阶段，人们意识到自己不仅仅是游戏的玩家，还是规则的制定者，也可以发明游戏。</p><p>如何毁掉一个孩子：</p><ul><li>在他做好事或尝试做好事时，惩罚他或忽略他；忽略比惩罚更可怕，因为惩罚他的时候至少你的注意力还在他身上</li><li>玩游戏时不要越过让孩子筋疲力尽的那个点[ 其实“培养”自己也是同理]</li></ul><p>在人生游戏中，克服痛苦的方式之一是创造意义：</p><ul><li>在技术层面，从挫折中学习到什么，真的会改变的微观生物构造，可以类比于冲浪；</li><li>我们在死亡（稳定或混沌都是死亡状态，你生活在阴阳交界）和生存之间保持着精妙的平衡。学习的过程也是这样，你需要杀死部分已知（生物结构），才能学到未知，虽然过程会带来痛苦；</li><li>在这个过程中，我们学习到游戏的“元玩法”</li><li>意义的重要性之一就在于，它会改变你看待世界、回应世界的方式</li></ul><h4 id="4-高级抽象认知"><a class="markdownIt-Anchor" href="#4-高级抽象认知"></a> 4. 高级抽象认知</h4><p>如图所示，要从小事做起、从多方面做起，从底层的行为感知序列开始，从实际的微观行为到高层抽象。</p><div align="center">  <img src="/2021/05/07/lobster/pyj_high.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><h2 id="弗洛伊德与潜意识"><a class="markdownIt-Anchor" href="#弗洛伊德与潜意识"></a> 弗洛伊德与潜意识</h2><p>潜意识的概念表明了，人们可以在无法解释的时候做出行动，比如孩子无法描述游戏规则，但他可以玩这个游戏。</p><p>人类深层的潜意识和其他哺乳动物、爬行动物等也有很多共通之处。</p><p>我们并不是完全掌握这些“后台运行的程序”，人格越没有整合好，就越容易失去对这类意识的掌控。</p><p>弗洛伊德提出我们的防御机制包括：压抑、否认、反向、转移、认同、合理化、理智化、升华、投射。</p><p><strong>JP教授对弗洛伊德的理论有几处不认同的地方</strong>：</p><p>首先是<strong>学习成长模型</strong>，教授更倾向认为在健康状态下是皮亚杰模型。病态发展中，你并没有把攻击冲动和性冲动整合到你的人格中去，而是让超我直接压抑了它们，没能称为活跃的一部分。你不去展示它们，所以成为了所谓的“好人”。你对自己非常专制，成为一个超严厉超我的受害者。你把父母、祖父母及内化的“父母”集合成了一个超级严厉的法官，一直在注视你，评判你。</p><p>其次是有关<strong>记忆与遗忘</strong>，弗洛伊德理论认为人类有连续录影带式的记忆，某些由于太过痛苦或其他原因而被压抑，导致遗忘。而教授则认为，人的记忆并不是这样有条理的，过去的信息是杂乱的，待清除的，所以人们没有注意到，而不是有意识（自我意识或潜意识）地去压抑。</p><p>最终有关<strong>梦境解析</strong>，弗洛伊德认为梦展现了被压抑的东西，梦尽力地像掩盖所要表达的内容。而教授更倾向于荣格的观点，即认为梦在尽可能清晰地表达，它不属于语义记忆系统，而更像是探索未知的触角，所以使用象征符号，而并不是想要向做梦者隐藏不愉悦的内容，而是用它唯一可以使用的语言体系。</p><h2 id="卡尔罗杰斯与现象学"><a class="markdownIt-Anchor" href="#卡尔罗杰斯与现象学"></a> 卡尔罗杰斯与现象学</h2><p>科学将主观剥离世界，它将任何主观视为偏见或错误，希望尽可能摆脱。但问题在于每个人都是一种主观。</p><p>在科学发展的当今，人成为了冷漠的客观事实中一种孤立的存在，容易导致存在的虚无。</p><p>尼采提出，上帝已死，带入虚无主义。海德格尔则希望从头构建体系，重新思考现实，解决这一问题。将现实看作我们经历/体验到的一切，抛弃主观和客观的划分，但这类研究并没有实质性进展。</p><p>荣格后半生转向现象学，提出如下三个必要层面，也可视为扩展了皮亚杰的道德模型：</p><ul><li><strong>将思想与情绪结合 [男女结合]</strong>；可以通过<strong>未来自我写作训练，让焦虑成为你的助手</strong>；想象如果你不去处理某个你目前尽量逃避的问题，会发生什么；也可以<strong>进行坚定性训练</strong>，想象你具体想要什么，不要因为当下得不到就不去想象，想象你不去这个做的代价，整合攻击性（如果你没有力量，你就无法谈判；而愤怒是一种有毒的情绪，会加剧身体负担，长期下来会影响健康）。</li><li>将思想与情绪的结合体，<strong>再与身体结合 [行动起来]</strong>；一定要知行合一，保证认知与行动的连贯性。从皮亚杰高层抽象模型的最底层做起，从多个方面的小事做起。没有肩负起存在的重任会导致神经质的愧疚和恐惧，所以从清扫你的房间做起。</li><li>最终消除<strong>自己与世界的区分[天人合一]</strong>；世界会随着你的目标而改变。</li></ul><p>人与人之间的沟通要做到<strong>真正的倾听</strong>，我在这里是为了一起达到更好的你的一部分；我希望了解你的观点，而不是输出我的然后只希望你赞同。人们很难找到真正倾听自己的人，真正的倾听是我们给别人的非常宝贵的礼物。</p><h2 id="存在主义"><a class="markdownIt-Anchor" href="#存在主义"></a> 存在主义</h2><p>存在主义相关人物包括尼采、陀思妥耶夫斯基、克尔凯郭尔等。</p><p>存在主义产生的背景为“上帝已死”。现代生活中不可避免地承担着很多焦虑，这可能是人们对科技的提升与意识觉醒付出的代价。人类暴露于一种无意义且痛苦的存在，如果你的价值系统瓦解了，那么你就会没有目标，没有积极情绪（虚无），这种情况下，人们也许会飞奔至极权主义的怀抱，牺牲理智与智力换取秩序与确定性。</p><blockquote><p>虚无主义(物质世界观中隐含)，陷入其中，你会一无所有，无法应对生活中的挑战或苦难</p><p>理性极权主义(激进意识形态)，过度信奉，你变成了一堆理论抽象中的提线木偶，走向毁灭</p></blockquote><p>存在主义<strong>悲观又极度乐观</strong>，它承认人是脆弱的、有限的，但一旦你<strong>直面恐惧</strong>，又会激发出我们无法估量的力量。这力量就是自性或称为内在潜能。不要低估自己的内在潜能，对于你自己，你还有很多不知道的事，<strong>去新的环境</strong>，会改变你的微观生物结构。如果你<strong>恰当地把自己推进世界</strong>，你会开启新的能力，并在探索过程中获得信息。<strong>自愿地以更多方式挑战自己</strong>，可以促进这种转变。不要针对死亡焦虑构建虚假的抵抗，而是积极学会如何应对这个世界。</p><p>存在主义包括三个基本理念：</p><ul><li><strong>行动比语言更有力</strong>。知行合一，身心协调，行胜于言。如果想了解别人或自己的信念，最好去看做了什么，而非说了什么。</li><li><strong>麻烦和痛苦是人类经历的固有元素</strong>。人难以摆脱苦难，即便这个人本身没什么问题。很多人生中的苦难没有原因，没有由来，并不只是因为童年/经历的锅，并不是你这个人哪里出了问题，也不要认为这些问题仅限于你自己或这一小部分人。<strong>人的生活本身就有问题，你要做的是面对和解决</strong>。合理程度的痛苦是正常的，类似圣经故事中所说，人们从美好的天堂掉落，总是处于残缺的状态，认为自己哪里出了问题，需要被纠正。</li><li><strong>存在主义包含一些浪漫主义</strong>。反对理性与智力的至高地位，理性并不是指引人们的根本原则。<strong>理性是需要与其它主观因素相辅相成的</strong>，而非如20世纪以来所呈现出的一家独大。科学是一种需要被合理利用的工具，而不是描述存在的方式。<strong>关注个体</strong>，个体才是心理分析的恰当层面，将每个患者视为独特的个体，有独特的问题，而不仅是使用精神分析那种成体系的框架。</li></ul><p>由此JP引到恐旷症的逐步治疗：</p><ul><li>四处嗅探的小白鼠；</li><li>早期阶段的回归与固化，可以看到患者退行回小孩子阶段，认为自己没有能力；</li><li>对于权威的态度，自己总处于次级、奴隶的状态。家庭成员对心理治疗的抵制（包括沉默的抵制），你真的希望伴侣变得更好吗，那意味着她会更坚定，而难以掌控；</li><li>一些心理分析师不赞成类似行为主义的治疗手段，认为对于电梯的恐惧并不针对电梯，而是象征着其它东西，如果治好了电梯，这恐惧还会从其它地方冒出来；JP认为不然，治好电梯，患者会自行挑战出租车，这是在教患者学会勇敢，而<strong>勇敢会扩大化</strong>；</li><li><strong>表现得弱小无用会成为一种武器</strong>；</li><li>[ 题外话 ] 联想到刚开始做科研时，自己对自己的批判，导致了自己的罢工，所以很喜欢DDL，因为那时候你必须专注，没心思再批判什么。发论文也是如此，直面恐惧，真的做到之后，就觉得没有那么困难。</li></ul><p>另外，人并不是理性的，这也是存在主义对乌托邦主义的批判。陀思妥耶夫斯基的作品也表明，<strong>人具有自由意志</strong>，人所做的事情，是为了时时刻刻证明他是人，不是钢琴键，不是可以计算、推理的事物，就算这可能会损害他自己的皮肤，就算要以自相残杀为代价。尼采也表示，也许感到不满足其实也是一种满足，也许你必须受限，这是你想要的，也许这些才会给你生活的意义。</p><h2 id="现象学"><a class="markdownIt-Anchor" href="#现象学"></a> 现象学</h2><p>相关人物，马丁海德格尔（哲学家）。</p><p>现象学认为人们生活在一个自我定义的感知框架之中。</p><p>JP认为<strong>临床心理学是有价值导向的，并不是纯粹的科学学科</strong>。</p><p>客体是非常复杂的，比如波粒二象性。即便以科学方法定义某个客体，也并非真正在定义，你只能说，这是个多维度物体，如果我以这样的方式接触它，也就是采取这样的过程或方法，它就会显现出那样的特质，但还有很多种其它的可能。</p><p>我们需要限制自己的感知范围，达到一个可以处理的范围，<strong>在这个收窄的现实中</strong>生活。这就意味着，你<strong>需要有一个目标（视野的焦点）</strong>，这代表了你的价值体系，以及世界将如何在你面前展开。目标启示着你的世界，组织着你的情感，并让你准备好做行动。目标包含了很多，比如内驱力、目的、动机等等，它是你人格的一部分。但同时也要了解到，我们正<strong>处于收窄的视野范围内</strong>。</p><p>人们为什么对某些事物好奇？要去追寻某些意义？</p><p><strong>宾斯旺格</strong>认为我们最先感知到的，不是味道、声调或触感印象，也不是物体或客体，而是意义；<strong>美丽是主观的</strong>，由于你的感知‘滤镜’产生的。<strong>鲍斯</strong>则认为，<strong>美丽固有于客体本身</strong>，显现了自身，向外发光，你追求那些向外发光的东西（比如《哈利波特》中的金色飞贼隐喻）。JP认为是这两种观点的结合，你无法完全掌控你的好奇心，但它也并不是完全随机的，因为没有主体能脱离结构去感知。同时，被感知的那个客体也带着它自己的潜能向外发光。</p><p>我们面前不是一个固定的客观世界，而是<strong>一个充满潜能的世界</strong>。你能和这种潜能的任何方面进行互动，在互动中，你将一些以前不存在的东西拉进了现实。这些潜能并不是无限的，因为你本身就是有限的，但不管你有什么目标打算，它都足够了，因为<strong>它永远比你所需的潜能更多</strong>。</p><p>你探索着某个新的东西，你从这次探索中生成了什么呢？首先，会产生一个新的你，物质的也是精神的，因为探索时你在学习，这个过程会改变你。而同样从你的探索中也生成了一个世界。</p><p>开放的想象力：</p><ul><li>梦境处于思考的前沿，走在你的前面</li><li><strong>艺术超越了语言能表达的东西，否则它就不是艺术，而是宣传</strong></li><li>发源于未知世界，并提供给你一些信息</li></ul><h2 id="索尔仁尼琴"><a class="markdownIt-Anchor" href="#索尔仁尼琴"></a> 索尔仁尼琴</h2><p>他是一位二战上了前线的苏联士兵，是存在主义作家，相关人物还包括撰写《活出生命的意义》的维克克多弗兰克、哈维尔等。</p><p>其著作《伊凡德尼索维奇的一天》是苏联时期第一本公开描述集中营的书，他还凭借《古拉格群岛》获得诺贝尔奖。</p><h4 id="1古拉格群岛"><a class="markdownIt-Anchor" href="#1古拉格群岛"></a> 1.《古拉格群岛》</h4><p>《古拉格群岛》于1973年出版，之后Samizdat地下传阅，1989年再次公开出版。古拉格，即纠正性劳动营主管部门，因为当时认为人们会犯罪是由于沙皇俄国体制的压迫，所以让一些罪犯（强奸犯、抢劫犯、小偷等）管理集中营，而集中营中关押着的是与“特权”有关的人士，带着基于阶级和种族的罪名。<strong>这本书书例证劳动人民的乌托邦可以实现的想法的幻灭</strong>。《昨日的世界》中也描绘了苏联邀请西方知识分子去参观的情景。1930年开始，《1984》、《动物农场》也都开始揭露一些现象。</p><p>这本书包括了压迫性苏联体制的产生，斯大林统治下全面展开和共产系统。当时人们把苏联解体归咎于斯大林的个人崇拜扭曲了最初准确的主义，认为如果列宁活得就一些，乌托邦就可以实现了。索尔仁尼琴从根本上反驳的了个观点。他梳理了主义与列宁定制的某些法律之间的问题，比如清除异己、个人崇拜、专制权力、无处不在的监控，KGB等。当时的情况下，即使是坚定的党员也无法幸免。毫无缘由地，即便他们没有对D犯下任何错误。<strong>人类的心无法承受被心爱的斧头所伤，却还要证明那把斧头是智慧的。</strong></p><p><strong>帕累托分布</strong>掌管了金钱分布、公司关系等情况，支配了几乎所有创造性生产的领域。这是<strong>一个根本性原则</strong>，而目前没人知道该怎么有效且持续地把资源从几乎掌握一切的人手里撒到下层几乎什么都没有的人那里（虽然顶端的人会变化）。<strong>即使是通过抛硬币决胜负</strong>，财富也会最终集中到少数人手里，时间足够长后，甚至会集中到一个人手里。</p><p>社会的病态和个体的病态间根本联系在于，<strong>个体倾向于欺骗自己</strong>，从而无法以真实真诚的方式行事。最终个体变为虚无主义者，或由于品格被逐渐削弱，不真诚称为了生活的一部分，转向意识形态和极权主义的解决方案，放弃恰当生活，放弃个体责任。而无法<strong>真诚真实地行事会导致走向虚无主义或极权主义</strong>。</p><p>如何分辨一个被意识形态控制住的人？其实一旦你掌握了他们意识形态底层结构的五六个公理，你<strong>甚至可以预测</strong>他会说什么（比如休蒙格斯采访视频的例子）。人们选择被意识形态控制，因为这减少了他们思考的负担，也让他们相信自己完全掌握了世界上所有的知识，而且相信自己不需要思考就可以分辨出谁在善的一边。</p><h4 id="2圣经故事"><a class="markdownIt-Anchor" href="#2圣经故事"></a> 2.圣经故事</h4><p>《巴别塔》，极权主义大厦或乌托邦，越建越好，要容纳更多的因素，更多不同的人，最终会成为一盘散沙。洪水一般隐喻是来自神的惩罚，包括事物瓦解的趋势、人类对罪的趋向、熵增原理等。</p><p>《失乐园》，理性思维产生的政治的、意识形态的理性建构与引导着人类组织的超然神话间有一种紧张的关系。<strong>上帝的最高天使撒旦就是这种经理性思维的拟人化象征</strong>。这类思维倾向于产生极权系统，并爱上极权系统，系统之外的东西都不允许存在，而最终将自己投入地狱。</p><p>艺术家、诗人、哲学家先后了解到未知。</p><h4 id="3箴言"><a class="markdownIt-Anchor" href="#3箴言"></a> 3.箴言</h4><p><strong>苦难是存在的一部分</strong>，这是存在主义的基础观点之一，大部分伟大宗教体系也拥抱这一观点。</p><p><strong>苦难的三大来源</strong>如下：</p><ul><li>自然界的人性，人生而有限</li><li>社会结构的武断审判（无论你处于哪个社会中都会有不同程度的专断）但你需要和他人生活在一起，你要找到你适合的地方</li><li>我们自身也对某些不必要的痛苦负有责任。你本可以做某些事，让你的生活以及别人的生活得到改善；你有未承担起来的责任，而你的良知明明知道。<strong>人心真的有良知，听一听来自良知的劝告（conscience）</strong>。</li></ul><p>正确度过人生的方式是：<strong>真诚地存在，拒绝参与说谎和欺骗，让你的语言/行动尽可能真实，为你的生活（也许还有其它人的生活）负起责任</strong>。这样做是有意义的、负责任的、高尚的。这样做有助于减轻痛苦，否则痛苦会带来虚无主义，或让人逃入极权主义的怀抱。</p><p>你需要一些东西来抵抗你自己的脆弱性，你可以采纳<strong>别人给你制定好的</strong>对现实的综合描述，有种描述把世界简洁地分为天真的无辜受害者和犯了罪的苦难制造者，而且他们都和你无关，这不是评估世界的合理方法，<strong>苦难是与生俱来的。即使你精神/心理没有出现问题</strong>，事情也向糟糕的方向发展了，但我们仍然有前进的方向，选择活得高尚一些，让你可以忍受你自己，甚至可能尊敬你自己，因为你能直面那可怕的脆弱和痛苦。</p><p>[ 避免欺骗、承担责任、试图改善 ]</p><p>什么是真实？ 你看到什么，听到什么，做什么，和谁在一起。有一种从心灵深处满溢出来的不懊悔，也不羞耻的和平与喜悦。爱你所爱，行你所行，听从你心，无问西东。</p><p><strong>成为你自己</strong></p><p>你很清楚你没有完全实现自己的潜能，你造成了部分的苦难，也许你可以换一种看待世界的方式，换一种行动的方式，不要再浪费眼前的机会！<strong>人心真的有良知</strong>，我们并不知道那是什么，但你要听从良心的劝告。以你真实自我建立起来的亲密关系会更强健、更愉快，一个完整的你，通过和伴侣协商，活出真实的生活，这是也是养育孩子的基础。</p><p>实际上人们不仅不做他们应该做的、让情况变好的事，他们还积极地把事情搞糟，因为他们**（我目前是他们的一员）**怀恨在心、怨气冲天、狂傲自满、尔虞我诈甚至杀人如麻，所有这些病态都纠缠在一起。</p><p>你振作起来，多大程度上活出真实的自我（不要误解为放浪形骸，而是存在主义中恰当生活的三个步骤）不仅关乎你一个人的命运，而是关乎所有与你产生联系的人的命运：</p><ul><li>不要低估自己的影响力，它如同涟漪</li><li>你的所作所为非常重要，大多数事情都是有意义的，这同时意味着你需要承当由此而来的责任</li><li>如果你过着病态的生活，你病态化了整个社会，如果有足够多的人这样做，社会会变成什么样子？</li><li>虚无主义者很痛苦，<strong>但他们的优势在于不用承担责任</strong>，是你自己你放手让你的价值体系崩溃了，于是你无需承担责任，代价是时常痛苦，但你可以一直哭哭啼啼，人们会为你感到难过</li></ul><p><strong>人们需要在生命中的某阶段时间全身心参与某项游戏</strong>，从某时刻起，你得在某一方面有所成就，即使你牺牲掉了其他所有的可能，但你必须做出选择（某种职业、某种价值观），否则会徒增年岁而依旧混乱。</p><h2 id="相关资源"><a class="markdownIt-Anchor" href="#相关资源"></a> 相关资源</h2><p>《人生十二法则》、《人格及其转变》课程 [B站，<a href="https://www.bilibili.com/video/BV1AW411M7vL" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1AW411M7vL</a>]</p><p>《哈佛幸福课》</p><p>王德峰教授讲座系列 [B站可搜索到，但被屏蔽了好多，喜马拉雅有音频资源]，包括王教授讲的《坛经》、《传习录》、《资本论》等</p><p>欧丽娟老师讲《红楼梦》[喜马拉雅有完整音频，B站有一些精彩节选]</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人文社科</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>哈佛《正义课》笔记</title>
    <link href="/2021/05/07/fair-havard/"/>
    <url>/2021/05/07/fair-havard/</url>
    
    <content type="html"><![CDATA[<h2 id="背景材料"><a class="markdownIt-Anchor" href="#背景材料"></a> 背景材料</h2><p>迈克尔.桑德尔教授是哈佛最受欢迎的教授之一，他说过“我的目标不是试图用什么理念去说服学生，而是把他们训练成有头脑的公民”。他在29岁时就成为社群主义向自由主义发起挑战的标志性人物。刘擎教授在《刘擎西方现代思想讲义》中有专门章节介绍。</p><p>课程视频：<a href="https://www.bilibili.com/video/BV1ct4y167fM%EF%BC%88B%E7%AB%99%EF%BC%89%E5%8F%A6%E5%A4%96%E5%8C%85%E6%8B%AC%E8%AE%B2%E5%BA%A7%E3%80%8A%E9%87%91%E9%92%B1%E4%B8%8D%E5%8F%AF%E4%B9%B0%E7%9A%84%E4%B8%9C%E8%A5%BF%E3%80%8B" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1ct4y167fM（B站）另外包括讲座《金钱不可买的东西》</a></p><p>课程讲稿发表为书籍《公正》，相关书籍还推荐《洞穴奇案》。</p><p>[ 但个人阅读体会觉得富勒写的第一部分比较精彩，萨伯补充的后九个观点略显逊色些 ]</p><p><em>对于哲学问题没有一劳永逸的解决方式，也许我们会产生怀疑论的回避，但永远无法平息内心渴望理性思考的不安</em></p><h2 id="功利主义与道德主义"><a class="markdownIt-Anchor" href="#功利主义与道德主义"></a> 功利主义与道德主义</h2>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人文社科</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cs224w《图机器学习》2021（二）图神经网络</title>
    <link href="/2021/05/06/cs224w2/"/>
    <url>/2021/05/06/cs224w2/</url>
    
    <content type="html"><![CDATA[<h2 id="相关论文"><a class="markdownIt-Anchor" href="#相关论文"></a> 相关论文</h2><p>GCN《<a href="https://arxiv.org/abs/1609.02907" target="_blank" rel="noopener">Semi-supervised classification with graph convolutional networks</a>》</p><p>GraphSAGE 《<a href="https://arxiv.org/abs/1706.02216" target="_blank" rel="noopener">Inductive representation learning on large graphs</a>》</p><p>GAT《<a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener"><strong>Graph attention networks</strong></a>》</p><p>GNN with skip connection_1 《<a href="https://arxiv.org/abs/1605.06431" target="_blank" rel="noopener"><strong>Residual networks behave like ensembles</strong> of <strong>relatively shallow networks</strong></a>》</p><p>GNN with skip connection_2 《<a href="http://proceedings.mlr.press/v80/xu18c.html" target="_blank" rel="noopener">Representation learning on graphs with jumping knowledge networks</a>》</p><p>DiffPool 《<a href="https://arxiv.org/abs/1806.08804" target="_blank" rel="noopener">Hierarchical graph representation learning with differentiable pooling</a>》</p><p>GIN《<a href="https://arxiv.org/abs/1810.00826" target="_blank" rel="noopener">How powerful are graph neural networks?</a>》</p><h2 id="信息传播与节点分类"><a class="markdownIt-Anchor" href="#信息传播与节点分类"></a> 信息传播与节点分类</h2><p>半监督节点分类问题（<strong>semi-supervised</strong> node classification）</p><p>消息传递框架（message passing framework），关键思想在于，<strong>同类/同标签的节点间倾向于有连接</strong>，也即correlations。</p><p>集体分类（collective classification），节点根据其邻居的标签更新其自身标签。</p><p>相关场景包括比如恶意网页检测、垃圾邮件、欺诈用户检测等等。</p><h3 id="一-基础概念"><a class="markdownIt-Anchor" href="#一-基础概念"></a> 一. 基础概念</h3><p>相关性（correlation）具体体现在以下两个方面：</p><ul><li><p><strong>同构/同质性（Homophily）</strong>：以社交网络为例，具有相似特征的人倾向于相互联系（社会学同质性概念）。具体定义为“The tendency of individuals to associate and bond with similar others”。</p></li><li><p>影响力（Influence）：以社交网络为例，社会关系会影响我们自己的特征或行为；</p></li></ul><p>考虑节点的属性及其邻居节点的标签和属性，确定某节点v标签，方法可以表达为Guilt-by-association。</p><p>可以使用概率框架，依照一阶<strong>马尔科夫假设（Markov Assumption）</strong>，即节点v的标签只取决于其邻居节点们的标签。这里的“一阶”表示我们只考虑当前节点的一跳邻居。</p><p>集体迭代分类包括三个步骤：</p><ul><li>局部分类器（Local Classifier）：为节点<strong>分配初始标签</strong>；这是一个基于节点属性预测标签的标准分类器，与网络结构信息无关。</li><li>关系分类器（Relational Classifier）：捕获节点时间的<strong>相互关系</strong>；此分类器应用到网络结构信息，基于邻居节点属性/标签预测当前节点标签。</li><li>集体推理（Collective Inference）：在网络中<strong>传递相关性/信念（belief）</strong>，直到出现标签并收敛；迭代地将关系分类器应用于每个节点知道相邻节点间的预测结果趋向一致。</li></ul><h3 id="二-经典方法"><a class="markdownIt-Anchor" href="#二-经典方法"></a> 二. 经典方法</h3><h4 id="1关系分类-relational-classification"><a class="markdownIt-Anchor" href="#1关系分类-relational-classification"></a> 1.关系分类 Relational Classification</h4><p>基本思想：节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>的标签概率是其<strong>邻居节点</strong>概率的平均值/边权重加权。有标记节点使用其真实标签，未标记节点，标签初始化为0.5。以随机的顺序更新全部节点概率直到到达迭代最大次数或结果收敛。</p><p>[ 这个方法没有应用到节点属性，也不保证收敛。 ]</p><h4 id="2迭代分类-iterative-classification"><a class="markdownIt-Anchor" href="#2迭代分类-iterative-classification"></a> 2.迭代分类 Iterative Classification</h4><p>基本思想：针对节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>，基于其属性和其节点集合的标签确定它的标签。</p><p>组成：训练两个分类器：1）基础分类器，基于节点属性预测其标签；2）两个输入的分类器：基于节点属性和邻居节点标签（a label summary vector z）预测节点v标签。</p><p>方法：1）训练阶段，完成上述两个分类器的训练；2）在测试集合（unlabeled nodes）中迭代直到收敛，首先使用基础分类器得到初始标签，计算出邻居向量z，使用分类器二得到预测结果；之后进入分类器二的迭代。</p><div align="center">  <img src="/2021/05/06/cs224w2/iteration.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>[ 这个方法不保证收敛。]</p><h4 id="3循环置信传播-loopy-belief-propagation"><a class="markdownIt-Anchor" href="#3循环置信传播-loopy-belief-propagation"></a> 3.循环置信传播 Loopy Belief propagation</h4><p>Loopy表示其适用的图数据中可能有环（cycles）。核心在于potential function/matrix。</p><div align="center">  <img src="/2021/05/06/cs224w2/notion.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>收集下游节点信息，然后结合自己的标签本性，决定其向节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span>传递的信息。</p><div align="center">  <img src="/2021/05/06/cs224w2/bp.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>即便图数据中有环（子图中的各节点不再独立，而是相互影响），我们也可以随意挑选起始节点，然后沿着边传递信息。</p><p>但是有环的时候可能无法收敛，但是实践中效果依然很好，<strong>环不是问题</strong>。最糟糕的情况是下面这样，但在实际中不常见：</p><div align="center">  <img src="/2021/05/06/cs224w2/bpw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>总结一下有关信念传播方法：</p><ul><li>容易代码实现，也容易并行化</li><li>易于应用于各种图模型，可以基于各种潜在矩阵（potential matrix）不一定是上述的label-label矩阵；不只考虑了同构性而是加入了更复杂的关系</li><li>这个方法也是不保证收敛的</li><li>potential matrix需要一定的估算</li><li>是一种非常强大、有效的半监督节点分类方法</li></ul><h2 id="gnn基础"><a class="markdownIt-Anchor" href="#gnn基础"></a> GNN基础</h2><p>之前学习的浅层encoder-decoder节点嵌入方法的局限性在于：</p><ul><li>每个节点有自己的embedding向量，即需要训练<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(|V|)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mord">∣</span><span class="mclose">)</span></span></span></span>个参数</li><li>属于转导学习（transductive），无法应对在训练阶段未出现的节点</li><li>没有考虑节点属性</li></ul><p>Deep Graph Encoders，encoders是基于图结构信息的多层非线性转换。这些编码器可以和之前学到的所有节点相似度计算方法结合。</p><p>这些方法可以应用于节点分类、链路预测、社区发现、网络相似度计算等任务。</p><h3 id="一-深度学习基础"><a class="markdownIt-Anchor" href="#一-深度学习基础"></a> 一. 深度学习基础</h3><div align="center">  <img src="/2021/05/06/cs224w2/sgd.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>常用优化器包括Adam、Adagrad、Adadelta、RMSprop等</p><p>使用深度学习框架使得训练时的梯度计算变得十分容易！</p><p><strong>批量归一化（Batch Normalization）</strong>，用于稳定模型训练过程。给定一批数据，通过变换将其移到均值为0，方差为1的情况。</p><div align="center">  <img src="/2021/05/06/cs224w2/nor.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>Dropout</strong>，防止过拟合。在GNN中，dropout被应用于消息传递即message部分的线性层。</p><div align="center">  <img src="/2021/05/06/cs224w2/dropout.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>激活函数</strong>，非线性化，常用ReLU、Sigmoid、Parametric ReLU（实际场景中基本效果比ReLU要好）等。</p><h3 id="二-图的深度学习"><a class="markdownIt-Anchor" href="#二-图的深度学习"></a> 二. 图的深度学习</h3><p>当图数据中没有节点属性时，一般会选择使用1）指示向量，即节点的one-hot编码或 2）全1向量。</p><p>最简单的方案是直接将邻接矩阵和节点属性矩阵拼接起来，直接加入一个深度神经网络中，类似视为一张图片加入CNN中。但这样做的问题在于：</p><ul><li>需要训练的参数非常多</li><li>无法使用与各种大小的图数据</li><li>对节点次序信息敏感</li></ul><p>将CNN的思想扩展到GNN上，将图片上的邻域扩展成图数据中的<strong>节点邻居集合</strong>。GNN变为两个关键步骤：1）确定某个节点的计算（子）图；2）在其上进行信息传递。</p><p>每个节点都会基于其邻居节点得到它的计算图。</p><div align="center">  <img src="/2021/05/06/cs224w2/graph.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>模型可以有多层，每一层都会有节点的表征信息，第0层节点向量是其输入特征向量，第k层的嵌入向量中考虑了K跳之外的邻居节点信息。</p><p>不同模型的邻居节点<strong>特征汇总方式</strong>会有所不同。注意，邻居聚合函数需要<strong>与节点次序无关</strong>。一般常用Average、sum等。重要的是该模型中所有的节点共享参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span>。</p><div align="center">  <img src="/2021/05/06/cs224w2/gnn.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>以上模型也可以写为矩阵分解的形式，最终的矩阵是比较稀疏的。[ 当聚合方式比较复杂时，这种GNN无法写为矩阵形式]</p><div align="center">  <img src="/2021/05/06/cs224w2/matrix.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/06/cs224w2/rewrite.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>可以基于节点标签进行有监督训练<strong>或者基于图结构信息进行无监督学习</strong>。无监督学习中关于节点相似性的定义可以使用之前介绍过的随机游走、矩阵分解等形式。</p><div align="center">  <img src="/2021/05/06/cs224w2/unsupervised.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>使用GNN模型获取节点表征向量的总结：</p><ul><li>定义邻域聚合函数，确定当前节点邻居节点集合及聚合方式 [key distinctions are in how different approaches aggregate information across the layers]</li><li>定义表征向量损失函数</li><li>在一个集合上完成模型训练</li><li>所有节点共享相同的聚合参数，所以模型参数规模与网络规模是<strong>次线性</strong>的（sublinear），而且模型<strong>具有归纳能力</strong>（inductive），可以扩展到训练集中未出现的节点上，所以可以应用到新的图数据或新发展出的节点上 [注意一下<strong>动态图模型</strong>的研究主题]。</li></ul><h2 id="gnn模型设计"><a class="markdownIt-Anchor" href="#gnn模型设计"></a> GNN模型设计</h2><p>GNN层 = <strong>Message + Aggregation</strong>，如何定义（1）消息和（2）聚合时区分不同GNN模型的关键。另外还有（3）<strong>如何进行层堆叠、（4）如何确定计算图</strong>，以及（5）如何进行模型学习。</p><div align="center">  <img src="/2021/05/06/cs224w2/framework.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="一-单层gnn"><a class="markdownIt-Anchor" href="#一-单层gnn"></a> 一. 单层GNN</h3><p>首先进行“消息计算”，定义一个message function，生成的消息会传递给其它节点。</p><p>之后是聚合，每个节点会聚合来自其邻居的消息，比如使用Sum、Mean、Max等，注意要与节点次序无关（order invariant）。将来自邻居的信息和来自上一层的节点自身的信息结合起来。</p><div align="center">  <img src="/2021/05/06/cs224w2/single.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>最后一般加一个激活函数，比如ReLU，sigmoid等，用来提升表达能力。</p><h4 id="1-gcn"><a class="markdownIt-Anchor" href="#1-gcn"></a> 1. GCN</h4><p>节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>在第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span>层的表征是其邻居节点表征的均值。按照message+aggregation的形式可以写成下面的样子：</p><div align="center">  <img src="/2021/05/06/cs224w2/GCN.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2graphsage"><a class="markdownIt-Anchor" href="#2graphsage"></a> 2.GraphSAGE</h4><p>扩展了GCN的聚合函数形式（比如Mean、Pooling、LSTM（需要特殊操作消除次序信息的影响）等），而且使用<strong>CONCAT</strong>的方式把自身消息和邻居消息结合起来。</p><div align="center">  <img src="/2021/05/06/cs224w2/graphsage.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>另外，GraphSAGE在每一层都<strong>加入了L2归一化</strong>，如果没有的话节点向量取值范围不同，加入L2归一化可以提升效果。</p><h4 id="3-gat"><a class="markdownIt-Anchor" href="#3-gat"></a> 3. GAT</h4><p>对于每个邻居加入一个权重，体现针对当前节点，其不同邻居的不同重要性。</p><p>GCN和GraphSage直接使用了平均即权重为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>N</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">1/N(v)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mclose">)</span></span></span></span>，即每个邻居节点都同样重要。</p><p><strong>Attention</strong>是受认知注意力启发而来，注意力系数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>v</mi><mi>u</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{vu}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span><span class="mord mathdefault mtight">u</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>强调输入数据中重要的部分而忽略其他内容。在模型训练过程中学习到哪部分数据是重要的。</p><div align="center">  <img src="/2021/05/06/cs224w2/GAT.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>注意力机制</strong></p><p>在节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>u</mi><mo separator="true">,</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">u,v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>之间计算注意力系数（attention coefficients <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>e</mi><mrow><mi>u</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">e_{uv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>），表示来自节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">u</span></span></span></span>的信息对节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>的重要程度。之后对<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>e</mi><mrow><mi>u</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">e_{uv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>进行归一化（比如softmax）得到最终的权重系数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mrow><mi>u</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">a_{uv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。最终这个参数和其它参数如权重矩阵W一起训练，模型是一个端到端的效果。</p><div align="center">  <img src="/2021/05/06/cs224w2/attention.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>加入注意力机制后的模型可能很难训练，很难收敛。通过使用多头注意力机制（multi-head attention）稳定其学习过程。本质思想是设计多个注意力函数，聚合之后一起计算，增加模型鲁棒性。每一个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mrow><mi>u</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">a_{uv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>使用不同的函数预测，而且每个函数都随机其初始值。</p><div align="center">  <img src="/2021/05/06/cs224w2/multi.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>加入注意力机制的好处：</p><ul><li>区分了不同邻居节点对当前节点的重要程度</li><li>计算高效，参数的计算可以并行。注意力系数可以按不同边并行；聚合操作可以在节点间并行</li><li>存储高效，稀疏矩阵运算符，参数数目固定（与图大小无关）</li><li>局部化，只关注局部网络邻居信息</li><li>可扩展，具有归纳能力，是一种shared edge-wise机制，不依赖全局图结构</li></ul><h3 id="二-gnn堆叠"><a class="markdownIt-Anchor" href="#二-gnn堆叠"></a> 二. GNN堆叠</h3><h4 id="1-经典方式"><a class="markdownIt-Anchor" href="#1-经典方式"></a> 1. 经典方式</h4><p>直接将GNN层sequentially叠加起来。</p><p>GNN的深度表示我们在得到某一节点表征的时候考虑了它的几跳邻居，而GNN模型的表达能力/复杂度是取决于单层GNN层的设计。</p><p>直接堆叠起来会带来<strong>over-smoothing</strong>的问题，无法构造比较深层的GNN模型。到达一定深度，所有节点的表征向量会收敛到一起。</p><p>可以通过<strong>Receptive field</strong>（感受野）解释这个问题，GNN中可定义为为获取某节点表征向量而需要考虑的节点集合。在一个K层GNN模型中，每个节点的感受野就是它的K跳（以内的）邻居。</p><p>堆叠多层GNN层 ——&gt; 各节点的感受野重合程度过大 ——&gt; 得到的节点表征向量过于近似  ——&gt; 造成过平滑问题</p><p>最直接的解决办法是在构造模型是注意堆叠的GNN层数。首先分析解决任务必要的感受野（比如<strong>首先计算一下图数据的半径</strong>），GNN的层数可以比这个感受野稍大一些。</p><p><strong>如何让浅层GNN模型更具表达力？</strong></p><p><strong>解决方法1，提升每层GNN的表达能力</strong>；之前介绍的模型中每层里的聚合或变换都只使用了线性层，我们可以改成使用深度学习网络，比如换成3层MLP。[ 这里也可以看出，GNN中的层概念和DNN中有所不同 ]</p><p><strong>解决方法2，添加不进行消息传递的层</strong>；GNN模型中不一定只包含GNN层，在GNN层之前/之后都可以加入MLP层进行预处理或后处理。</p><div align="center">  <img src="/2021/05/06/cs224w2/MLP.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-skip-connection"><a class="markdownIt-Anchor" href="#2-skip-connection"></a> 2. Skip connection</h4><p>如何构建深层GNN模型？可以在GNN层之间加入一些跳过连接（skip connection）。</p><p>研究发现有时候比较前面的GNN层得到的节点表征结果更有利于区分节点，所以可以在最终结果中提升这些层的重要性。skip connection可以构建mixture of models，即上一层和当前层信息的加权组合。由此我们得到了一个深度GNN模型和一些浅层GNN模型（如果有N个跳过连接，会得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mi>N</mi></msup><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2^N-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.924661em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>个浅层模型）的混合模型。</p><p>跳过的操作可以如下图所示，也可以直接跳到最后一层（如ICML 2018中写到的），有很多种方式。</p><div align="center">  <img src="/2021/05/06/cs224w2/skip.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="三-图征增强-graph-augmentation"><a class="markdownIt-Anchor" href="#三-图征增强-graph-augmentation"></a> 三. 图征增强 Graph Augmentation</h3><p>很多情况下原始的图数据不适合直接作为计算图数据，而是需要一些改进。</p><h4 id="1特征增强"><a class="markdownIt-Anchor" href="#1特征增强"></a> 1.特征增强</h4><p>原始的图数据可能缺少某些特征信息  ——&gt; feature augmentation</p><p><strong>问题1，输入图数据中不含有节点特征</strong></p><p>方法1：直接赋予常量值作为特征</p><p>方法2：给每个节点赋予独特的ID，并且进行独热编码；但这个方法问题在于无法很好地扩展到其它图数据上</p><div align="center">  <img src="/2021/05/06/cs224w2/aug.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>问题2，有些结构对于GNN来说很难学</strong></p><p>比如，如下图所示，节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>处于某个环形结构中，GNN很难学习到这个结构的周长，区分这两个结构。</p><div align="center">  <img src="/2021/05/06/cs224w2/feature.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>方法，可以把这个结构信息编辑到节点特征中去，比如左图中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>v</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">v_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的独热编码在第三位（处于周长为3的环形中），右图中的在第四位。</p><p>其它方法还包括使用节点度、聚合系数、PageRank、中心度等之前介绍过的特征。</p><h4 id="2结构增强"><a class="markdownIt-Anchor" href="#2结构增强"></a> 2.结构增强</h4><p>原始图数据结构可能：</p><ul><li>过于稀疏，影响消息传递   ——&gt; 添加虚拟节点或边</li><li>过于密集，消息传递花费过大 ——&gt; 消息传递时进行邻居采样，降低计算成本</li><li>图太大，无法装载如显存 ——&gt; 计算时进行子图采样</li></ul><p><strong>添加虚拟边</strong></p><p>使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><mo>+</mo><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">A + A^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>进行计算，而非单独使用邻接矩阵A表示图，这样我们给2-hop的邻居加入了虚拟边。</p><p>比如在author-paper的二部图中，使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">A^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>就构造了一个共同作者网络。</p><p><strong>添加虚拟节点</strong></p><p>给稀疏图中加入一个节点，这个虚拟节点与图中所有的节点都相连。</p><p><strong>节点采样</strong></p><p>可以采样当前节点的邻居节点进行消息聚合，而非考虑其所有邻居的消息。</p><p>如何选取邻居子集呢？是一个可以优化研究的问题</p><h2 id="gnn模型训练"><a class="markdownIt-Anchor" href="#gnn模型训练"></a> GNN模型训练</h2><div align="center">  <img src="/2021/05/06/cs224w2/GNNt.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="一-预测任务"><a class="markdownIt-Anchor" href="#一-预测任务"></a> 一. 预测任务</h3><h4 id="1node-level-prediction"><a class="markdownIt-Anchor" href="#1node-level-prediction"></a> 1.Node-level prediction</h4><p>直接把GNN得到的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span>维表征向量映射到为K类标签向量。</p><div align="center">  <img src="/2021/05/06/cs224w2/node.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-edge-level-prediction"><a class="markdownIt-Anchor" href="#2-edge-level-prediction"></a> 2. Edge-level prediction</h4><p>以节点对表征向量为输入映射到K类标签向量。</p><p>处理节点对向量常用方法有两类：1）Concat + Linear（GAT中使用过）；</p><div align="center">  <img src="/2021/05/06/cs224w2/2d.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>2）向量内积，一般会得到一个标量结果，如果想做成K向预测，可以使用类似多头注意力机制的处理方法。</p><div align="center">  <img src="/2021/05/06/cs224w2/kway.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="3-graph-level-prediction"><a class="markdownIt-Anchor" href="#3-graph-level-prediction"></a> 3. Graph-level prediction</h4><p>需要将节点的表征向量聚合来得到表示整个图的向量。可以使用很多pooling方法，比如Mean、Max、Sum等。</p><p>但有时候简单的全局池化会损失太多信息，尤其当图规模比较大的时候。</p><p>改善这个问题可以使用<strong>分层池化</strong>（hierarchical global pooling）。可以加入聚类方法确定每层中可以聚合的节点集合。这里的两个GNN模型是独立的，每层的模型可以并行训练。</p><div align="center">  <img src="/2021/05/06/cs224w2/diffpool.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="二-预测和标签"><a class="markdownIt-Anchor" href="#二-预测和标签"></a> 二. 预测和标签</h3><p>图上的监督学习：标签来自外部；图上的无监督学习：标签来自图数据自己，比如链路预测问题。但实际上二者之间的界限比较模糊。</p><p>在无监督学习中，可以使用图数据自身产生的标签信息：</p><ul><li>Node-level，使用节点统计数据，比如聚类系数、PageRank等</li><li>Edge-level，比如在链路预测任务中隐藏两节点间的边</li><li>Graph-level，使用图特征，比如是否两个图是同构的</li></ul><h3 id="三-损失函数与评估"><a class="markdownIt-Anchor" href="#三-损失函数与评估"></a> 三. 损失函数与评估</h3><p>GNN可以进行分类任务，也支持回归任务（标签<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span></span>具有连续值）</p><p>分类问题中常用交叉熵，重点强调了K向预测时的交叉熵。</p><div align="center">  <img src="/2021/05/06/cs224w2/entropy.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>回归问题一般使用MSE，K向回归计算如下：</p><div align="center">  <img src="/2021/05/06/cs224w2/kreg.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>评估GNN时一般也是使用sklearn中的常规指标。评估回归问题的时候常用RMSE、MAE等。</p><h3 id="四-数据切分"><a class="markdownIt-Anchor" href="#四-数据切分"></a> 四. 数据切分</h3><p>图数据中有的时候无法明确区分出测试集，因为图中的节点是相互连接的，彼此之间不独立。</p><p>方案1：transductive setting，只根据标签区分，训练阶段可以看到所有数据，我们只需要区分不同节点的标签</p><div align="center">  <img src="/2021/05/06/cs224w2/tran.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>方案2：inductive setting, 将图数据中的某些边截断，区分成多个子图</p><div align="center">  <img src="/2021/05/06/cs224w2/ind.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>两个方法的区别主要在于：</p><div align="center">  <img src="/2021/05/06/cs224w2/vs.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>在各类任务中注意一下inductive setting的<strong>链路预测任务</strong>，一般说起链路预测的话是transductive setting的任务。</p><p><strong>链路预测任务</strong>中，训练时可以看到message edge而supervision edge不喂入GNN模型。</p><p>inductive setting设置如下：</p><div align="center">  <img src="/2021/05/06/cs224w2/indl.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>transductive setting为：</p><div align="center">  <img src="/2021/05/06/cs224w2/tranl.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>【注意，以上是教授提出的认为正确的区分方法，不同论文中的划分方法有所不同】</p><h2 id="gnn表达能力"><a class="markdownIt-Anchor" href="#gnn表达能力"></a> GNN表达能力</h2><p>目前已经有很多GNN模型被提出来，大家的区分点主要是在消息处理和聚合时使用的网络不同。</p><p>例如GCN使用element-wise mean pooling+ Linear + ReLU。GraphSAGE使用MLP + max-pool。</p><div align="center">  <img src="/2021/05/06/cs224w2/model.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>GNN模型的表达能力主要体现在它的计算图（或叫做subtree rooted around each node）上，如下图所示，GNN得到的映射要尽可能做到<strong>单射（injective）</strong>。如果每层GNN上的聚合都是单射的，那么这个GNN模型就可以完全区分不同的子树模型。</p><div align="center">  <img src="/2021/05/06/cs224w2/emb.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>GCN和GraphSAGE并不是单射的，这部分内容在《图神经网络的局限》博文中有总结和介绍。</p><p>由此比较，SUM pooling具有较强表达/判别能力，其次是Mean pooling，再次是Max pooling。</p><h3 id="一-设计具有强表达能力的gnn模型"><a class="markdownIt-Anchor" href="#一-设计具有强表达能力的gnn模型"></a> 一. 设计具有强表达能力的GNN模型</h3><p>通过设计单射的邻居聚合方法构建具有最佳表达能力的基于消息传递的GNN模型。</p><div align="center">  <img src="/2021/05/06/cs224w2/inject.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>根据“含有一层足够多神经元隐藏层的MLP可以拟合任意函数”的定力，作者使用MLP设计<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">ϕ</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span>。实验表明，大概用100~500个神经元，可以训练出很好的单射函数。由此作者提出GIN模型。</p><p>[ GIN is the most expressive GNN n the class of message-passing GNNs! ]</p><p>[ The key is to use element-wise sum pooling. ]</p><h3 id="二-使用wl测试解释gin模型"><a class="markdownIt-Anchor" href="#二-使用wl测试解释gin模型"></a> 二. 使用WL测试解释GIN模型</h3><p>WL测试中可以使用color refine算法，如果两个图模型拥有相同颜色集合，则表示它们同构。</p><div align="center">  <img src="/2021/05/06/cs224w2/color.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>GIN模型使用MLP模仿color refine算法中的单射哈希方法.</p><div align="center">  <img src="/2021/05/06/cs224w2/gin.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>两种方法的对比如下，<strong>两模型的表达能力相当</strong>。GIN的优势在于：1）它可以得到低阶表征向量；2）方程的参数训练可以利用到下游任务中的信息。WL已于1992年被证实可以区分大多数实际图模型。</p><div align="center">  <img src="/2021/05/06/cs224w2/com.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="三-当前挑战"><a class="markdownIt-Anchor" href="#三-当前挑战"></a> 三. 当前挑战</h3><p>目前还有一些GNN无法区分的基础图结构，比如之前提到的不同节点数的环形。</p><p>已有工作在这方面开始努力。 [You et al. AAAI 2021, Li et al. NeurIPS 2020]</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络经典论文</title>
    <link href="/2021/05/06/GNN-paper/"/>
    <url>/2021/05/06/GNN-paper/</url>
    
    <content type="html"><![CDATA[<p>本文包括GNN相关论文如GAT、GCN、GraphSAGE、PinSAGE、TGN等</p><h2 id="基础"><a class="markdownIt-Anchor" href="#基础"></a> 基础</h2><p>邻接矩阵可以构造位置矩阵（location matrix），位置矩阵常用于基于谱分析的GNN中，基于傅里叶变换构造拉普拉斯特征根。得到特征根之后就可以把图上的信号视为这些特征向量的加权。[ 和信号处理中相似 ]</p><h2 id="gcn"><a class="markdownIt-Anchor" href="#gcn"></a> GCN</h2><h2 id="graphsage"><a class="markdownIt-Anchor" href="#graphsage"></a> GraphSAGE</h2><h2 id="gat"><a class="markdownIt-Anchor" href="#gat"></a> GAT</h2><p>论文全称《Graph Attention Network》，基于无向图，是一种基于空间的GNN，而且具有归纳能力。</p><p>重点在于引入注意力机制（Transformer 2017）。</p><h2 id="tgn"><a class="markdownIt-Anchor" href="#tgn"></a> TGN</h2><h2 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料</h2>]]></content>
    
    
    <categories>
      
      <category>知识梳理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>联邦图模型论文与开源框架</title>
    <link href="/2021/05/05/fedgraphNN/"/>
    <url>/2021/05/05/fedgraphNN/</url>
    
    <content type="html"><![CDATA[<p>目前的论文课题需要用到图模型联邦学习技术，将调研结果总结为本文。</p><p>文章主要包括联邦学习内容梳理（依据FedML论文、谷歌的巨长综述等）、图模型联邦学习相关论文介绍、联邦学习框架、FedGraphNN代码等部分。</p><h2 id="联邦学习"><a class="markdownIt-Anchor" href="#联邦学习"></a> 联邦学习</h2><p>自2016年由谷歌提出以来，联邦学习已成为一个比较火热的研究方向。联邦学习（FL）是一种分布式学习框架，许多客户端（如移动设备、组织）在中央服务器（如服务提供商）的协调下<strong>共同训练模型</strong>，同时保持训练数据的<strong>去中心化及分散性</strong>。</p><p>大量工作试图使用中央服务器在<strong>保护隐私的同时从本地数据中学习</strong>。目前没有任何一项工作可以直接解决FL定义下的全部挑战。“联邦学习”这个词为这一系列特征，约束和挑战<strong>提供了便捷的简写</strong>，这些约束和挑战通常在隐私至关重要的机器学习问题中同时出现。</p><h2 id="图模型联邦学习"><a class="markdownIt-Anchor" href="#图模型联邦学习"></a> 图模型联邦学习</h2><h2 id="联邦学习框架"><a class="markdownIt-Anchor" href="#联邦学习框架"></a> 联邦学习框架</h2><h3 id="一-fedml"><a class="markdownIt-Anchor" href="#一-fedml"></a> 一. FedML</h3><p>以科研为主要导向</p><h2 id="fedgraphnn"><a class="markdownIt-Anchor" href="#fedgraphnn"></a> FedGraphNN</h2><p>以FedML为基础打造</p><h2 id="参考材料"><a class="markdownIt-Anchor" href="#参考材料"></a> 参考材料</h2><ul><li>《FedML: A Research Library and Benchmark for Federated Machine Learning》</li><li>《FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks》</li><li><a href="https://fedml.ai/" target="_blank" rel="noopener">https://fedml.ai/</a></li><li><a href="https://github.com/chaoyanghe/Awesome-Federated-Learning" target="_blank" rel="noopener">https://github.com/chaoyanghe/Awesome-Federated-Learning</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>知识梳理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
      <tag>联邦学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cs224w《图机器学习》2021（一）经典方法</title>
    <link href="/2021/05/05/cs224w/"/>
    <url>/2021/05/05/cs224w/</url>
    
    <content type="html"><![CDATA[<h2 id="学习资料"><a class="markdownIt-Anchor" href="#学习资料"></a> 学习资料</h2><p>相比于2019年的课堂录播，本年度直接使用线上课程形式，更加方便理解学习。</p><div align="center">  <img src="/2021/05/05/cs224w/dagang.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>课程官方网站：<a href="http://web.stanford.edu/class/cs224w/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224w/</a></p><p>课程视频链接：Youtube（<a href="https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn%EF%BC%89%E3%80%81B%E7%AB%99%EF%BC%88https://www.bilibili.com/video/BV1RZ4y1c7Co%EF%BC%89" target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn）、B站（https://www.bilibili.com/video/BV1RZ4y1c7Co）</a></p><p>参考书目：《Graph Representation Learning 》by Will Hamilton</p><p>编程工具：<a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html" target="_blank" rel="noopener">Pytorch Geometric（PyG）</a>、DeepSNAP、GraphGym、<a href="http://SNAP.PY" target="_blank" rel="noopener">SNAP.PY</a>、<a href="https://networkx.org/documentation/stable/tutorial.html" target="_blank" rel="noopener">NetworkX</a></p><h3 id="相关论文"><a class="markdownIt-Anchor" href="#相关论文"></a> 相关论文</h3><ul><li>PinSAGE，《Graph Convolutional Neural Networks for Web-Scale Recommender Systems》KDD，2018</li><li>DeepWalk, 《Online Learning of Social Representations》KDD 2014</li><li>node2vec，《node2vec: Scalable Feature Learning for Networks.》KDD 2016</li><li>Graph Embedding Survey，《Graph Embedding Techniques, Applications, and Performance: A Survey》2017</li><li>子图表征，引入虚拟节点，《Gated Graph Sequence Neural Networks》</li><li>匿名游走，《Anonymous Walk Embeddings》ICML 2018</li><li>矩阵分解与节点表征，《Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec》WSDM 2018</li></ul><h2 id="背景"><a class="markdownIt-Anchor" href="#背景"></a> 背景</h2><p>为什么使用图模型？</p><p>不仅考虑数据本身，还要考虑实体间的复杂关系。How do we take advantage of relational structure for better prediction?</p><p>多种图模型样例：计算机网络、疾病传播、食物链网络、交通网络、社交网络、论文引用、神经元连接、知识图谱、代码、化学分子、3D建模等等。</p><p>当前深度学习模型一般应用于序列、图片等简单的数据结构。Graphs are the new frontier of deep learning.</p><p>图模型常见任务：</p><ul><li>节点分类：DeepMind提出AlphaFold解决生物学领域蛋白质折叠问题；蛋白质序列中的氨基酸为节点，氨基酸（残基）之间的接近度为边；</li><li>链路预测：推荐系统（users-items）、多种药物一起的副作用预测</li><li>图/子图分类：交通流量预测、药物发现</li><li>聚类</li><li>生成图：新分子发现</li><li>图进化：物理仿真</li></ul><p>选择节点和连接，构成何种图模型这一步非常重要。</p><p>图模型基础概念：</p><ul><li>有向、无向图  -&gt; 度、节点平均度（2E/N）</li><li>Bipartite Graph（二部图）-&gt; Folded network（映射图）</li><li>邻接矩阵（Adjacency Matrix），现实网络的邻接矩阵通常非常稀疏</li><li>边缘列表（Edge list），在深度学习工程实现时十分常用</li><li>邻接表（Adjacency list）</li><li>其它可用属性：边权重、排名、类型、标签以及其它与场景契合的属性等</li><li>自环（self-loops）</li><li>多图（multigraph）：一对节点间有多个边</li><li>连通性、强连接（有向图中每对节点可以相互访问，SCCs，Strongly connected components，强连接部分）、弱连接</li></ul><h2 id="经典图机器学习方法"><a class="markdownIt-Anchor" href="#经典图机器学习方法"></a> 经典图机器学习方法</h2><p>经典机器学习方法重点在于<strong>提取有效特征</strong>（hand-designed features）。</p><h3 id="一-特征提取"><a class="markdownIt-Anchor" href="#一-特征提取"></a> 一. 特征提取</h3><h4 id="1-node-features"><a class="markdownIt-Anchor" href="#1-node-features"></a> 1. Node features</h4><p><strong>节点度（node degree）</strong>[importance, structure]，相同度数的节点无法区分</p><p><strong>节点中心度（node centrality）</strong>[importance]，考虑了图中不同节点的重要程度，包括engienvector centrality，betweenness centrality, closeness centrality等。<strong>engienvector centrality</strong>，某节点的重要程度是其邻居节点重要程度的归一化和（递归计算）；<strong>betweenness centrality</strong>，存在于其它节点对间最短路径上的节点更重要；<strong>closeness centrality</strong>，与其它各节点间最短路径长度和越短的节点越重要。</p><div align="center">  <img src="/2021/05/05/cs224w/ec.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/05/cs224w/bc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/05/cs224w/cc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>聚类系数（clustering coefficient）</strong>[structure]，节点附近的局部结构，衡量某节点的邻居节点间的联通程度如何。基本上是在计算自网络（ego-network）中的<strong>三角形</strong>个数。</p><div align="center">  <img src="/2021/05/05/cs224w/coefficient.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>GDV(graphlet degree vector)</strong> [structure]，将上述三角形概念扩大，Rooted connected non-isomorphic subgraphs。以下概念中节点的位置也很重要。GDV即该节点在某种graphlet出现的次数。</p><div align="center">  <img src="/2021/05/05/cs224w/graphlets.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/05/cs224w/gdv.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-link-prediction-features"><a class="markdownIt-Anchor" href="#2-link-prediction-features"></a> 2. Link Prediction features</h4><p>链路预测任务，比如随机丢失了当前图模型中的某些连接信息，或需要预测下一时间窗口中的节点连接信息。</p><p><strong>Distance-based features</strong>，如两节点之间的最短距离</p><p><strong>Local neighborhood overlap</strong>，两节点间的共有邻居数，Jaccard系数、Adamic-Adar index等</p><p><strong>Global Neighborhood Overlap</strong>，katz index计算一对节点间的全部路径数目（使用邻接矩阵A计算）；邻接矩阵的N次幂表示了每对节点间长度为N的路径的数量。</p><div align="center">  <img src="/2021/05/05/cs224w/katz.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="3-graph-level-features"><a class="markdownIt-Anchor" href="#3-graph-level-features"></a> 3. Graph-level features</h4><p>核方法（kernel），核心是设计一个kernel而不是使用特征向量。两个图模型之间的Kernel衡量的是图数据之间的相似度。内核矩阵是测量每对数据点之间的相似度，它一定是正定的，即只有正数特征值。</p><p>当前存在很多Graph kernels，比如graphlet kernel，Weisfeiler-Lehman Kernel, Random-walk kernel，shortest-path graph kernel等等。</p><p>Graph kernel背后的思想是给图模型做词袋（Bag-of-words），将图中的节点视为词，比如Bag of node degrees。而graphlet kernel，Weisfeiler-Lehman Kernel是这类方法的延伸，即使用了比节点度更复杂一些的表示方法。</p><p>Graphlet kernel使用不同graphlet数目，这边的graphlet定义与node-level features那一节中的有所不同。首先这类graphlet中允许存在孤立节点，而且它们不是rooted。注意，计算两个图模型相似性的时候可以把向量f归一化一下。这类方法的问题在于graphlets计数非常困难，类似的subgraph isomorphism test问题是NP难的。</p><div align="center">  <img src="/2021/05/05/cs224w/graphlet.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>Weisefeiler-Lehman Kernel</strong>旨在缓解这个问题，使用<strong>color refinement</strong>。其中确定属性使用到了哈希函数。这个方法计算效率较高，是线性于两个图的边数目的。Weisefeiler-Lehman Kernel是衡量图相似度的一种非常有效的方式（很难被击败），也是与GNN是息息相关的。</p><h2 id="节点表征"><a class="markdownIt-Anchor" href="#节点表征"></a> 节点表征</h2><p>重点包括三部分内容：1）编解码器框架，encoder是一个简单的表征向量查询，decoder是基于表征向量计算与定义的节点相似度匹配程度；2）节点相似度衡量方法：基于随机游走；3）图级别表征方法，可以直接将节点表征向量聚合或者利用匿名游走。</p><h3 id="一-encoder-decoder-框架"><a class="markdownIt-Anchor" href="#一-encoder-decoder-框架"></a> 一. Encoder + Decoder 框架</h3><p>相较于传统的图学习方法，图表征学习省去了特征工程的步骤，直接自动学习图特征，之后应用于不同下游任务。</p><p>Efficient task-independent feature learning for machine learning with graphs. Encode nodes so that similarity in the embedding space (e.g., dot product) approximates similarity in the graph.</p><p>重点在于定义：1）什么是图上的节点相似性；2）节点到向量的映射函数。</p><p>一个最简单的encoder（shallow encoder）就是对于embeddings的查询。我们可以去直接优化每个节点的表征向量。但如果是大图的话，这个方法会很慢，因为嵌入矩阵Z会有很多列，要针对每一个进行表征向量估计。</p><div align="center">  <img src="/2021/05/05/cs224w/embedding.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>如何衡量图中节点相似度？这也是不同算法工作中比较大的差异点。大多流行方法会使用<strong>随机游走</strong>。</p><p>节点表征方法的特点：</p><ul><li>这类方法属于无监督或自监督学习，并没有利用到标签信息</li><li>没有利用节点属性，方法的目标是表征向量体现出图结构信息</li><li>表征结果是独立于下游任务的</li></ul><h3 id="二-基于random-walk的节点表征方法"><a class="markdownIt-Anchor" href="#二-基于random-walk的节点表征方法"></a> 二. 基于Random Walk的节点表征方法</h3><p>关于图中节点间的相似度衡量定义为两节点同时出现在图中统一随机游走记录中的概率。</p><p>使用随机游走的优势在于：1）可以同时考虑局部和高阶邻居信息；2）只需要考虑在随机游走中同时出现的节点对，而不是考虑图上的全部节点对。</p><div align="center">  <img src="/2021/05/05/cs224w/rw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>一般会在某些游走策略R下使用short fixed-length随机游走，收集某节点的邻居节点集合（此集合为multiset，即允许重复，因为某些节点可以被访问多次），之后调整嵌入向量z（参数）去优化最大似然概率。下图使用了softmax方法。</p><div align="center">  <img src="/2021/05/05/cs224w/rwe.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>但直接进行这样的计算代价很大，复杂度是图中节点数的平方。我们同样使用<strong>负采样（negative sampling）<strong>进行优化，只在负样本集合上计算而非在全部数据上计算。那么</strong>如何选取负样本</strong>呢？采样概率依照每个不同节点的度设定，共采样K个。K值越高得到的模型越鲁棒，但同时对负样本的偏向也越高，实际中<strong>一般选取K=5-20</strong>。</p><p>最后使用梯度下降方法优化即可。</p><p>最后一个问题是<strong>如何设定随机游走策略</strong>？最简单的方法是直接进行fixed-length, unbiased random walks（DeepWalk使用）。而node2vec认为更灵活地定义邻居节点可以获取包含信息量更大的节点表征，所以提出了biased 2nd order随机游走来生成节点邻居集合，这个方法可以综合权衡局部（BFS）和全局（DFS）信息。</p><div align="center">  <img src="/2021/05/05/cs224w/node2vec.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>因此node2vec提出return参数p（返回到之前的节点）和in-out参数q（BFS和DFS的比例）。关键在于<strong>记录了上一节点信息</strong>，对于处于某个节点的随机游走，下一节点有三种选择：1）退回上一节点；2）去和上一节点距离相同的节点；3）去距离上一节点更远的节点。</p><div align="center">  <img src="/2021/05/05/cs224w/pq.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>Node2vec算法总结如下，算法为<strong>线性复杂度</strong>，三个步骤可以并行处理。</p><ul><li>计算随机游走概率</li><li>对每个节点u模拟r次长度为l的随机游走</li><li>使用随机梯度下降法优化目标函数</li></ul><p>还有很多<strong>其它经典算法</strong>应用了不同的随机游走策略、优化策略和一些数据预处理技巧。</p><div align="center">  <img src="/2021/05/05/cs224w/others.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>截至目前，我们学习了三类图中<strong>节点相似度度量</strong>的方法：</p><ul><li>Naive：有连接的节点相似；</li><li>第二节中的Neighborhood overlap，即两节点间共有邻居情况</li><li>基于随机游走的节点向量表征</li></ul><p><strong>Must choose definition of node similarity that matches your application!</strong></p><h3 id="三-图表征"><a class="markdownIt-Anchor" href="#三-图表征"></a> 三. 图表征</h3><p>图或子图级别（即多个节点）的向量表征。</p><p>方法一：最简单是沿用节点向量表征方法，之后直接加和或平均，作为整个图的表征向量，虽然简单但是实际效果还不错。也可以使用层次聚类的方法逐步计算。</p><p>方法二：引入一个虚拟节点“virtual node”代表整个图/子图。</p><div align="center">  <img src="/2021/05/05/cs224w/subgraph.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>方法三</strong>：使用<strong>匿名游走（anonymous walks）</strong>，名字由来在于最终结果与具体图中节点信息无关。随着随机游走长度的增长，匿名游走数量呈指数型增长。可以按不同长度L匿名游走下不同游走类型的数量/概率作为表征向量。</p><div align="center">  <img src="/2021/05/05/cs224w/aw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>如何确定我们需要的匿名游走的数量？</p><div align="center">  <img src="/2021/05/05/cs224w/count_aw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>以同时出现在时间窗口T中的匿名游走为样本进行训练。这也是和DeepWalk之间的一大差异，即没有用节点集合左右邻居域。</p><div align="center">  <img src="/2021/05/05/cs224w/awe.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/05/cs224w/awg.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="pagerank一些链路分析方法"><a class="markdownIt-Anchor" href="#pagerank一些链路分析方法"></a> PageRank（一些链路分析方法）</h2><p>从矩阵角度进行图数据分析，由此可以进行：1）基于随机游走衡量节点重要程度（PageRank）；2）通过矩阵分解（Matrix factorization，MF）获取节点向量表征；3）将其它节点向量表征视为MF。</p><p><strong>Random walk，matrix factorization and node embeddings are closely related!</strong></p><div align="center">  <img src="/2021/05/05/cs224w/summary.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>之前的假设是把互联网中的（静态）网页视为节点，超链接视为边，去<strong>衡量网页的重要性</strong>。[ 虽然目前随着互联网的发展有了很多<strong>动态页面</strong>以及一些<strong>无法访问的生成页面</strong>。之前的网站链接多为navigational，而如今的更多是transactional。]</p><h4 id="1-pagerank"><a class="markdownIt-Anchor" href="#1-pagerank"></a> 1. PageRank</h4><p>将网页链接视为投票，使用in-links权衡，但每个连接的重要程度又不同，从而形成一个递归问题。</p><div align="center">  <img src="/2021/05/05/cs224w/page.jpg" srcset="/img/loading.gif" width="20%" height="20%" alt="oauth"></div><p>计算使用<strong>列随机矩阵M</strong>，最终得到<strong>秩向量r</strong>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>r</mi><mo>=</mo><mi>M</mi><mo separator="true">⋅</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">r = M · r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span></span>。可以将r视作<strong>随机游走</strong>收敛到的平稳分布，或者<strong>视为M的特征值为1对应的特征向量</strong>，即主特征向量。它是随机游走方程、基于流的方程式 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo separator="true">⋅</mo><mi>r</mi><mo>=</mo><mi>M</mi><mo separator="true">⋅</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">1·r = M · r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span></span>和线性代数的特征向量、特征值的完美融合。</p><p>[ 这里与前文中介绍的节点中心度中的<strong>eignvector centrality</strong>（针对无向图）和<strong>Katz centrality</strong>有一些梦幻联动。]</p><div align="center">  <img src="/2021/05/05/cs224w/pc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>使用幂迭代（power iteration）方法求解r，一般来讲迭代50次会大致收敛。[ 下图中左右两侧的迭代公式意义相同 ]</p><div align="center">  <img src="/2021/05/05/cs224w/pi.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>还有<strong>两个问题</strong>需要关注：</p><p>第一，死胡同问题（dead ends）[数学问题]，有的页面时没有out-link的。解决方法是，给没有out-link的页面所在的M列<strong>赋均值</strong>。</p><p>第二，蜘蛛陷阱（spider traps）[非数学问题]，有些页面相互连接，最终吸收了所有的“重要性”。解决方法是，引入参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>，表示继续沿着当前link游走的概率，而有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">1-\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>的概率**随机传送（teleport）**到任意页面，<strong>一般取值在0.8-0.9之间</strong>。</p><p>所以，Google的做法如下，或者写成矩阵形式<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mo>=</mo><mi>β</mi><mi>M</mi><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo><mo stretchy="false">⌈</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><msub><mo stretchy="false">⌉</mo><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msub><mo separator="true">,</mo><mi>r</mi><mo>=</mo><mi>G</mi><mo>∗</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">G = \beta M + (1-\beta) \lceil \dfrac{1}{N} \rceil_{N \times N},  r = G * r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">G</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mopen">⌈</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose"><span class="mclose">⌉</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">G</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span></span></p><div align="center">  <img src="/2021/05/05/cs224w/google.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>我们虽然从随机游走的角度来理解算法，但运行过程中并不实际进行游走，而是假定它游走了无限长时间。</p><h4 id="2-personalized-pagerankppr-random-walk-with-restarts"><a class="markdownIt-Anchor" href="#2-personalized-pagerankppr-random-walk-with-restarts"></a> 2. Personalized PageRank（PPR）&amp; Random walk with restarts</h4><p>PPR：在传送的时候不像PageRank那样概率传送到图中每个节点而是只取<strong>一个节点子集S</strong>。这个子集由之前的随机游走记录而得，每个节点的概率由访问次数决定。</p><div align="center">  <img src="/2021/05/05/cs224w/ppr.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>Random walk with restarts：将传送节点子集S缩小至单个节点，即总是传送回起始节点。</p><p>优势在于这类方法考虑了：1）一对节点间多种连接；2）多条路径；3）连接是否有向；4）路径中节点的度。</p><h4 id="3-矩阵分解与节点表征的关系"><a class="markdownIt-Anchor" href="#3-矩阵分解与节点表征的关系"></a> 3. 矩阵分解与节点表征的关系</h4><p>以“存在边连接”定义节点相似度的内积形式的解码器与邻接矩阵A的矩阵分解等价。</p><p>DeepWalk、node2vec等方法具有更为复杂的节点相似度定义（基于随机游走），它们等同于形式更为复杂的矩阵的矩阵分解。</p><p>下图为DeepWalk对应的矩阵形式：</p><div align="center">  <img src="/2021/05/05/cs224w/DeepWalk.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="4方法局限性"><a class="markdownIt-Anchor" href="#4方法局限性"></a> 4.方法局限性</h4><p>这类基于矩阵分解/随机游走的节点表征方法，如DeepWalk、node2vec等（PageRank也可以视为一维嵌入）有以下几点局限：</p><ul><li><p>无法得到新加入（未在训练集中出现过）的节点的表征向量；</p></li><li><p>无法捕获结构相似性（structurally similar），比如下图中节点1和节点11会有很不同的表征。如果使用匿名随机游走也许有提升；</p></li><li><p>无法利用节点、边或整个图的特征信息</p></li></ul><div align="center">  <img src="/2021/05/05/cs224w/limit.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>针对以上这些缺陷的解决方法是：Deep Representation Learning和Graph Neural Networks。</p><h2 id="课程作业借鉴"><a class="markdownIt-Anchor" href="#课程作业借鉴"></a> 课程作业借鉴</h2><h4 id="1可视化函数"><a class="markdownIt-Anchor" href="#1可视化函数"></a> 1.可视化函数</h4><pre><code class="hljs python"><span class="hljs-comment"># Helper function for visualization.</span>%matplotlib inline<span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> networkx <span class="hljs-keyword">as</span> nx<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-comment"># Visualization function for NX graph or PyTorch tensor</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">visualize</span><span class="hljs-params">(h, color, epoch=None, loss=None)</span>:</span>    plt.figure(figsize=(<span class="hljs-number">7</span>,<span class="hljs-number">7</span>))    plt.xticks([])    plt.yticks([])    <span class="hljs-keyword">if</span> torch.is_tensor(h):        h = h.detach().cpu().numpy()        plt.scatter(h[:, <span class="hljs-number">0</span>], h[:, <span class="hljs-number">1</span>], s=<span class="hljs-number">140</span>, c=color, cmap=<span class="hljs-string">"Set2"</span>)        <span class="hljs-keyword">if</span> epoch <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> loss <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:            plt.xlabel(<span class="hljs-string">f'Epoch: <span class="hljs-subst">&#123;epoch&#125;</span>, Loss: <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">.4</span>f&#125;</span>'</span>, fontsize=<span class="hljs-number">16</span>)    <span class="hljs-keyword">else</span>:        nx.draw_networkx(G, pos=nx.spring_layout(G, seed=<span class="hljs-number">42</span>), with_labels=<span class="hljs-literal">False</span>,                         node_color=color, cmap=<span class="hljs-string">"Set2"</span>)    plt.show()</code></pre>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络的局限</title>
    <link href="/2020/12/22/limit-graph/"/>
    <url>/2020/12/22/limit-graph/</url>
    
    <content type="html"><![CDATA[<p>后面计划进行有关图模型攻击方面的研究，学习斯坦福<a href="http://web.stanford.edu/class/cs224w/" target="_blank" rel="noopener">CS224W《图机器学习》</a>a&gt;和大佬Stephan Günnemann教授<a href="https://www.in.tum.de/daml/teaching/mlgs/ " target="_blank" rel="noopener">MLGS课程</a>中“Limitations of GNN”部分，记录如下。</p><p>关键点：</p><ul><li>图同构判断问题：单射，max/mean/sum pooling，WL Test</li><li>对抗攻击：Nettack，离散数据（无法直接梯度下降优化）、双层优化问题、如何对抗（certification）</li><li>Robutness and certification部分</li></ul><h2 id="mlgs"><a class="markdownIt-Anchor" href="#mlgs"></a> MLGS</h2><div align="center">  <img src="/2020/12/22/limit-graph/summary.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="一-表达能力"><a class="markdownIt-Anchor" href="#一-表达能力"></a> 一. 表达能力</h3><h4 id="1-图同构问题"><a class="markdownIt-Anchor" href="#1-图同构问题"></a> 1. 图同构问题</h4><p>如何判断两个图是否在结构上相同？此问题最优解最差时间复杂度呈指数形式。</p><p><strong>WL test</strong>（Weisfeiler-Lehman Test），只能得出“两个图同构或可能同构”的结论。</p><div align="center">  <img src="/2020/12/22/limit-graph/wl.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>在这个问题上GNN无法做到比WL test更好，尤其是它使用了非单射的聚合操作的时候更是无法区分图同构问题。</p><div align="center">  <img src="/2020/12/22/limit-graph/increase.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2-过平滑问题"><a class="markdownIt-Anchor" href="#2-过平滑问题"></a> 2. 过平滑问题</h4><p>随着层数增加GNN的预测结果过于平滑。无穷多层的GNN会导致所有的</p><p>节点得到同样的表征向量，这个向量表达了整个图的结构信息（和PageRank类似）而无法区分局部信息。</p><div align="center">  <img src="/2020/12/22/limit-graph/limit.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>关注一下<strong>PageRank</strong>。在PageRank里我们使用teleport vector进行信息局部化（关注邻居），同理可以应用到GCN场景中，相关工作为<strong>PPNP</strong>（Personalized Propagation of Neural Predictions，2018，建议阅读原文）。将转换与传播操作分开，并加入personalized teleportation，最终将迭代公式修改为：</p><div align="center">  <img src="/2020/12/22/limit-graph/shizi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>PPNP在防止过平滑、计算效率、扩展性等方面有如下优势：</p><div align="center">  <img src="/2020/12/22/limit-graph/ppnp.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-鲁棒性"><a class="markdownIt-Anchor" href="#二-鲁棒性"></a> 二. 鲁棒性</h3><p>有关图数据的对抗可以发生在<strong>节点属性</strong>和图<strong>结构信息</strong>两方面（后者在现实世界中更普遍），进行针对某些节点的<strong>有目标攻击</strong>或进行针对整个图的<strong>全局攻击</strong>。</p><p>图对抗攻击的<strong>挑战</strong>：</p><ul><li>针对离散变量的优化问题；通过非凸的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">L_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>范数计算扰动；</li><li>样本节点间相互关联，不可以单独计算；</li><li>如何定义“难以察觉”的扰动？</li><li>现实中多抽象为投毒攻击（影响训练数据集），抽象为一个双层优化问题。</li></ul><div align="center">  <img src="/2020/12/22/limit-graph/gongshi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div>最早的攻击为Nettack‘2018，目标是影响single node's prediction。关键操作在于首先将分类器线性化（为简化模型去掉了激活函数ReLU），之后通过贪心算法迭代找到最优扰动。<p>如何提升鲁棒性：</p><p>1）启发式防御方法：adjacency low-rank approximaition via truncated Singular Value Decomposition （Entezari 2020）; filtering of malicious edges via attribute similarity（Wu 2019）等，但这些方法在CNN领域已被证明无法应对最差情况的扰动。</p><p>2）鲁棒的训练方法，如 via Projected Gradient Descent（Xu et al，2019，但目前这种通过生成其它图样本的方法效果不是特别好）或者propose with a certification technique（low up bound，这个方面教授发表了很多论文）</p><ul><li>《Certifiable Robustness and Robust Training for Graph Convolutional Networks》</li><li>《[Certifiable robustness of graph convolutional networks under structure perturbations](javascript:void(0))》</li><li>《Certifiable Robustness to Graph Perturbations》</li></ul><p>3）随机平滑（randomized smoothing），如何在离散的图结构信息上加入高斯噪声？将邻接矩阵上的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>个边视为伯努利随机变量，但由于实际中的网络大多比较稀疏，很难找到一个合适的概率参数p。所以我们需要进行sparsity-aware random sampling。（这部分需要更详细得看一下）《Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more》‘ICML 2020</p><p>这方面的问题依然大有可为！（GNN robustness/certification is a highly active research area）</p><h3 id="三-扩展性"><a class="markdownIt-Anchor" href="#三-扩展性"></a> 三. 扩展性</h3><p>消息传递机制下需要同时处理整个网络，节点数据非独立同分布，动态增删节点/边会造成较大影响。</p><h2 id="cs-24w"><a class="markdownIt-Anchor" href="#cs-24w"></a> CS 24W</h2><h3 id="一-capture-graph-structure"><a class="markdownIt-Anchor" href="#一-capture-graph-structure"></a> 一. Capture graph structure</h3><p>Graph Isomorphism（图同构问题），邻居节点聚合函数（mean，max）并不单射。提出GIN（Graph Isomorphism Network），使用sum pooling。GIN可以更好地把握图结构信息，对于图分类问题表现更优秀，尤其是当网络中没有节点属性信息时。</p><p>GIN的思想与WL测试法近似。WL可以解决实际中的绝大多数图同构判断问题，但有一些例外，比如下面的例子：</p><div align="center">  <img src="/2020/12/22/limit-graph/except.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-vulnerability-of-gnns-to-noise-in-graph-data"><a class="markdownIt-Anchor" href="#二-vulnerability-of-gnns-to-noise-in-graph-data"></a> 二. Vulnerability of GNNs to noise in graph data</h3><p>以图上半监督节点分类问题为例，重点介绍了KDD18上Stephan Günnemann的工作，第一次提出该问题的数学模型并解答。解如下优化问题有两个难点：1）离散数据难以使用梯度下降；2）该问题为双层优化问题，如果使用迭代求解，每一步重新训练GNN非常耗时。作者为了保证高效，使用了很多启发式近似方法，比如贪心地一步步进行图修改，删除GCN中的ReLU激活函数进行简化等。（更多细节可以直接看论文，Adversarial Attacks on Neural Networks for Graph Data，PPT也做的很赞）。</p><div align="center">  <img src="/2020/12/22/limit-graph/attack.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><div align="center">  <img src="/2020/12/22/limit-graph/math.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><h3 id="三-challenges-and-future"><a class="markdownIt-Anchor" href="#三-challenges-and-future"></a> 三. Challenges and Future</h3><p>带标签数据集不容易获得（这是整个ML领域的问题），数据集不足又比较容易出现过拟合问题。为解决这个问题，提出Pre-training GNNs [Hu+ 2019]，先在某些相关数据集上训练之后，遇到真实任务再进行finetune。</p><p>如何防御上述类型对抗攻击？</p><p>攻击过程中如何在离散数据上找到最优解？</p><p>如何在准确性和鲁棒性之间找到最佳平衡？</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大规模图数据分析技术：挑战与机遇</title>
    <link href="/2020/11/16/talk-1/"/>
    <url>/2020/11/16/talk-1/</url>
    
    <content type="html"><![CDATA[<p>2020年11月16日，FIT楼多功能厅，听取樊文飞教授报告。以下为报告笔记，因为背景知识有限，听取现场报告，可能会有一些遗漏或理解错误之处。</p><p>当前图数据已成为大数据分析场景下的重要数据来源，老师在报告中就<strong>4V问题</strong>（Volume，Variety，Velocity，Veracity/value），分析图数据存储、分析中遇到的问题与部分解决思路。</p><h4 id="1volume体量"><a class="markdownIt-Anchor" href="#1volume体量"></a> 1.Volume（体量）</h4><p>大型图网络中包含百亿节点，百亿条边，如何存储处理？</p><p>DFS虽然是线性复杂度，但在大图上已经难以运行。</p><p>使用并行计算系统完成大图处理，在工业界存在很多问题：</p><p>1）已知的图算法能否并行化，学术界有很多这样的尝试比如google的pregel，CMU的Graphlab/PowerGraph，Facebook的Giraph，Berkeley的GraphX，IBM的Giraph++，但工业界未使用。樊老师团队有此类工作发表在SIGMOD，同时项目被阿里收购，今年开源为GraphScope。</p><div align="center">  <img src="/2020/11/16/talk-1/graphscope.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>2）并行计算中如何选择同步或异步，樊老师团队提出AAP工作，《Adaptive Asynchronous Parallelization of Graph Algorithms》</p><p>2）如何衡量某个并行算法是有效的？</p><p>3）传统计算复杂性理论在并行计算环境下有什么样的新表现？</p><h4 id="2velocity动态"><a class="markdownIt-Anchor" href="#2velocity动态"></a> 2.Velocity（动态）</h4><p>实际中的网络动态变化，但变化部分相对原网络占比较小，如何高效处理变化的信息？</p><p>计算output的变化。</p><p>1）很多被证明不是bounded的问题仍要解决，樊老师团队提出relative bounded，《Bounded incremental graph computations: Undoable and doable》。</p><p>2）如何提出增量式的算法，《Incrementalization of graph partitioning algorithms》VLDB 2020。</p><h4 id="3-variety异构"><a class="markdownIt-Anchor" href="#3-variety异构"></a> 3. Variety（异构）</h4><p>如何达成图数据库与原有关系型数据库的统一？比如阿里提到的数据中台概念。但这一方向距离落地还有很多工作要做。樊老师提出gSQL概念，以及联邦数据库（这里与联邦学习的概念不同，强调的是多类型数据的联邦）。</p><div align="center">  <img src="/2020/11/16/talk-1/gsql.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="4-veracity质量"><a class="markdownIt-Anchor" href="#4-veracity质量"></a> 4. Veracity（质量）</h4><p>存在过时数据、链接丢失与语义不一致等问题，如何解决？这个问题是4V中解决度最低的。</p><p>樊老师认为可以将logic规则与AI统一在一个框架下，同时还可以提高ML的可解释性。</p><div align="center">  <img src="/2020/11/16/talk-1/ml-1.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/11/16/talk-1/ml-2.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/11/16/talk-1/ml-3.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3><p>大数据场景下，图数据分析在4V中存在如下挑战：</p><ul><li>Volume，并行处理——reduction，complete problems等</li><li>Velocity，增量计算理论与实践</li><li>Variety，在SQL上做到图数据库与关系型数据库的统一</li><li>Veracity，统一规则（logic）与ML；图数据清洗；链路预测等</li></ul><h3 id="相关文献"><a class="markdownIt-Anchor" href="#相关文献"></a> 相关文献</h3><ol><li>Application driven graph partitions. SIGMOD 2020</li><li>Capturing associations in graphs VLDB 2020</li><li>Incrementalization of graph partitioning algorithms. VLDB 2020</li><li>Graph algorithms: Parallelization ans scalability. Science China Information Sciences, 2020</li><li>Adaptive asynchronous paralleization of graph algorithms, TODS2020</li><li>Deducing certain fixes to graphs, VLDB 2020</li><li>Parallelizing sequential graph computations TODS 2018</li><li>Incremental graph compulations: doable and undoable SIGMOD 2017</li></ol><h3 id="思考"><a class="markdownIt-Anchor" href="#思考"></a> 思考</h3><p>1）有关数据质量的部分，老师提到将规则与AI纳入统一框架，这个让我联想到网络安全领域目前基于规则和基于AI的方法，是否可以进行结合？比如Yara规则与CFG恶意软件检测等。这部分可以搜一下相关资料，樊老师并没有更详细地分享。</p><p>2）<strong>理论与系统</strong>是计算机专业领域的核心，要结合考虑理论研究的应用落实，同时学会如何在项目开发中发现待解决问题。</p><p>3）科研技术与产业落地之间还会有比如，政策、安全性、市场等多方面问题，需要综合考虑。比如XML并没有实现统一，比如在当前已有标准的情况下，为什么不能把关系型数据库全部转化为图数据存储。</p>]]></content>
    
    
    <categories>
      
      <category>交流报告</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
      <tag>大数据分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【小故事】无法消散</title>
    <link href="/2020/07/07/story-1/"/>
    <url>/2020/07/07/story-1/</url>
    
    <content type="html"><![CDATA[<p>下午四点半，9号楼的小洛死了。</p><p>一个女孩就这么从楼上摔下去，“肝脑涂地”，围观的人们在惋惜中带着阵阵恶心。</p><p>警队来了，120无聊地待在一边。</p><p>小洛家在十一楼，电梯坏了，郝队爬得气喘吁吁，“大夏天的，整这出？”</p><p>小洛和父母同住，郝队进门，家里一尘不染，物件规整，门口立着扫帚，卫生间里的拖布还没晾干。阳台的花鸟，墙上的字画，尤其沙发、床头随处可见的书籍，显示出这家人对文化生活的习惯，不是那种张扬在外的追捧。</p><p>“没有打斗的痕迹，看来是失足坠楼。”</p><p>小洛摔下去的地方正对着家里小阳台的窗户，窗页大敞着，郝队看了看，有齐腰高，如果不是故意爬上去，应该没可能发生意外。奇怪的是，周围窗台也一尘不染，连在附近活动过的痕迹都没有。如果是失足坠落，这窗户就显得过分冷静了。</p><p>“真会给我出难题。”</p><p>在屋内转了一圈，没获得什么实质性线索，郝队转过头来询问家人亲友。据了解，小洛是个成绩不错的学生，正在读博士，平时安安静静，不太可能涉及校园贷、勒索威胁等杂七杂八的事情。小洛有近期和同学出游的计划，前一天洛爸还听见她在电话上兴奋地讨论行程安排。听同学说，小洛最近有一篇论文在写，她还报名了半个月之后的一场线上比赛。哦，还有在小区附近的美容院里预约了两天后的祛痘清洁服务。</p><p>“你家孩子近期遇到什么事情了么？”，  “没有啊，疫情在家，每天平平淡淡的，哪有什么大事。” 洛妈已经哭晕了，都是洛爸在撑着回答。</p><p>“孩子人际交往怎么样？”</p><p>“性格有些内向，打小害羞，爱自己一个人玩，但长大就好很多，上学也结识了几个挺不错的朋友，这几天晚上还经常一起打游戏聊天呢。”</p><p>“那她平时生活状态怎么样？我看她是博士生，是不是课业压力挺大的？”</p><p>“刚读博的时候确实是，她老觉得毕业没希望，打电话回家也都挺沮丧的，后来发了两篇论文，就好很多了。我孩子不可能自杀，她最近也作息规律，偶尔锻炼，跟我俩聊天，都很好的，警官您可得仔细给查查啊，我们都配合，都配合。”</p><p>“不像啊”，郝队心想，“这感觉过得挺好。” 忽然，郝队看到垃圾桶里有个弯了的勺子，是平时做饭用的不锈钢厨具。要不是强外力，勺子不可能拧成这样。刚刚在书房里发现的塑料碎片，应该就是这勺子把手上掉下来的。郝队一阵激动，“这案子应该另有隐情。”</p><p>其实，这就是一场简单的自杀。</p><p>小洛是个理解力远超表达力几个维度的人，这样的人是孤独的，她在期盼与失落中交替，觉得没劲，就走了。</p><p>临走的几个小时前，又一次失落后，小洛很愤怒，刷碗时猛地把勺子砸向地板。看着弯折的勺子和四散的勺柄，她觉得挺好笑的。小洛有很多年，或者甚至说是从小，就不会生气，她好像总能站过去理解对面的逻辑，然后承认现实。但承认现实，安慰不了自己。因为无法被理解，所以时常失落，又因为能理解，所以她的失落没有焦点。</p><p>不如算了吧，通过模仿别人而产生的烟火气，总也不能落地，搞得大家都麻烦。后来她想到，家里人都爱干净，就一一收拾好，还彻底打扫了卫生。以往这种时候，洛妈回来都会表扬几句，小洛听着，觉得“你开心就好”。</p><p>窗台上，小洛一边擦去最后的痕迹，一边记起初中时，在思想政治书上背过，“热爱生活，珍惜生命，回报父母，贡献社会”。真没办法，对不起当时考出的98分。下坠时，她记起，之前和同学讨论起生命的意义、自杀等问题，同学说，“死不死得无所谓，走之前可以把角膜啥的这些器官，捐献给那些想活着的人”。</p><p>我走了，就任你们处置了，最好可以尽快消散掉。不过，好像没大可能，而且又要为当代青年抹黑了，真是不好意思。</p><p>“现在这些年轻人啊，生活条件那么好了，蜜罐里泡大，心理素质就是差。”</p>]]></content>
    
    
    <categories>
      
      <category>人文艺术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>瞎写</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>这个博客的搭建过程</title>
    <link href="/2020/06/01/build-blog/"/>
    <url>/2020/06/01/build-blog/</url>
    
    <content type="html"><![CDATA[<p>本文重点介绍基于Hexo+Github搭建个人网站流程，最初基本源自<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog" target="_blank" rel="noopener">这篇文章</a>（如何使用Github从零开始搭建一个博客），作者把步骤已经介绍得非常详细完善了。我只是补充踩到的一些坑….</p><p>后来，我背弃了极简审美，开始使用<strong>Fluid</strong>主题。</p><h3 id="一-基础搭建"><a class="markdownIt-Anchor" href="#一-基础搭建"></a> 一. 基础搭建</h3><p>搭建过程使用的两个最基本、最重要的东西是Hexo和Github，其中前者是一个轻量级博客框架，支持将Markdown编写的文章直接编译为静态网页文件并发布，省去了数据库问题。Github则用来解决域名问题，其Github Pages允许每个用户创建一个名为{username}.github.io的仓库，发布博客网页。当然也可以自己申请域名，使用CNAME跳转。</p><h4 id="1-github创建仓库"><a class="markdownIt-Anchor" href="#1-github创建仓库"></a> 1. Github创建仓库</h4><p>在Github上创建一个名为{username}.github.io的仓库，注意必须是github.io结尾。比如我的github账户为“DeepDeer”，创建仓库为“<a href="http://deepdeer.github.io" target="_blank" rel="noopener">deepdeer.github.io</a>”。另外，申请对应仓库时不要弄成private的，否则开放博客Github要收费哈。</p><h4 id="2-安装环境"><a class="markdownIt-Anchor" href="#2-安装环境"></a> 2. 安装环境</h4><p>首先在自己电脑上安装Node.js，确保环境变量配置好，可以使用npm命令；</p><p>其次使用npm命令安装Hexo，安装后确保可以使用<code>hexo</code>命令。</p><pre><code class="hljs bash">npm install -g hexo-cli</code></pre><h4 id="3-初始化项目"><a class="markdownIt-Anchor" href="#3-初始化项目"></a> 3. 初始化项目</h4><p>选定存储博客文件的位置，在此文件夹中使用如下命令创建项目及对应文件夹：</p><pre><code class="hljs bash">hexo init &#123;name&#125;</code></pre><p>命令下产生的文件夹包括themes、source等文件夹，调用如下命令，则在public文件夹中生成js、css、font等内容。</p><pre><code class="hljs verilog">hexo <span class="hljs-keyword">generate</span></code></pre><p>使用server命令在本地运行博客，可以看到类似结果：</p><pre><code class="hljs routeros">hexo<span class="hljs-built_in"> server </span> #或简写为 hexo s</code></pre><div align="center">  <img src="/2020/06/01/build-blog/hello.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/hexo.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><h4 id="4-部署至github"><a class="markdownIt-Anchor" href="#4-部署至github"></a> 4. 部署至Github</h4><p>安装一个支持Git的部署插件</p><pre><code class="hljs sql">npm <span class="hljs-keyword">install</span> hexo-deployer-git <span class="hljs-comment">--save</span></code></pre><p>修改Hexo的配置文件_config.yml，找到Deployment部分，修改为如下内容：</p><pre><code class="hljs bash"><span class="hljs-comment"># Deployment</span><span class="hljs-comment">## Docs: https://hexo.io/docs/deployment.html</span>deploy:  <span class="hljs-built_in">type</span>: git  repo: git@github.com:DeepDeer/deepdeer.github.io <span class="hljs-comment">#你自己的Github仓库地址</span>  branch: master</code></pre><p>使用deploy命令部署后，可通过域名deepdeer.github.io访问，Github上传代码如下：</p><pre><code class="hljs ebnf"><span class="hljs-attribute">hexo deploy</span></code></pre><div align="center">  <img src="/2020/06/01/build-blog/github.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p><a href="http://xn--deploy-hp7ik1vdf32vsxrqigxw5bw73eklg.sh" target="_blank" rel="noopener">可编写如下内容脚本deploy.sh</a>，此后每当有内容更新时，<code>.deploy.sh</code>运行脚本即可。</p><pre><code class="hljs bash">hexo cleanhexo generatehexo deploy</code></pre><h3 id="二-加入主题"><a class="markdownIt-Anchor" href="#二-加入主题"></a> 二.  加入主题</h3><p>目前我加入的是Fluid主题（因为看上了颜值），之前用过一段时间Next主题，也很推荐。</p><h4 id="1-next主题"><a class="markdownIt-Anchor" href="#1-next主题"></a> 1. Next主题</h4><p>有关Next主题的配置及各种插件，在<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog" target="_blank" rel="noopener">这篇文章</a>中介绍地非常详细，这里只补充有关1）添加Gitalk插件和2）修改字体部分。</p><h5 id="1-gitalk插件"><a class="markdownIt-Anchor" href="#1-gitalk插件"></a> 1&gt; Gitalk插件</h5><p>申请Gitalk就在Github个人账户的settings——&gt; Developer settings ——&gt; OAuth Apps，点击 New OAuth App，出现申请界面。其中应用名称随便写就行，Hompage URL和Authorization callback URL写博客链接。如果有自己的域名可以更改Authorization callback URL。点击注册，生成Client ID和Client Secret。</p><p>注意，如果自己配置了域名，这个callback URL要改成自定义域名</p><div align="center">  <img src="/2020/06/01/build-blog/oauth.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>在配置了_config.yml文件后，第一次进入界面会出现下图效果。如果点击Github登录后跳转到了404界面，那么, 就说明配错了。我当时是在写_config.yml忘了把client_id, client_secret字段带的{ }去掉。这给我一顿google啊…</p><div align="center">  <img src="/2020/06/01/build-blog/begin.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>最后效果就像这个博客里一样，相关评论会显示在对应仓库的issues里，记得在仓库的settings里把features—&gt;issues勾选上（貌似默认就是开启的）</p><div align="center">  <img src="/2020/06/01/build-blog/comment.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/issues.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><h5 id="2-修改字体"><a class="markdownIt-Anchor" href="#2-修改字体"></a> 2&gt; 修改字体</h5><p>Next主题默认的博文正文字体大小有点大了，可以在配置文件里改一下。相关配置在hexo\themes\next\source\css\variables路径下的base.styl文件里的Font Size部分。这里面每个变量控制某一部分的字体大小，我是挨个试出来的font-size-large是正文字体（简单粗暴，真开心…)</p><div align="center">  <img src="/2020/06/01/build-blog/font.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>另外，在Hexo配置文件和Next主题配置文件中，都有一些有关网站信息的配置选项，最终使用Next主题搭建出的网站效果如下：</p><div align="center">  <img src="/2020/06/01/build-blog/next.jpg" srcset="/img/loading.gif" width="70%" height="50%" alt="oauth"></div><h4 id="2fluid主题"><a class="markdownIt-Anchor" href="#2fluid主题"></a> 2.Fluid主题</h4><p>其实这个主题有非常好的<a href="https://fluid-dev.github.io/hexo-fluid-docs/guide/" target="_blank" rel="noopener">配置指南</a>，其配置文件_config.yml的注释也很清晰，可以从头摸索。有几点经验包括1）图片插入；2）评论插件。</p><h5 id="1-图片插入"><a class="markdownIt-Anchor" href="#1-图片插入"></a> 1&gt; 图片插入</h5><p>需要注意的一点是，在Fluid主题下有文章背景图等存在于框架中的图片，这些图片一律存放在<code>./themes/fluid/source/img</code>文件夹下。即使是某个文章的缩略图也是这样。</p><div align="center">  <img src="/2020/06/01/build-blog/pic.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>其他存在于文章中的图片，可用如下形式添加。</p><p>首先，把_config.yml文件里的post_asset_folder选项设置为true。</p><p>其次安装一个插件，据说原有插件有一些bug，下面是修改过的插件，亲测有效，感谢<a href="https://www.jianshu.com/p/3db6a61d3782" target="_blank" rel="noopener">这篇博客</a></p><pre><code class="hljs vim">npm install http<span class="hljs-variable">s:</span>//github.<span class="hljs-keyword">com</span>/<span class="hljs-number">7</span>ym0n/hexo-asset-image --<span class="hljs-keyword">sa</span></code></pre><p>有了这些配置后，再运行hexo new xxx，在/source/_posts/路径下，除了可以生成新文章xxx.md之外，还生成一个同名文件夹。插入图片时放到这个文件夹里即可，在markdown里用如下语句：</p><pre><code class="hljs routeros">&lt;img <span class="hljs-attribute">src</span>=<span class="hljs-string">"xxx/图片名称.png"</span> <span class="hljs-attribute">alt</span>=<span class="hljs-string">"图片标识"</span> <span class="hljs-attribute">style</span>=<span class="hljs-string">"zoom:30%;"</span> /&gt;</code></pre><p>但是，在Fluid主题下，这些图片并没有默认居中，可以采用如下HTML代码控制位置和大小：</p><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">align</span>=<span class="hljs-string">center</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">img</span> <span class="hljs-attr">src</span>=<span class="hljs-string">"build-blog/pic.jpg"</span> <span class="hljs-attr">width</span>=<span class="hljs-string">"30%"</span> <span class="hljs-attr">height</span>=<span class="hljs-string">"30%"</span> <span class="hljs-attr">alt</span>=<span class="hljs-string">"oauth"</span>  /&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span></code></pre><h5 id="2评论插件"><a class="markdownIt-Anchor" href="#2评论插件"></a> 2&gt;评论插件</h5><p>Fluid推荐的utteranc.es插件，经常会有加载比较慢的问题，乍一看以为不让评论…</p><p>这个配置过程也很简单。</p><p>首先在github创建一个公开的仓库，比如命名为’deepdeer_comments’。</p><p>点击<a href="https://github.com/apps/utterances" target="_blank" rel="noopener">这个链接</a>安装应用，选择“only select repositories”选项，找到刚刚建立好的仓库，点击install。</p><p>在配置中填写repo名，格式为“用户名/仓库名”，如“DeepDeer/deepdeer_comments”。Issue的命名方式建议选择第一个“Issue title contains page pathname”。</p><p>根据个人喜好选择主题之后，最后一栏会自动生成配置信息，复制这些信息。</p><p>在fluid主题的配置文件中，找到<code>comments</code>部分，将enable设置为true，并将type写成utterances。</p><p>在后面的comments具体配置部分，改成之前自动生成的配置。</p><div align="center">  <img src="/2020/06/01/build-blog/utter.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p>这个博客目前用的是Gitalk插件，配置与Next主题中提到的大致相同。Fluid代码中该插件配置有问题，评论无法分页显示，感谢<a href="https://juejin.im/post/5ed177e36fb9a047923a39fe" target="_blank" rel="noopener">这篇文章</a>。即更改fluid主题下的<code>layout/_partial/comments/gitalk.ejs</code>文件内容中的’id’一栏部分</p><pre><code class="hljs python"><span class="hljs-comment">#原有的</span>id: <span class="hljs-string">'&lt;%- md5(theme.gitalk.id) %&gt;'</span>,<span class="hljs-comment">#改正后</span>id: &lt;%- theme.gitalk.id %&gt;,</code></pre><h3 id="三-自定义域名"><a class="markdownIt-Anchor" href="#三-自定义域名"></a> 三. 自定义域名</h3><p>本博客使用了阿里云上购买的域名。</p><p>在<a href="https://wanwang.aliyun.com/domain/searchresult/?keyword=skylasun&suffix=.cn#/?keyword=skylasun&suffix=cn" target="_blank" rel="noopener">这里</a>点击“控制台”，登录后，找到边栏中的“域名”。选择“域名注册”</p><div align="center">  <img src="/2020/06/01/build-blog/domain.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/reg.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p>进入后，查询你喜欢的关键字相关的域名的注册情况，选择中意的域名就可以交钱了。最终付款之前还需要一些身份认证。</p><div align="center">  <img src="/2020/06/01/build-blog/buy.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>购买成功，认证通过后，在已有域名那里，点击“解析”，添加解析规则。这里添加的IP地址是之前deepdeer.github.io的解析情况，可以通过各类IP或域名查询网站找到，比如<a href="https://site.ip138.com/" target="_blank" rel="noopener">这里</a>。注意下面要加上一条CNMA规则。</p><div align="center">  <img src="/2020/06/01/build-blog/map.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/ip.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>另外，在Github仓库的Settings里，需要加上“Custom domain”，保存配置后，会自动生成名为CNAME的文件，内容如下。但需要注意的是，每次我们重新部署时，使用deploy clean再generate后，会清除掉这个CNAME文件。为解决这个问题，可以把CNAME文件放到博客的“source”文件夹中。</p><div align="center">  <img src="/2020/06/01/build-blog/cname.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><h3 id="四-其它小经验"><a class="markdownIt-Anchor" href="#四-其它小经验"></a> 四. 其它小经验</h3><h4 id="1markdown编辑器推荐"><a class="markdownIt-Anchor" href="#1markdown编辑器推荐"></a> 1.Markdown编辑器推荐</h4><p>这些博客都是用<a href="https://www.typora.io/" target="_blank" rel="noopener">Typora</a>写的。该软件界面简洁，即时效果，很好用，推荐~</p><p>Typora除了支持公式块之外，还支持行内公式，在偏好设置中勾选”内联公式“即可。</p><div align="center">  <img src="/2020/06/01/build-blog/gongshi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2markdown中内容折叠"><a class="markdownIt-Anchor" href="#2markdown中内容折叠"></a> 2.Markdown中内容折叠</h4><p>有时文章内容过多不便于显示，可以使用如下语法进行折叠</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">details</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">summary</span>&gt;</span>可显示的标题<span class="hljs-tag">&lt;/<span class="hljs-name">summary</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">pre</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">code</span>&gt;</span>   折叠内容  <span class="hljs-tag">&lt;/<span class="hljs-name">code</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">pre</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">details</span>&gt;</span></code></pre><p>比如</p><div align="center">  <img src="/2020/06/01/build-blog/zhedie.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>效果如下，点击后显示黑色部分</p><div align="center">  <img src="/2020/06/01/build-blog/xiaoguo.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>如果有其它的坑，欢迎大家评论补充，谢谢！</p>]]></content>
    
    
    <categories>
      
      <category>教程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>工程技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>THU研究生国际会议出行准备流程</title>
    <link href="/2020/05/30/baoxiao/"/>
    <url>/2020/05/30/baoxiao/</url>
    
    <content type="html"><![CDATA[<p>下文仅限清华大学网络科学与网络空间研究院研究生同学使用，包含护照、签证、报销等</p><blockquote><p>提示：出国手续涉及部门较多，请尽早准备提前办理。如遇假期会有所调整，要关注邮件通知~</p></blockquote><h3 id="首先"><a class="markdownIt-Anchor" href="#首先"></a> 首先</h3><p>​你要有个<strong>护照</strong>，如果没有，办理的时候把发票留好，可以报销。</p><h3 id="前期准备"><a class="markdownIt-Anchor" href="#前期准备"></a> 前期准备</h3><p>注册会议并拿到<strong>邀请函</strong></p><p>预定<strong>机票和酒店</strong>（办签证使用）。目前携程等网站貌似不再支持付款前先打印行程单。</p><ol><li>机票时间，一般可定在会议安排往前往后各一天。有特殊情况，赶在自己文章汇报前到达即可。</li><li>酒店一般订会议推荐的，预定前注意一下学校给的当地住宿报销额度。有些会议官网会贴出提前订酒店有优惠的通知，发邮件过去即可。</li></ol><h3 id="学校审批"><a class="markdownIt-Anchor" href="#学校审批"></a> 学校审批</h3><h4 id="1申请出国批件"><a class="markdownIt-Anchor" href="#1申请出国批件"></a> 1&gt;申请出国批件</h4><ol><li>首先在info上进行申请，找“出国出境申报”——&gt;“因公出国（境）申报系统（新）</li></ol><div align="center">  <img src="/2020/05/30/baoxiao/apply.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><ol start="2"><li>进入系统后，选择”新申请”，点击“我已阅读”，在因公出境申请表上填写信息，其中会<strong>比较犹豫的几个字段</strong>有：</li></ol><ul><li>出访基本信息：出入境时间大概在会议日程往前往后各一天，离境、入境时间是否需要过境等如实填写即可</li><li>出访类别：单位公派，会议</li><li>出访经费：费用来源一般选择“全部校内支付”，“纵向科研经费”，校内支付，人民币（大致写一个费用即可）</li><li>日程计划：简单填写就行，比如出发，抵达，开会，回程等等（可适当扩展）</li></ul><ol start="3"><li><p>提交之后会有一个预算表，大概可以看到给当地的住宿、日常消费额度等。这类信息也可以在边栏中“预算、外汇与报销”的“政策与标准”的表格中看到。</p><div align="center">  <img src="/2020/05/30/baoxiao/biaozhun.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div></li><li><p>“报批材料”里上传会议的邀请函和论文录用证明。</p></li><li><p>点击提交，打印申请表，这个表需要自己和导师签字。</p></li><li><p>提交完成后，返回主界面会显示出当前进度，完成后圆圈会变绿。大概两周左右“单位审核”会变绿。等到“学校审批”通过，显示批件下达之后，可以下载电子版。</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/jindu.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div><h4 id="2批件领取"><a class="markdownIt-Anchor" href="#2批件领取"></a> 2&gt;批件领取</h4><p>去国际处，在李兆基4楼（可以进楼的门有点多，但失之毫厘谬以千里，所以可以问下保安…）</p><p>去之前先准备一份**“派出证明”<strong>。还是在刚刚的出入境申请系统的边栏里面。点击进去下载对应模板，注意老师们已经用最直接醒目的方法标示出的</strong>注意事项**。</p><div align="center">  <img src="/2020/05/30/baoxiao/chat.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div><div align="center">  <img src="/2020/05/30/baoxiao/chats.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><div align="center">  <img src="/2020/05/30/baoxiao/attention.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>在国际处主要有以下<strong>几个事情</strong>：</p><ul><li>拿批件</li><li>拿外汇预算单</li><li>派出证明需要盖章</li><li>如果办签证时需要单位法人证明一类的材料，需要<strong>主动</strong>和老师提及</li></ul><h3 id="签证办理"><a class="markdownIt-Anchor" href="#签证办理"></a> 签证办理</h3><p>这个就要看去哪个国家了，我以希腊为例，需要申根签，可以先在官网上填写表格申请，然后按照里面写的去依次准备材料。去使馆办事处。一定要注意时间，选最最最是工作时间的时段过去。我第一去的时候好像是下午3点左右到的，说是刚刚停止办理…</p><p>其它细节事项：</p><ol><li><p>保险可以直接在淘宝上买，搜“申根保险”就可以，看清楚额度是否符合要求；</p></li><li><p>户口页，如果户口在学校的话，直接在info上申请，”集体户口卡借阅”，里面包括“借阅预约”和“首页打印”。预约之后直接去地图里圈出的小房子（保卫处）里拿就好了；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/hukou.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="3"><li><p>银行对账单可以直接在C楼打印；</p></li><li><p>在读证明，在info上预约然后直接与三教打印（貌似改到了六教？，反正C楼应该都是万能的）；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/zaidu.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="5"><li>办理签证最后需要交纳现金。留好发票，这个在报销范围内。</li></ol><h3 id="出行及报销"><a class="markdownIt-Anchor" href="#出行及报销"></a> 出行及报销</h3><ol><li><p>行程中尽量保存好<strong>所有票据</strong>，回来整理<strong>报销</strong>。（我都是先垫付再报销，据说还可以先去学校<strong>借款</strong>）</p><p>各类车票，登机牌，行程单，机票购买记录及发票（让网站寄过来），酒店账单/发票，会议注册费发票等</p></li><li><p>去首都机场的话，清华科技园那里有大巴，车费是30块？这种貌似属于城建交通，也可以报销，不行的话，也有日常杂费可以cover掉。</p></li><li><p>回来后的报销主要是填写一个报销表格，还是在刚刚的出入境申报系统的边栏上的“表格下载”里，选择**”报销表格下载“**，表格如下图，里面也标明了一些报销流程和注意事项；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/baoxiaochat.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><div align="center">  <img src="/2020/05/30/baoxiao/items.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="4"><li><p>其它细节事项</p><ul><li>大额机票需要发票验真，通过官网或发票自带的网站都可以，截图打印</li><li>打印护照的出入境记录页</li><li>提供交易记录截图（微信通知，短信账单，订单等均可）</li><li>在发票上签字需要用<strong>油笔</strong></li></ul><p>我们组的报销可以去对门实验室请教<strong>乔老师</strong>，老师会给予很多帮助，在此表示感谢~</p></li></ol><p>本文凭借对半年前的回忆整理，如有疏漏，欢迎大家评论指正！</p>]]></content>
    
    
    <categories>
      
      <category>办公事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
