<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>网络安全基础</title>
    <link href="/2021/06/22/security-base/"/>
    <url>/2021/06/22/security-base/</url>
    
    <content type="html"><![CDATA[<h1 id="网络安全应用技术与工程实践"><a class="markdownIt-Anchor" href="#网络安全应用技术与工程实践"></a> 网络安全——应用技术与工程实践</h1><p>北京理工大学闫怀志老师的在线课程（<a href="https://www.xuetangx.com/course/bit08091002817/5884048?channel=learn_title%EF%BC%89%E3%80%82" target="_blank" rel="noopener">https://www.xuetangx.com/course/bit08091002817/5884048?channel=learn_title）。</a></p><h2 id="一背景"><a class="markdownIt-Anchor" href="#一背景"></a> 一.背景</h2><p>信息是用来消除不确定性的东西（香农），是人们在适应外部世界并使这种适应反作用于外部世界的过程中同外部世界进行互相交换的内容和名称（控制论创始人维纳）。现代科学认为信息是对世界各种事物的运动状态和变化的反映。</p><p>信息具有可识别性、可传载性、可共享性和可度量性。</p><p>信息系统的<strong>计算模式</strong>，从集中计算模式、分布式计算模式再到云计算模式，计算架构包括单机、C/S、B/S、C/S和B/S混合、P2P、多层、SOA架构、微服务架构等。</p><p><strong>安全领域CIKW模型</strong>，底层为安全相关数据（实体环境采集的原始素材），到安全相关信息（加工处理后的逻辑数据），进一步到安全相关知识（提炼信息之间的关联），最后到智能（前瞻性预测与决策支撑）。</p><p>网络空间（ITU，国际电信联盟）定义，是由计算机、计算机系统网络及其软件支持、计算机数据、内容数据、流量数据以及用户等上述全部或部分要素创建或组成的物理或非物理领域。是陆海空天之后的第五维空间。</p><p>以<strong>安全属性</strong>的不变应信息系统形态、处理技术和<strong>安全技术</strong>的万变。</p><p>信息安全属性包括：保密性（C）、完整性（I）和可用性（A）。扩展的安全属性还包括，不可否认性（non-repudiation）、可控性（Controllability）、可追溯性等。</p><p>网络空间安全领域牵涉到自然科学（数学、通信、信息、计算机等）和社会科学（法律、心理学、教育和管理等）两个方面。</p><p>关于网络空间安全的八个基本观点：相对安全、适度安全、木桶理论、安全困境（保密性vs可用性）、墨菲定律、蝴蝶效应、冰山原理和剃刀定律。</p><h2 id="二-安全技术体系"><a class="markdownIt-Anchor" href="#二-安全技术体系"></a> 二. 安全技术体系</h2><p>内部是信息系统的安全构建，包括网络、应用系统及实现，外部包括防火墙、入侵检测、态势感知等。</p><p>网络安全滑动标尺模型，由基础架构安全能力、被动防御能力、主动防御能力、智能分析能力到攻击反制能力。</p><p>安全三要素包括，资产（识别）、脆弱性（利用）和威胁（应对）。</p><h3 id="1-基本思想"><a class="markdownIt-Anchor" href="#1-基本思想"></a> 1. 基本思想</h3><p>基本思想，<strong>网络安全系统是一个复杂系统，需要采用系统工程方法</strong>。复杂性主要体现在层次性、不确定性、开放性、非线性、系统规模、主动性、智能性以及动态性等。当前发展体现在由<strong>基元性转向组织性</strong>（考虑相互的组织与协同）、线性转向非线性（关注关联、交互、涌现等问题）、简单性转向复杂性。</p><p>网络安全系统是一种复杂系统，网络安全风险是一种客观存在，需要采用系统思想来考察，涉及系统论、信息论、控制论、耗散结构理论、协同论与突变论等。系统工程方法包括霍尔三维（时间维、逻辑维、知识维）结构方法论、切克兰德方法论、从定性到定量的系统综合方法、基于模型的系统工程、体系工程等。</p><p>经典系统科学理论（老三论）包括系统论、信息论和控制论。系统论可以从宏观、中观、微观三个层面指导网络空间安全体系的构建。信息论的应用可包括密码学、信息隐藏、隐私保护等。控制论则可以指导安全控制、攻防对抗、防御构建等。</p><p>新兴系统科学理论包括耗散结构理论、协同论和突变论。耗散结构理论主要解释复杂系统的自组织运动规律，比如大规模网络故障预测与控制，舆情管理等；协同论研究相互协调、共同作用，构建度量信息安全技术、管理各因素（子系统）之间协同程度的评价指标和评价模型；突变论是对量化和质变规律的深化，如网络安全突发事件的机理分析及应对等。</p><h3 id="2-滑动标尺模型"><a class="markdownIt-Anchor" href="#2-滑动标尺模型"></a> 2. 滑动标尺模型</h3><p>对于网络安全防御体系，国内提出P2DR、OODA、PDCA等动态循环模型。美国SNAS提出网络安全滑动标尺模型获得广泛应用。此模型将安全防御分为逻辑递进的五个阶段：基础架构安全、被动防御、主动防御、智能分析以及反制攻击。</p><div align="center">  <img src="/2021/06/22/security-base/slide.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="3-法律法规"><a class="markdownIt-Anchor" href="#3-法律法规"></a> 3. 法律法规</h3><div align="center">  <img src="/2021/06/22/security-base/law.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="三-基础架构安全概述"><a class="markdownIt-Anchor" href="#三-基础架构安全概述"></a> 三. 基础架构安全概述</h2><p>网络结构需要有合理的安全域划分，软件采用合理架构和技术体系。信息系统的实现可能会引入缺陷和漏洞，在基础实现层面应该尽力避免。</p><div align="center">  <img src="/2021/06/22/security-base/base.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="1-物理与环境安全"><a class="markdownIt-Anchor" href="#1-物理与环境安全"></a> 1. 物理与环境安全</h3><p>物理位置选择，物理访问控制，防盗窃和防破坏，防雷击，防火，防水和防潮，防静电，温湿度控制，电力供应和电磁防护。</p><p><strong>扩展要求</strong>：针对云计算安全、移动互联安全、物联网安全、工业控制系统安全等网络空间信息系统新形态新应用有特殊保护要求。</p><p>云计算安全，保证云计算基础设施、云服务客户数据、用户个人信息等存储于中国境内；</p><p>移动互联安全，选择合理位置，避免过度覆盖和电磁干扰；</p><p>物联网安全，感知节点设备物理环境不对其造成物理损害，能正确反映环境状态，不对其正常工作造成影响。关键网关节点、感知节点有可供长时间工作的电力供应。</p><p>工业控制安全，室外防火、透风、防盗、防雨、散热；远离强电磁干扰，远离强热源等；</p><h3 id="2-网络与主机安全"><a class="markdownIt-Anchor" href="#2-网络与主机安全"></a> 2. 网络与主机安全</h3><p><strong>网络安全</strong>重点是针对通信网络的安全控制，对象是广域网、城域网和局域网等，涉及网络设备的业务处理能力、网络带宽、网络区域划分及通信线路等。</p><div align="center">  <img src="/2021/06/22/security-base/device.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>第一，合理选用网络组建设备。</p><p>第二，合理划分网络区域，实施不同安全策略。</p><p>第三，规划IP地址分配策略、路由、交换策略，可根据情况选用静态分配地址、动态分配地址、NAT措施等。</p><p>第四，实现网络线路和设备的冗余配置，确保系统可用性。</p><p>第五，网络边界部署安全设备，规划具体位置和部署措施，部署网络安全审计系统。</p><p>第六，设计远程安全访问系统，部署IPsec安全通信设备，部署SSL，VPN安全设备</p><p>扩展要求：</p><p>云计算要实现客户虚拟网络的隔离，为客户提供定制、自主式的安全能力。</p><p>工业控制系统内部根据业务特点划分为不同安全域并采取隔离，涉及<strong>实时</strong>传输和数据传输的，应独立组网并与其它网络实现物理安全隔离。</p><p><strong>主机安全</strong>主要是操作系统安全和数据库安全。</p><p>操作系统安全涉及自身安全机制、安全配置及安全使用等。考虑三个方面：1）资源利用、数据传输和客体重用；2）密码支持、安全功能保护、访问控制、身份鉴别、审计、数据完整性等；3）安全管理、生存周期支持、配置管理、脆弱性（漏洞）管理等。</p><p>数据库安全包括自身安全和数据安全。保护数据库系统、服务器和数据库中的数据、应用、存储以及相关网络连接。防止数据库系统及数据遭到泄露、篡改或破坏。</p><h3 id="3-加密认证技术"><a class="markdownIt-Anchor" href="#3-加密认证技术"></a> 3. 加密认证技术</h3><h4 id="基础"><a class="markdownIt-Anchor" href="#基础"></a> 基础</h4><p>涉及密码编码学与密码分析学。</p><p>古代加密方法阶段，人工方式加密，比如“藏头诗”。</p><p>古典密码阶段，纯机械或者电子机械方式加密，比如单标代替密码、多表代替密码、转轮密码等。</p><p>近现代密码阶段，采用计算机等先进计算手段作为加密工具。1949年香农发表《保密系统通信原理》，1976年发表《密码学的新方向》。</p><div align="center">  <img src="/2021/06/22/security-base/encrypted.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/clas.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>主要用到数学（概率论、统计、数论、有限域理论）、信息论（自信息、条件自信息、互信息、熵、条件熵、联合熵）、计算复杂性理论（时间复杂性、空间复杂性、复杂性渐进表示、NP难、NPC等）等。</p><p>设计准则包括，计算安全性（原理上可通过计算破解）、可证明安全性（理论证明算法安全性）、无条件安全性（原理上不可破译，基本只有一次一密能达到）。</p><p>良好的密码系统有三个条件：1）给定加密算法、明文、秘钥，容易得到密文，反之亦然；2）解密算法、解密秘钥未知，不能推明文；3）密文不应使别人看起来异样。</p><p>柯克霍夫斯密码设计原则：1）易于计算；2）难于破解；3）一切秘密寓于秘钥；4）整体安全原则。</p><div align="center">  <img src="/2021/06/22/security-base/save.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="对称密码-vs-非对称密码"><a class="markdownIt-Anchor" href="#对称密码-vs-非对称密码"></a> 对称密码 VS 非对称密码</h4><p>对称，即加密解密秘钥相同，或者二者可以简单推算导出。</p><p>对称密码中，秘钥的保密程度决定了体制的安全性。密钥管理的高难度和复杂性。</p><p>对称密码包括<strong>序列密码（流密码）<strong>和</strong>分组密码（块密码）</strong>。</p><p>序列密码起源于香农证明的绝对安全的一次一密，对明文逐字符或逐位加密，加密变换随时间而变。算法要素包括明文、密文、密钥（种子密钥与密钥流）、密钥流字母表、加密算法、解密算法。典型算法有RC4（字节流方式）和A5（密钥流位）。</p><p>优点在于处理速度快、误差传播少，缺点在于扩散（扰乱及混淆）不足，对插入及修改不敏感。</p><div align="center">  <img src="/2021/06/22/security-base/seq.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>分组密码采用软件加密，避免了对硬件的依赖。一次加密明文中的一个组（通常为64或128），加密输出的每一位数字与一组长为m的明文数字有关。算法要素包括明文、密文、密钥、加密算法、解密算法。分组密码包括DES（传统换位和置换方法，64位）、3DES（提升安全性做了扩展，但效率极低）和<strong>AES（代换和混淆方法，安全性高、计算效率高、实现性好、灵活性强，是目前常用的对称加密算法）</strong>。</p><p>优点在于扩散性好、对插入敏感，缺点是处理速度慢、误差易传播。</p><div align="center">  <img src="/2021/06/22/security-base/group.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/work.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>非对称密码体制包括公钥和私钥，也叫作公钥密钥体制，起源于1976年《密码学新方向》。公钥加密的数据只有私钥可解，反之亦然。极大方便了密钥管理。常用算法分为大整数分解类（RSA）、离散对数问题类（Diffie-Hellman）和椭圆曲线类（ElGamal）。</p><p>通过<strong>单项函数、单向陷门函数</strong>两类正向易于计算，反向难以计算的函数实现公钥密码构造。</p><blockquote><p>单向函数：难以根据输出推算输入，即正向计算容易但求逆计算不可行。</p><p>单向陷门函数：如果给定某些额外数据（陷门）容易计算单向函数的逆函数，陷门为解密秘钥。</p></blockquote><p>日常对称密码和非对称密码会<strong>组合使用</strong>，实现加密强度、运算效率和密钥分配管理的平衡。一般采用对称密码进行数据加密，<strong>用非对称密码对对称密码的加密秘钥加密</strong>。核心、关键的机密数据依然用非对称加密。</p><h4 id="摘要算法"><a class="markdownIt-Anchor" href="#摘要算法"></a> 摘要算法</h4><p>将任意长的输入消息串变换为唯一的固定长度的输出，实现<strong>唯一认证标识</strong>，也称为无秘钥加密算法、哈希算法。经常与对称密码、非对称密码组合使用，构成完整的加密认证体系。</p><div align="center">  <img src="/2021/06/22/security-base/hash.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>常用MD系列、SHA系列。</p><p>MD5概要：输入是任意长度输出128位哈希。2004年王小云找出了碰撞，网上有MD5密文与明文的对照库，几乎已经淘汰。</p><p>SHA算法：安全哈希算法，已经逐步替代MD5，2004年王小云找到了SHA-1的碰撞，现在建议安全级别高的选择SHA-2。</p><p>在认证领域的具体应用有<strong>数字签名</strong>。附加在数据单元的一些数据，或者是对数据单元的密码变换，用于鉴定签名人身份及数据内容认可，防止源点或重点抵赖或欺骗。常用哈希算法和非对称加密。</p><ul><li>使用哈希算法，首先计算发送文件的哈希值，用私钥对该摘要进行签名，形成发送方数字签名。</li><li>使用非对称加密算法，首先将明文用自己的私钥加密得到数字签名，再将其和验证原文一起发送。</li></ul><div align="center">  <img src="/2021/06/22/security-base/digit.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/digit_p.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="密钥管理"><a class="markdownIt-Anchor" href="#密钥管理"></a> 密钥管理</h4><div align="center">  <img src="/2021/06/22/security-base/mima.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>公钥安全基础设施（PKI）</strong>，利用公钥技术实施和提供安全服务的普适性安全基础设施，是管理密钥和证书的系统或平台，在开放环境中为开放性业务提供公钥加密或数字签名。组成部分包括：公钥密码系统、数字证书、认证中心（CA）以及相关公钥的安全策略。</p><div align="center">  <img src="/2021/06/22/security-base/pki.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><ul><li>认证中心：核心执行机构，具有权威性（国家批准）、可信任性（高度安全）、公正性（第三方机构），负责所有证书的签发、注销以及证书注销列表的签发等。</li><li>注册中心：证书注册审批机构，负责证书申请者的信息录入、审核以及证书发放等。发放的证书可以存储在U盘、硬盘中。</li><li>密钥管理中心：CA密钥是整个PKI的核心机密，也管理由客户机生成的用户密钥。</li><li>在线证书状态查询服务：发布用户证书和相关黑名单的信息。</li></ul><p>数字证书用于保证公钥的真实性和有效性，是经过CA数字签名的包含公钥拥有者信息以及公钥的权威性电子文档。由权威公正的第三方CA签发，具有一定有效期，证明某个实体身份及其公钥的匹配绑定关系和合法性。当前常用X.509标准，包括基础版和扩展版。</p><div align="center">  <img src="/2021/06/22/security-base/ca.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>PKI中以分层结构信任即严格层次模型最为常用。</p><div align="center">  <img src="/2021/06/22/security-base/liang.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/china.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>### 4. 协议安全与安全协议<p>TCP/IP协议族面临的安全威胁举例：</p><p>1）明文传输；2）网络窥探；3）IP地址欺骗；4）路由攻击；5）IP隧道攻击；6）网络控制报文协议攻击；7）IP层拒绝服务型攻击；8）IP栈攻击；9）IP地址鉴别攻击；10）TCP序列号攻击。</p><p>网络安全协议基于密码学，用于保障信息安全，作用于TCP/IP下的安全协议或套件。以OSI网络层次及TCP/IP协议族为基础，通过加密、认证及相关协议进行密码学安全加固。</p><p>常用的网络安全协议举例（八种）：</p><p>1）L2TP，链路层扩展；2）IPsec，IP层安全；3）SSL/TLS，传输层安全；4）SSH，会话安全；5）Socks，代理安全；6）Kerberos，认证协议；7）PGP，应用安全；8）HTTPS，应用安全。</p><h4 id="l2tp链路层扩展第二层隧道协议"><a class="markdownIt-Anchor" href="#l2tp链路层扩展第二层隧道协议"></a> L2TP（链路层扩展）第二层隧道协议</h4><p>虚拟隧道协议，常用于VPN，支持IP、ATM、帧中继等网络类型。不提供加密和可靠性验证，通常需要与安全协议（特别是IPSec）搭配使用，实现数据加密传输。</p><h4 id="ipsecip层安全"><a class="markdownIt-Anchor" href="#ipsecip层安全"></a> IPsec（IP层安全）</h4><p>对IP协议的分组进行加密和认证，包括认证头（AH）、封装安全载荷（ESP）、安全关联（SA），提供不可否认性、数据完整性以及不可否认性等。</p><h4 id="ssltls传输层安全"><a class="markdownIt-Anchor" href="#ssltls传输层安全"></a> SSL/TLS（传输层安全）</h4><p>SSL协议位于TCP/IP协议与各种应用层之间，为高层协议提供数据封装、压缩、加密等基本功能，提供保密性和数据完整性。</p><h4 id="ssh协议会话安全"><a class="markdownIt-Anchor" href="#ssh协议会话安全"></a> SSH协议（会话安全）</h4><p>安全外壳协议，建立在应用层基础上，提供基于口令和基于密钥的两种安全验证。</p><h4 id="socks代理安全"><a class="markdownIt-Anchor" href="#socks代理安全"></a> Socks（代理安全）</h4><p>工作在会话层，使用UDP传输数据，提供一个通用框架使FTP、Telnet、SMTP等协议安全透明地穿过防火墙。只是简单地传递数据包，不关心是何种应用协议。</p><h4 id="kerberos认证协议"><a class="markdownIt-Anchor" href="#kerberos认证协议"></a> Kerberos（认证协议）</h4><p>通过密钥系统为客户机，服务器应用程序提供认证服务。基于可信第三方密钥分配中心，由两个独立的逻辑部分组成，即Kerberos认证服务器（AS）和票据授权服务器（TGS）。</p><h4 id="pgp应用安全"><a class="markdownIt-Anchor" href="#pgp应用安全"></a> PGP（应用安全）</h4><p>优良保密协议（Pretty Good Privacy），用于消息加密、验证的应用程序，采用RSA加密、IDEA散列算法验证，由一系列散列、数据压缩、对称秘钥加密，以及公钥加密的算法组成，常用于电子邮件加密，提供数据加密和数字签名服务。</p><h4 id="https应用安全"><a class="markdownIt-Anchor" href="#https应用安全"></a> HTTPS（应用安全）</h4><p>HTTP + 加密 + 认证 + 完整性保护 = HTTPS。</p><p>是HTTP先和SSL协议通信，SSL再和TCP通信，可解决明文传输、认证以及数据完整性问题。</p><h3 id="5-应用软件安全实现"><a class="markdownIt-Anchor" href="#5-应用软件安全实现"></a> 5. 应用软件安全实现</h3><p>应用软件安全需求获取；</p><p>应用软件安全设计：MVC、微内核架构、管道-过滤器架构、SOA架构、巨石型架构、微服务架构等；</p><p>应用软件安全编码：安全编码八大原则、安全编码标准与指南；</p><p>应用软件安全测试：安全功能测试、安全漏洞测试；正向测试（静态分析、基于模型、模糊测试、基于风险、基于故障树、渗透测试、故障注入等）、逆向测试（依据已知软件缺陷空间，建立威胁模型，寻找潜在入侵点进行测试）；</p><h3 id="6-备份与灾难恢复技术"><a class="markdownIt-Anchor" href="#6-备份与灾难恢复技术"></a> 6. 备份与灾难恢复技术</h3><p>数据量爆炸式增长，访问方式和访问时间要求高，异构数据（结构化、半结构化、非结构化）大量存在，云计算、物联网、移动通信、工业互联网等领域新问题。</p><p>直接连接存储（DAS）、网络接入存储（NAS）、存储区域网络（SAN）。</p><p>虚拟化存储，包括带内虚拟化、带外虚拟化；主机虚拟化、子系统虚拟化、网络级虚拟化。</p><div align="center">  <img src="/2021/06/22/security-base/vistual.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>数据备份策略：完全备份、差异备份、增量备份。实现方式包括Host-based、LAN-based、SAN LAN-Free、SAN Server-Free。</p><div align="center">  <img src="/2021/06/22/security-base/drp.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="四-被动防御技术体系"><a class="markdownIt-Anchor" href="#四-被动防御技术体系"></a> 四. 被动防御技术体系</h2><p>依赖于专家经验，通过预先设计的规则对已知攻击手段和威胁方式进行防御。</p><div align="center">  <img src="/2021/06/22/security-base/negative.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>技术局限性：1）取决于威胁特征，包括来源、特征、途径、行为等先验特征的精确表达；2）非对称和不确定背景下，攻击行为感知困难。</p><h3 id="1防火墙技术"><a class="markdownIt-Anchor" href="#1防火墙技术"></a> 1.防火墙技术</h3><p>防火墙是网络访问控制机制的一种具体实现，部署在不同网络或不同安全域之间，实现不同信任域之间的安全隔离。</p><p><strong>传统防火墙</strong>基本假设：部署于被保护信息系统边界的唯一信息交互通道，通过的信息需要防护策略明确授权，软硬件实现具有高安全性和高可靠性。视内网为可信区，外网为不可信区域，中间为隔离区。</p><div align="center">  <img src="/2021/06/22/security-base/shixian.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>1）过滤机制：</p><p><strong>包过滤机制</strong>，作用于IP层和传输层，与应用层无关，主要检查IP地址、端口、协议类型、IP选项、TCP选项以及数据包流向等；具体规则包括拒绝/允许与某主机或某网段的所有连接、指定端口的连接等。优点为易于实现与部署、处理速度快、效率高，尤其适合小规模网络应用。缺点是无法过滤数据包内容，不支持用户认证、大规模网络的规则过滤表配置困难。</p><p><strong>应用代理过滤机制</strong>，代理设备Proxy实现外网和内网的隔离，应用于应用层，类似一个中间人。支持用户级身份认证、日志记录功能强大。但额外增加的连接处理易导致性能瓶颈、应用层协议总量受限、无法改进底层协议安全性。</p><div align="center">  <img src="/2021/06/22/security-base/app.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>状态检测过滤机制</strong>，基于会话机制，将属于同一连接的所有包视为一个整体。其检测规则包括多种连接状态信息，并结合通信数据、状态信息及关联、安全策略、应用识别知识等。工作于链路层和网络层之间，从底层捕获信息从高层分析，层次高于传统包过滤但低于应用代理。所以网络层和传输层的控制能力强，应用协议适用性强、配置方便、效率高，但无法彻底识别数据包中的恶意内容等。</p><p>2）软硬件实现方式：</p><p><strong>软件防火墙</strong>，运行于操作系统之上而不依赖于硬件。扩充空间大、配置灵活，但需要占用驻留主机资源，存在性能的双向影响。多用于安全性要求不高的单机系统。</p><p><strong>软硬件结合实现</strong>，安装在专作防火墙的主机之上。</p><p><strong>硬件实现</strong>，软件运算硬件化，将主要的运算程序通过X86、专用集成电路、网络处理器、可编程门阵列等专用硬件架构来实现。采用专用硬件加速，是中高端防火墙的首选。功能完善、效率高、性能稳定，但配置专业性强，价格比较高。</p><p><strong>虚拟防火墙</strong>，将物理实体防火墙划分为多个防火墙，每个虚拟防火墙有独立系统资源、安全策略，可以部署于核心交换机和主要服务器群处、物理网络和虚拟网络之间或两个虚拟网络之间。</p><p>防火墙性能指标主要分为四大类：吞吐量、时延、新建连接速率和并发连接数。</p><div align="center">  <img src="/2021/06/22/security-base/firewall.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><ol start="3"><li>部署方式：</li></ol><p>需要考虑具体网络应用环境和性能指标的综合平衡。</p><p><strong>网络式防火墙</strong>，双穴主机（一个主机具有两个地址）网关型部署、单穴堡垒主机型部署(屏蔽主机网关型)、双穴堡垒主机型、屏蔽子网型。</p><p><strong>分布式防火墙</strong>，整体安全策略由安全策略管理服务器负责定义，并由分布于网络中的各防火墙断点执行。分为实体部署方式和虚拟化部署方式。</p><h4 id="web应用防火墙waf技术"><a class="markdownIt-Anchor" href="#web应用防火墙waf技术"></a> <strong>Web应用防火墙（WAF）技术</strong></h4><p>主要应对Web2.0架构中的HTTP/HTTPS应用的安全威胁，比如XSS（跨站脚本攻击）、SQL注入、DDos、Cookie篡改、网页挂马、网页篡改、敏感信息泄露等。</p><p>部署在Web服务器和客户端之间，采用在线（串行部署，采用透明代理、反向代理、路由代理等模式）或离线模式（旁路部署，仅检测而无法阻断），此外还有云WAF。</p><h4 id="工业防火墙"><a class="markdownIt-Anchor" href="#工业防火墙"></a> 工业防火墙</h4><p>特殊要求：机械侵入防护能力、气候适应、电磁兼容；工业通信协议的脆弱性及其解析与过滤，比如工业Ethernet、RS232/422/485、IEC104、OPC等；实时性（硬件加速）与可靠性（软件旁路保护、双机热备、电源冗余）保障。</p><h4 id="攻击与防范技术"><a class="markdownIt-Anchor" href="#攻击与防范技术"></a> 攻击与防范技术</h4><p>通常采用多层次安全性机制保护防火墙，硬件层面包括ASIC、FPGA、NP或多核架构；软件层面采用分权分域的权限管理，采用最小特权策略，细粒度访问控制等；系统层面，优先使用非Windows系统，对外仅提供电源、数据及管理等接口，弱化启发服务功能。</p><p>针对防火墙的<strong>探测</strong>工具常用NMAP、Firewalk以及shodan等，为防范上述工具，要尽量减少端口开放，关闭不必要的服务，并获取扫描服务器IP，添加到防火墙黑名单。</p><p>绕过防火墙方法主要有，协议隧道攻击（防御措施包括，最小开放、合理的策略、白名单机制等），分片绕过防火墙认证攻击（分析分片报文的统计数据）、IP地址欺骗攻击（阻止有风险IP、数据进站过滤、反向路径转发等，对于普通木马要合理配置强化IP包过滤规则，对于反弹木马要监测非法连接，关闭相关端口）等。</p><h3 id="2脆弱性与漏洞检测与防范"><a class="markdownIt-Anchor" href="#2脆弱性与漏洞检测与防范"></a> 2.脆弱性与漏洞检测与防范</h3><p>Vulnerability，脆弱性，源自信息系统在硬件、软件、协议等设计和实现中的先天缺陷，安全策略配置不当或者系统使用失误。脆弱性是信息系统的客观存在。</p><p>威胁来源通过<strong>攻击向量</strong>作用于信息系统的安全弱点，通过安全控制实现对信息系统资产或功能的技术影响，最终实现对系统的业务功能产生影响。攻击向量（AV）指攻击者用来攻击信息系统的一种手段，即可资网络渗透攻击利用的各种维度，比如恶意代码、恶意邮件、网页、社会工程攻击法等。</p><div align="center">  <img src="/2021/06/22/security-base/vul.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="漏洞分类"><a class="markdownIt-Anchor" href="#漏洞分类"></a> 漏洞分类</h4><p>漏洞是脆弱性之一，是存在于信息系统之中的、作用于一定环境的、可能被利用且会对系统中的组成、运行和数据造成损害的一切因素。国际漏洞库如CVE、CWE，国内有CNNVD、CNVD。</p><p>漏洞分级包括访问路径（远程、邻接和本地），漏洞被利用的复杂程度（简单、复杂），影响程度（完全、部分、轻微和无影响），综合判断（超危、高危、中危和低危）。</p><p>脆弱性扫描/漏洞扫描，扫描主要是逐个发现单个脆弱性，分析是度量网络系统整体脆弱性。</p><div align="center">  <img src="/2021/06/22/security-base/lou.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/loug.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="漏洞分析"><a class="markdownIt-Anchor" href="#漏洞分析"></a> 漏洞分析</h4><p><strong>常用的脆弱性分析方法包括图论分析法、攻击树分析法、可生存性分析法</strong>:</p><ul><li>图论分析法，将常见脆弱性利用方法、利用步骤、网络拓扑数据、系统基础参数等作为基础输入信息，选取图的节点和路径进行分析。</li><li>攻击树分析法，将外部攻击步骤映射为决策树，对信息系统的脆弱性进行量化处理，以概率描述各脆弱性被利用的可能。</li><li>可生存性分析法，在规范的网络系统中设置脆弱性和攻击事件，运用贝叶斯定理等概率计算来图形化描述和揭示安全脆弱性被利用的影响程度。</li></ul><div align="center">  <img src="/2021/06/22/security-base/live.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>漏洞检测和分析的新需求和技术<strong>发展趋势</strong></p><div align="center">  <img src="/2021/06/22/security-base/new.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="防范技术"><a class="markdownIt-Anchor" href="#防范技术"></a> 防范技术</h4><p>OWASP Top10，开放式Web应用程序安全项目，按照严重程度分别给出十大Web应用安全脆弱性和漏洞，并提供一些防范措施。</p><p>注入攻击、身份认证失效、敏感数据泄露、XML外部实体（XXE）、访问控制失效、安全配置错误、跨站脚本攻击（XSS）、不安全反序列化、使用含有已知漏洞的组件、日志记录和监控的欠缺。</p><h3 id="3-恶意代码检测及防范技术"><a class="markdownIt-Anchor" href="#3-恶意代码检测及防范技术"></a> 3. 恶意代码检测及防范技术</h3><p>恶意代码不仅自身有安全危害，更能够借助信息系统的脆弱性来实现传播和破坏。</p><h4 id="基本分类"><a class="markdownIt-Anchor" href="#基本分类"></a> 基本分类</h4><p>恶意代码有很多形式，比如病毒、蠕虫、木马、间谍软件、僵尸程序、恶意脚本、流氓软件、逻辑炸弹、后门、网络钓鱼工具等。恶意代码在<strong>代码独立性</strong>和<strong>自我复制性</strong>方面各有区别，其分类界限并不明显。</p><p><strong>病毒</strong>，可自我复制的一组代码，具有传染性、非授权可执行性、隐蔽性、潜伏性、可触发性以及破坏性等特征。</p><p><strong>蠕虫</strong>，消耗宿主资源，能够传播，自我复制、独立运行、消耗系统资源；分为传播、隐藏及目的功能模块。</p><p><strong>木马</strong>，隐藏在正常程序中的一段后门程序，伪装自身吸引用户下载执行，安置“后门”以便实施攻击，没有自我复制功能。</p><p><strong>间谍软件</strong>，在用户不知情或未经用户准许的情况下搜集、使用、并散播用户的个人数据或敏感信息。</p><p><strong>僵尸程序</strong>，具有恶意控制功能，形成一对多的控制网络。</p><p><strong>恶意脚本</strong>，利用脚本形式/纯文本形式保存。</p><p><strong>流氓软件</strong>，未明确提示用户或未经允许情况下强行安装，抵制卸载，浏览器劫持，恶意捆绑等。</p><p><strong>逻辑炸弹</strong>，隐含在正常功能中，因一定条件的触发而具有破坏性。</p><p><strong>后门</strong>，绕过安全控制而获取对程序或系统访问权的方法，以便于再次秘密进入或控制系统。</p><p><strong>网络钓鱼</strong>，利用欺骗性电子邮件和伪造的Web站点进行诈骗活动。</p><h4 id="分析检测技术"><a class="markdownIt-Anchor" href="#分析检测技术"></a> 分析检测技术</h4><p>特征主要有<strong>特征码</strong>（恶意代码的结构自身）和<strong>特征行为</strong>（需要动态运行），基于特征码校验的恶意代码检测为静态检测，而基于特征行为的恶意代码检测为动态检测。</p><p>特征码字符串多为恶意代码文件里对应代码或汇编指令的地址，也可以直接采用入口点的代码段来生成特征码。</p><div align="center">  <img src="/2021/06/22/security-base/tezheng.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>各类恶意软件的特征码会被写入代码特征库中，包括特征起始地址、特征长度、具体特征等，之后进行匹配检测。准确度高、简单快捷，但一定会出现滞后问题。</p><p>使用静态手段检测<strong>多态性恶意代码</strong>（Chameleon，Casper）有一定难度，因为它每次复制时都会更改其自身，变形方式包括采用不固定密钥或随机数加密病毒程序代码、病毒运行过程中改变代码、通过一系列模糊变换、等价指令替换、加壳、花指令、混淆等变换实现多态。同一种恶意代码的多个实例样本的代码都不相同。</p><p><strong>恶意行为</strong>指代码运行时执行的若干互不重复的内置恶意动作以及一系列扩展的时序恶意动作。孤立行为往往难以成为恶意行为特征，而且要厘清需要关注的重点行为。主要采用动态污点跟踪或状态机模型方法。</p><p>另外还有一种使用校验和的检测方法。优势在于考察程序代码完整性，不依赖外部信息，既可检测已知恶意代码，又可检测未知。缺点在于无法识别恶意代码种类和具体版本，程序自身变化导致<strong>误报率高</strong>。</p><div align="center">  <img src="/2021/06/22/security-base/sum.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="防范与治理"><a class="markdownIt-Anchor" href="#防范与治理"></a> 防范与治理</h4><div align="center">  <img src="/2021/06/22/security-base/fang.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>恶意代码清除，代码附着于宿主系统之上，只要正确恢复了文件头就可以实现清除病毒的目的。</p><p>恶意代码删除，对于无需宿主系统的代码。</p><p>恶意代码隔离，采用加密或者修改文件后缀名的方式。</p><p>当前面临的挑战：</p><ol><li>病毒采用自身加密和压缩（如加壳）</li><li>僵尸网络采用C&amp;C加密、通信采用快通量DNS</li><li>虚拟分析的虚拟机逃逸</li><li>需要提升对恶意代码的容忍和沙箱分析手段</li></ol><h3 id="4-入侵检测技术"><a class="markdownIt-Anchor" href="#4-入侵检测技术"></a> 4. 入侵检测技术</h3><p>防火墙之后的有一道防线，通用架构如下，其中目录服务组件，负责实现组件定位、数据传输控制及认证、密钥发布和管理。</p><div align="center">  <img src="/2021/06/22/security-base/in.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/ids.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>IDS核心模块主要是安全数据采集、入侵分析和入侵告警。</p><h4 id="基础架构模块"><a class="markdownIt-Anchor" href="#基础架构模块"></a> 基础架构模块</h4><p>安全数据包括主机数据（系统日志、状态、服务、事件等）、网络数据、其它特定网络组件分析处理后的抽象或结论数据。</p><p><strong>主机数据</strong>包括如下几类：</p><ul><li>内核信息：在操作系统内核获取入侵信息；</li><li>审计记录：系统身份认证无法确认的身份、访问安全等级不相符的数据、对系统运行产生重要影响的动作、其它与安全相关的动作等。使用审计探测器，记录审计产生并检查按时序排列的系统事件记录过程；</li><li>日志信息：系统程序按照每次一行写成的文本文件，反映了不同系统事件和设置。使用日志探测器，包括系统日志、安全日志、应用日志等。</li><li>系统资源信息：使用系统资源监视器，包括系统参数如CPU占用、内存利用率、I/O、网络连接数等。</li><li>应用程序信息：应用程序事件日志和内部数据。</li></ul><p><strong>网络数据</strong>包括特定协议/地址/端口的报文和字节流量的采集，反映受保护网段网路攻击或异常行为的网络包数据。使用共享式网络安全数据采集器，交换式网络安全数据采集器（增加总线式集线器、SPAN端口、TAP等）。</p><p><strong>其它数据</strong>主要由防火墙（报警）、交换机、路由器数据组成。</p><p>入侵分析是基于采集到的安全数据进行统计、模式匹配、协议解析、数据关联等，发现违反安全策略的行为或潜在的入侵行为。</p><p><strong>异常检测</strong>：假设入侵行为和网络正常行为存在较大差异。无需对规则库进行不断更新维护，可以根据异常现象发现未知攻击方式的入侵。但是批处理导致检测实时性不强，统计分析无法反映攻击事件时间相关性特征，难以确定具体的攻击方式。</p><div align="center">  <img src="/2021/06/22/security-base/ano.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>误用检测</strong>，假设已知攻击行为可以采用特定模型或规则来表示。包括协议解析、模式匹配、关联分析等技术。</p><div align="center">  <img src="/2021/06/22/security-base/wuyong.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><ul><li>协议解析，利用网络协议的高度规则性，实现入侵行为的快速检测。可处理数据包、帧和链接等元素。检测速度快、准确率高、资源消耗少，可具体针对IPv4/v6以及工控协议等，但是协议树动态构建困难，高速网络下协议识别性能受限。</li><li>模式匹配，将入侵场景抽象为特征组合，与已知的攻击方法抽象得到的模式知识库进行匹配。检测速度快、准确率高、资源消耗少，可具体针对IPv4/v6以及工控协议等，但攻击模式抽取、枚举与更新工作量大，检测准确度依赖于知识库，难以检测已知攻击变种以及未知攻击。</li><li>关联分析，基于探测到的数据信息构建规则集并进行分类匹配，通过相似度对比来抖索关联规则库，采用该规则库进行规则匹配。可以分析隐含性、未知性、异常性的特征，能够实现时间、事件等关联。但是相似度定义和匹配判定比较困难，智能化要求高。</li></ul><p>总结来看，异常检测通用性强，对于未知模式的新攻击有一定检测能力，但无法准确判别攻击类型。而误用检测需要实现构建并维护较为完备的入侵检测特征知识库，检测能力依赖于特征知识库的完备性，对未知模式的新攻击检测能力不足。</p><p>入侵响应包括报警、预警和攻击阻断。</p><h4 id="性能指标与应用部署"><a class="markdownIt-Anchor" href="#性能指标与应用部署"></a> 性能指标与应用部署</h4><p>准确性（检测率、误报率、漏报率等）、效率（最大处理能力、每秒并发会话数、系统延迟时间、负荷能力等）、可用性（稳定性、可持续性、易用性）以及自身安全性（体系结构、软件实现、安全管理）等。</p><p>部署时需要考虑整体安全策略、应用环境和已有安全机制、IDS性能等。基本部署在防火墙之内/外，骨干网络及关键子网上，待监测目标主机之上等。</p><h2 id="五-网络安全主动防御"><a class="markdownIt-Anchor" href="#五-网络安全主动防御"></a> 五. 网络安全主动防御</h2><p>攻防不对称性，威胁不确定性。被动防御是主动防御的基础，主动防御是被动防御的延伸。</p><p>主动功能包括：1）攻击或者安全破坏发生之前，实现及时、准确识别和预警；2）主动规避、转移、降低或消除网络空间所面临的安全风险。</p><div align="center">  <img src="/2021/06/22/security-base/active.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="1-入侵防御容忍技术"><a class="markdownIt-Anchor" href="#1-入侵防御容忍技术"></a> 1. 入侵防御/容忍技术</h3><div align="center">  <img src="/2021/06/22/security-base/fy.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>入侵防御技术（IPS），基于入侵检测的深度防御安全机制，通过主动响应方式，实时阻断入侵行为，降低或减缓网络异常状况处理的资源消耗，以保护网络空间信息系统免受更多侵害。技术体现在智能检测、深层防御和主动响应。</p><ul><li><strong>智能检测</strong>主要是高效检测和联动，作用基础是相对完备的恶意代码特征知识库和入侵攻击特征知识库。实现智能协议识别与解析、数据报文的深层检测等。将基于特征的检测和基于统计的检测方法智能结合。</li><li><strong>深层防御</strong>，主要作用于恶意代码和应用层数据解析，应对多种威胁如恶意代码、通用网关接口、跨站脚本攻击、注入攻击、信息泄露等，可进行应用层攻击载荷防护，需要检测报文应用层内容和数据流重组分析和检测等。</li><li><strong>主动响应</strong>，自动执行或用户驱动来阻断、延缓入侵进程或改变受攻击信息系统环境配置，融合恶意代码检测、脆弱性评估、防火墙等进行安全联动。</li></ul><div align="center">  <img src="/2021/06/22/security-base/method.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>入侵容忍技术</strong>是为了提升系统自身的免疫力，包括可生存技术、容错技术。</p><div align="center">  <img src="/2021/06/22/security-base/basen.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>容错技术在网络攻防对抗环境下适用性不强。因为容错技术并非针对恶意入侵/攻击而设计，而且并非所有攻击/入侵均表现为信息和系统的破坏进而呈现显示错误。</p><div align="center">  <img src="/2021/06/22/security-base/wrong.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>入侵容忍技术（ITS）提出的原因是IDS、IPS无法完全检测所有的入侵行为，一旦有入侵发生，可以继续保障机密性、完整性、真实性和安全性。核心目标是实现系统权限的分立以及单点失效的技术防范，确保任何少数设备、局部网络以及单一场点均不可能拥有特权或对系统整体运行构成威胁。主要分为<strong>攻击响应（重新分配资源，实现系统重构）<strong>和</strong>攻击遮蔽技术（适度冗余配置，实现高生存性）</strong>。</p><p>系统资源调整方式：</p><div align="center">  <img src="/2021/06/22/security-base/adjust.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>攻击遮蔽中的冗余技术无需再系统遭受入侵或失效时重构，系统操作以及连接可以继续保持。</p><h3 id="2-蜜罐密网技术"><a class="markdownIt-Anchor" href="#2-蜜罐密网技术"></a> 2. 蜜罐/密网技术</h3><p>&quot;欺骗型&quot;主动防御的关键技术之一，专门设置特定的脆弱系统来诱捕攻击。蜜罐技术采用单台主机构建诱骗环境，密网技术采用网络系统构建诱骗环境。</p><p>部署具有脆弱性或漏洞的相应主机、网络服务或信息作为诱饵，引诱攻击者发起攻击，为防御者提供安全威胁提示，最后通过技术和管理手段提升系统安全防御能力。</p><p>蜜罐中不进行传统目的的网络活动，即不运行其它进程、服务和后台程序。蜜罐系统中的所有交互行为，都视为恶意活动的嫌疑对象，有利于攻击活动的检测与识别。</p><div align="center">  <img src="/2021/06/22/security-base/honey.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>低交互蜜罐，仅让攻击者有限访问系统；静态欺骗环境（仅模拟存在漏洞的真实操作系统的某个部分）、部署简便、诱骗能力有限。</p><p>高交互蜜罐，完整服务栈，部署复杂，安全风险较大。</p><h3 id="3-沙箱技术"><a class="markdownIt-Anchor" href="#3-沙箱技术"></a> 3. 沙箱技术</h3><h3 id="4-可信计算与可信平台"><a class="markdownIt-Anchor" href="#4-可信计算与可信平台"></a> 4. 可信计算与可信平台</h3><h3 id="5-移动目标防御与拟态防御技术"><a class="markdownIt-Anchor" href="#5-移动目标防御与拟态防御技术"></a> 5. 移动目标防御与拟态防御技术</h3><h2 id="六-网络安全智能分析与反制技术"><a class="markdownIt-Anchor" href="#六-网络安全智能分析与反制技术"></a> 六. 网络安全智能分析与反制技术</h2><h2 id="七-网络安全度量-分析与测评技术"><a class="markdownIt-Anchor" href="#七-网络安全度量-分析与测评技术"></a> 七. 网络安全度量、分析与测评技术</h2><h2 id="八-新应用安全技术"><a class="markdownIt-Anchor" href="#八-新应用安全技术"></a> 八. 新应用安全技术</h2><h1 id="相关资料"><a class="markdownIt-Anchor" href="#相关资料"></a> 相关资料</h1><p>要不要考个证噻？<a href="http://zhuanlan.zhihu.com/p/101574681" target="_blank" rel="noopener">http://zhuanlan.zhihu.com/p/101574681</a></p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>网络安全</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数学基础</title>
    <link href="/2021/06/22/math/"/>
    <url>/2021/06/22/math/</url>
    
    <content type="html"><![CDATA[<h1 id="李宏毅线性代数2018"><a class="markdownIt-Anchor" href="#李宏毅线性代数2018"></a> 李宏毅《线性代数》2018</h1>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>好好生活（心理、哲学相关资料）</title>
    <link href="/2021/06/21/active/"/>
    <url>/2021/06/21/active/</url>
    
    <content type="html"><![CDATA[<h2 id="耶鲁幸福课简课"><a class="markdownIt-Anchor" href="#耶鲁幸福课简课"></a> 耶鲁“幸福课”简课</h2><p>B站视频（<a href="https://www.bilibili.com/video/BV1aX4y1G7we%EF%BC%9Bhttps://www.youtube.com/watch?v=duhW4of8bOA&amp;t=5s%EF%BC%89" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1aX4y1G7we；https://www.youtube.com/watch?v=duhW4of8bOA&amp;t=5s）</a></p><p>2018年的《心理学与美好生活》课程，完整版在Coursera上（<a href="https://www.coursera.org/learn/the-science-of-well-being%EF%BC%89" target="_blank" rel="noopener">https://www.coursera.org/learn/the-science-of-well-being）</a></p><p>寻找比较“快乐”的人，关注他们的生活方式，总结出以下十条观点：</p><h5 id="1-we-can-control-more-of-our-happiness-than-we-think"><a class="markdownIt-Anchor" href="#1-we-can-control-more-of-our-happiness-than-we-think"></a> 1. We can control more of our happiness than we think</h5><p>比如半空半满态度，基因在50%的程度上决定着我们的幸福水平。</p><h5 id="2-our-life-circumstances-dont-matter-as-much-as-we-think"><a class="markdownIt-Anchor" href="#2-our-life-circumstances-dont-matter-as-much-as-we-think"></a> 2. Our life circumstances don’t matter as much as we think</h5><p>比如中了彩票大奖的人或遭遇重大挫折的人，在半年之后幸福水平会回归他的常态值。遭遇重大挫折或重大疾病的人反而会逐渐获得平静（但我们不能总是期待依靠失去生命而学会如何真正地活着）。</p><p>外部环境在10%的程度上决定着我们的幸福水平。</p><p>所以还剩下整整40%的部分是可以由我们自己决定的。</p><h5 id="3-you-can-become-happier-but-it-takes-work-and-daily-effort"><a class="markdownIt-Anchor" href="#3-you-can-become-happier-but-it-takes-work-and-daily-effort"></a> 3. You can become happier but it takes work and daily effort</h5><p>像学习其它技能时一样，通过大量、坚持地联系，学会更加幸福。</p><p>knowing is（not even） half the battle（知道是成功的一半，还不到）</p><h5 id="4-your-mind-is-lying-to-you-a-lot-of-the-time"><a class="markdownIt-Anchor" href="#4-your-mind-is-lying-to-you-a-lot-of-the-time"></a> 4. Your mind is lying to you a lot of the time</h5><p>比如人们认为有更多的钱，比如加薪，就可以增加幸福感。</p><p>我们的目标会移动，越来越大。追求物质的人更加不快乐。</p><h5 id="5-make-time-for-making-social-connections"><a class="markdownIt-Anchor" href="#5-make-time-for-making-social-connections"></a> 5. Make time for making social connections</h5><p>**这一点非常重要！**有研究表明，独处的时间越长的人越容易被归类为“不幸福的”。这一点我自己也是有亲身体会，但是这不说明我们不需要独处。</p><p>只是不要过度独处，要与他人（朋友、家人、爱人）建立关系。</p><p>而且要<strong>注意是真正与他人相处</strong>，之前我的社交是“表面社交”，因为在和别人一起时，我其实更多地还是关注自己的心理问题，并没有做到全心全意，有点像只是躯壳在于他人共处。</p><p>与他人建立真正的联系会让我们感到更加幸福。</p><p>虽然可能存在双向相关性，比如是这个人本身很幸福所以才去社交，但研究表明，与他人交往确实会提升我们的幸福感（这一点我也是亲身体会）。</p><h5 id="6-helping-others-makes-us-happier-than-we-expect"><a class="markdownIt-Anchor" href="#6-helping-others-makes-us-happier-than-we-expect"></a> 6. Helping others makes us happier than we expect</h5><p>志愿活动，帮助、关心他人确实会提升幸福感。不要把“善良”、“无私”等词语只挂在嘴边。</p><h5 id="7-make-time-for-gratitude-every-day"><a class="markdownIt-Anchor" href="#7-make-time-for-gratitude-every-day"></a> 7. Make time for gratitude every day</h5><p>写感恩日记（每天写3~5个，坚持两周）可以促成这种习惯，直到别人问你最近怎么样的时候，可以自发自动的首先想到一些好事。</p><p>告诉别人，你真的很感激。比如写感谢信，不要觉得尴尬，想象一下作为收信人的感受。</p><p>注意感恩的力量。</p><h5 id="8-healthy-practices-matter-more-than-we-expect"><a class="markdownIt-Anchor" href="#8-healthy-practices-matter-more-than-we-expect"></a> 8. Healthy practices matter more than we expect</h5><p>锻炼对身体好，对心理健康也好！具有很强的情绪价值。</p><p>好好睡觉也会提升幸福感，非常重要，坏的睡眠习惯甚至会导致抑郁。</p><h5 id="9-being-in-the-present-moment-is-the-happiest-way-to-be"><a class="markdownIt-Anchor" href="#9-being-in-the-present-moment-is-the-happiest-way-to-be"></a> 9. Being in the present moment is the happiest way to be</h5><p>活在当下，体察/品尝当下的你。<strong>冥想</strong>会有所帮助。</p><p>大部分人大部分时间在“心理徘徊”，这意味着我们失去了大部分人生，而且会降低幸福感。</p><p>个人体会是一边看剧一边玩游戏或一边刷知乎，都不会让人感到轻松。组会上划水也特别心累。</p><h5 id="10become-wealthy-in-time-not-in-money"><a class="markdownIt-Anchor" href="#10become-wealthy-in-time-not-in-money"></a> 10.Become wealthy in time not in money</h5><p>并不是客观上的，而是主观上认为自己时间充裕。<strong>追逐时间的人</strong>甚至比追逐金钱的人还要不幸福。</p><h2 id="杨立华教授"><a class="markdownIt-Anchor" href="#杨立华教授"></a> 杨立华教授</h2><h3 id="一-青年朋友读书建议"><a class="markdownIt-Anchor" href="#一-青年朋友读书建议"></a> 一. 青年朋友读书建议</h3><p>2021年2月19日直播，<a href="https://www.bilibili.com/video/BV1V54y157pb" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1V54y157pb</a></p><p>南怀瑾先生更多是实践派而非学院式研究。</p><p>人活着不能图省力，正确的道路一定是用力的道路。活得太轻松一定不对，要么是方向不对，要么是努力程度不够。后者的问题在于把上天给我们的天赋浪费掉了。“天生一世人，自足了一世事”。做捶打生活的铁匠。</p><p>阅读可以凝聚精神，调动起人的主动性。不要只去读让自己舒服的书。读得少一点，读得精一点，读得深一点。</p><p>《论语》终身读之，另外推荐《庄子》。五经里面选自己气和的精读。</p><h3 id="二-关于中国哲学问题"><a class="markdownIt-Anchor" href="#二-关于中国哲学问题"></a> 二. 关于中国哲学问题</h3><p>中国核心品格为<strong>此世性格</strong>，是唯一没有创世神话的民族。从而引申出一种根本不同的审视世界的眼光。</p><p>此世是唯一的目的，也是唯一的过程。</p><p>哲学就是关于世界人生最根本问题的系统体系化、整体性思考。</p><p>概括归纳是自然科学的道路，而不是哲学的道路，我们无法从经验中获得整体性认识。</p><p>哲学在表述形态层面不断发展。<strong>哲学一定源自于时代精神危机的深化</strong>。</p><p>孔子的思想很饱满，一生中基本没有遇到需要论证的时候。</p><p>尼采说过，“<strong>论证是一种颓废的意志</strong>”。论证本身就是否定性的，一个人想证明自己在一方面就意味着你的存在已经不再饱满了。</p><p>宋明道学的发展大概可以分为五个阶段。</p><h3 id="三-王阳明"><a class="markdownIt-Anchor" href="#三-王阳明"></a> 三. 王阳明</h3><h4 id="1"><a class="markdownIt-Anchor" href="#1"></a> 1.</h4>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>心理学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TUM《图与时序数据机器学习》笔记</title>
    <link href="/2021/06/18/tum-mlgs/"/>
    <url>/2021/06/18/tum-mlgs/</url>
    
    <content type="html"><![CDATA[<p>最开始由于“Certified Robustness”部分而开始关注到这个课程，课程官网（<a href="https://www.in.tum.de/daml/teaching/mlgs/%EF%BC%89%EF%BC%8C%E7%94%B1%E4%BA%8E%E7%96%AB%E6%83%85%E9%A6%96%E6%AC%A1%E7%BA%BF%E4%B8%8A%E5%BC%80%E6%94%BE%E3%80%82TUM" target="_blank" rel="noopener">https://www.in.tum.de/daml/teaching/mlgs/），由于疫情首次线上开放。TUM</a> Stephan Günnemann大佬团队出品（好想有机会去这个课题组访问学习一段时间）。</p><p>由标题可以看出，这个课程专注于non-IID（非独立同分布）数据，主要分为两大类，即时序数据（temporal data / sequence）和图数据（graphs/networks）。其中，时序数据的non-IID主要体现在当前数值依赖于过去取值，而图数据的non-IID体现在结构信息部分。</p><h2 id="生成模型"><a class="markdownIt-Anchor" href="#生成模型"></a> 生成模型</h2><h2 id="鲁棒性分析"><a class="markdownIt-Anchor" href="#鲁棒性分析"></a> 鲁棒性分析</h2><h3 id="1-背景与问题抽象"><a class="markdownIt-Anchor" href="#1-背景与问题抽象"></a> 1. 背景与问题抽象</h3><p>我们出于以下两个方面考虑而关注模型鲁棒性：1）Real-world risks；2）Conceptual gaps，我们原以为模型已经像人一样学习到了数据中的重要语义信息（但对抗样本的存在反证了这一点），而且我们希望了解在worst-case noise下模型的效果如何。</p><p>几个重要概念：Perturbation，L-norm，targeted attack，untargeted attack</p><p>对抗样本生成可以抽象为一个优化问题。<strong>第一种方式</strong>如下：</p><div align="center">  <img src="/2021/06/18/tum-mlgs/p1.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>一般使用交叉熵函数替换0，1函数（没有明确的梯度值）。之后，使用<a herf="https://freemind.pluskid.org/machine-learning/projected-gradient-method-and-lasso/">PGD（Projected Gradient Descent）</a>&gt;求解此问题，另外还有经典的FGSM算法，使用梯度方向而非梯度来进行参数更新。</p><p><strong>第二种方式</strong>如下：</p><div align="center">  <img src="/2021/06/18/tum-mlgs/p2.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>或者可以写为无约束优化问题：</p><div align="center">  <img src="/2021/06/18/tum-mlgs/p3.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>一种比较有效的损失函数为：</p><div align="center">  <img src="/2021/06/18/tum-mlgs/loss.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="2-鲁棒性提升"><a class="markdownIt-Anchor" href="#2-鲁棒性提升"></a> 2. 鲁棒性提升</h3><p>到目前为止一些不太可行的研究角度：</p><ul><li><strong>Post-hoc prevention of attacks（事后防御</strong>），这类防御手段事后基本都被更厉害的攻击手段攻破。比如梯度混淆，通过处理（随机化或破坏）梯度信息防御基于梯度的攻击。</li><li>对抗样本检测，比如out-of-distribution shift，通过数据样本的分布情况检测出对抗样本，但是这类防御对targeted attacks基本无效。</li></ul><p>比较可行的方向是<strong>鲁棒训练（Robust training）</strong>，本质上是优化最差情况下的损失，the loss achieved under the worst-case perturbation。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/rb.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>进行鲁棒训练有两种方式：<strong>Adversarial training</strong>和<strong>robustness certification</strong>。</p><p>对抗训练是<strong>使用对抗样本代表最坏的扰动情况</strong>。求解时需要对上述<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mrow><mi>r</mi><mi>o</mi><mi>b</mi></mrow></msub></mrow><annotation encoding="application/x-tex">R_{rob}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是使用梯度下降法。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/at.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>而为了计算红框中的值，我们使用<strong>Danskin’s theorem</strong>证明，如果可以找到worst-case扰动，就可以解决问题。这里使用对抗样本指代worst-case。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/dt.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>所以对抗学习的流程如下：</p><div align="center">  <img src="/2021/06/18/tum-mlgs/atp.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>对抗学习方法优势和缺点如下图：问题在于过程中以训练速度换取了模型鲁棒性，而且得到的模型在“干净数据”上的表现形式稍差一些，而且我们没有关于模型的理论上的鲁棒性证明。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/rtp.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="3-可验证鲁棒性"><a class="markdownIt-Anchor" href="#3-可验证鲁棒性"></a> 3. 可验证鲁棒性</h3><p>本质思想是，证明在某范围的扰动之外分类器的预测结果不会变化。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/cr.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>有研究表明针对L_max扰动下的使用ReLU的神经网络进行Exact certification是NP难问题。但可以近似解决一些小型或中型网络。</p><p>第一种方法是<strong>MILP（Mixed Integer Linear Programming）</strong>，即有些线性规划条件限制变量是整数，而另一些没有这样的要求。如下所示，将问题简化为worst-case margin下的优化问题。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/mlp.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>此问题中最大的复杂性来自于ReLU。</p><h2 id="序列数据"><a class="markdownIt-Anchor" href="#序列数据"></a> 序列数据</h2><h2 id="图数据"><a class="markdownIt-Anchor" href="#图数据"></a> 图数据</h2><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅《机器/深度学习》2021笔记（三）</title>
    <link href="/2021/06/17/lhy-2021-c/"/>
    <url>/2021/06/17/lhy-2021-c/</url>
    
    <content type="html"><![CDATA[<h2 id="强化学习"><a class="markdownIt-Anchor" href="#强化学习"></a> 强化学习</h2><h3 id="1-基础概念"><a class="markdownIt-Anchor" href="#1-基础概念"></a> 1. 基础概念</h3><p>常用名词包括state、actor、action、environment、reward、return、trajectory（包含状态与动作的交互过程）等。</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/rl.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>对于计算出的action结果，依照其概率随机采样。</p><p>RL依然可以视为符合传统ML框架的三个阶段，但是模型存在随机性（actor行动选择部分，env和reward都是黑盒），所以在训练过程中的优化方法有所不同。</p><h3 id="2-policy-gradient"><a class="markdownIt-Anchor" href="#2-policy-gradient"></a> 2. Policy Gradient</h3><p>RL与传统ML分类问题的区别在于：1）当前动作会影响到之后的环境以及后续rewards；2）Reward delay，有时候需要牺牲短期利益换取长期收益。</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/pc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>训练RL可以有如下几种思路：</p><ul><li>Version 0：直接用当前action在当前state下带来的reward作为label进行训练。但这种方法无法考虑长期策略，比如玩射击游戏时，只开火不移动。</li><li>Version 1：使用cumulated reward而非仅考虑当前步骤的reward。但这种方法过多强调了早期action的重要性。</li><li>Version 2：在计算cumulated reward的时候不是单纯加和而是加入discount factor，逐步减少早期action的重要性。</li><li>Version 3：对cumulated reward进行标准化，比如所有值都减去一个参数b，让最终值有正有负。</li></ul><p>需要注意的是，在policy gradient的训练过程中，需要同步收集数据，每更新模型，重新收集一次数据。所以训练很耗时。这种训练叫做on-policy learning。</p><p>另外也是存在off-policy learning方式，即收集数据时的actor和训练中的actor不同，比较经典的方法是<strong>PPO（Proximal Policy Optimization）</strong>。</p><p>在收集训练数据的时候，使用Exploration方式，故意提升actor采取行动的随机性，有很多相应的技巧。</p><h3 id="3-actor-critc"><a class="markdownIt-Anchor" href="#3-actor-critc"></a> 3. Actor-Critc</h3><p>Critc评判actor在s下的效果。使用value function估测在s下actor <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span>的cumulated reward。</p><p>一种是使用基于蒙特卡洛的方法训练critic，直接完成整轮拿到结果，训练Value function；</p><p>第二种是使用基于temporal-difference的方法，解决有些“游戏”很难终止的问题，使用t+1和t两者之间的差别来训练Value function。</p><p>这两种方法可能算出来的Value function会不同。</p><ul><li>Version3.5：之后使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>G</mi><mo mathvariant="normal">′</mo></msup><mo>−</mo><mi>V</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">G&#x27;-V(s_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.835222em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>衡量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">{</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{s_t,a_t\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>的好坏。</li><li>Version 4：使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><msup><mi>V</mi><mi>θ</mi></msup><mo stretchy="false">(</mo><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>−</mo><msup><mi>V</mi><mi>θ</mi></msup><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r_t+V^\theta((s_{t+1})-V^\theta(s_t))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.099108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.099108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span>衡量，这个方法叫做“advantage actor-critic”</li></ul><div align="center">  <img src="/2021/06/17/lhy-2021-c/v3.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/17/lhy-2021-c/v4.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>实际实现的时候，发现其实actor和critic是可以共享参数的。</p><p>另外还可以直接使用critic做，比如<strong>DQN</strong>。有一个知名的文章叫做Rainbow。</p><h3 id="4-reward-shaping"><a class="markdownIt-Anchor" href="#4-reward-shaping"></a> 4. Reward shaping</h3><p>如果大多数情况下的reward都是0怎么办？也就是sparse reward问题。</p><p>要想办法提供额外的reward引导agent学习，也就是reward shaping。</p><h3 id="5-learning-from-demonstration"><a class="markdownIt-Anchor" href="#5-learning-from-demonstration"></a> 5. Learning from Demonstration</h3><p>有的时候连reward也没有用，让机器自己来定义（inverse reinforcement learning）。从expert的示范中学习到reward function。</p><p>基本概念是：Teacher is always the best。这个过程中可以把actor想象为generator，把reward function想象为discriminator。</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/irl.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="终身学习life-long-learning"><a class="markdownIt-Anchor" href="#终身学习life-long-learning"></a> 终身学习（Life Long Learning）</h2><p>还有Continuous Learning, Never Ending Learing, Incremental Learning等名称。</p><p>比如要解决Task1和Task2，一般模型可以同时比较好地学习并解决两个模型，但无法串行学习，会出现学会任务二后忘掉了任务一的现象。有种狗熊掰棒子的感觉，<strong>Catastrophic Forgetting</strong>（灾难性遗忘）。</p><p>由于计算资源和训练时间的限制，我们不能直接用<strong>Mutli-task training</strong>（即把所有任务的数据全部倒在一起同时训练）来解决这一问题。科研中一般把mutli-task training的结果当做life long learning的上限。</p><p>另外，和<strong>迁移学习</strong>的关注点也不同。迁移学习重点在于，由于已经学习过任务1，所以也可以做任务2或者简化任务2的训练过程。而终身学习的重点在于，保证即便学习了任务2，模型也不会忘记如何解决任务1。</p><p>Life long learning的模型评估一般使用多个task的表格，计算accuracy，backward transfer和forward transfer。</p><h4 id="1-selective-synaptic-plasticityregularization-based-approach"><a class="markdownIt-Anchor" href="#1-selective-synaptic-plasticityregularization-based-approach"></a> 1. Selective Synaptic Plasticity（Regularization-based Approach）</h4><p>每个参数对过去学过的任务的重要性不同，希望学习新任务时这些重要的参数不要变化太大。</p><p>给每个参数一个“保镖”<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，表示该参数对过去任务的重要程度。将训练任务的损失函数改为如下形式：</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/lf.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>研究的关键在于如何设定<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。如果设定过大会有intransigence问题，就是新任务学不起来。</p><p>之前还有一个名为**Gradient Episodic Memory（GEM）**的方法，修改参数更新方向。这个方法需要存储过去task上模型的资料，这一点和终身学习的初衷有些偏离。</p><h4 id="2additional-neural-resource-allocation"><a class="markdownIt-Anchor" href="#2additional-neural-resource-allocation"></a> 2.Additional Neural Resource Allocation</h4><p>Progressive Neural Networks，如下图所示，针对新的task新增模型。</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/pnn.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>PackNet，最开始的时候就开一个大的网络模型，分部分给不同的task使用。</p><p>CPG，有点像上述两个思想的结合。</p><h4 id="3-memory-reply"><a class="markdownIt-Anchor" href="#3-memory-reply"></a> 3. Memory Reply</h4><p>训练生成器，每次产生一些符合原有任务的伪数据。生成器占用的空间比存储先前数据占用的空间小。实验表明这种方法非常有效。</p><p>如果不同任务的样本类别不同如何解决？</p><p>另外，更换学习不同任务的顺序可以改变模型的训练效果，有些任务就没有遗忘问题。研究这方面策略的方向叫做“Curriculum Learning”。</p><h2 id="元学习"><a class="markdownIt-Anchor" href="#元学习"></a> 元学习</h2><p>让机器学习如何学习。</p><p>通常当前的few-shot learning的模型是通过元学习方法得到的。</p><p>一些专有名词：Across-task training/testing，Within-task training/testing，</p><h4 id="1-maml"><a class="markdownIt-Anchor" href="#1-maml"></a> 1. MAML</h4><p>比较难训练，有进阶版本MAML++（《How to train your MAML》）。</p><p>自监督学习中的Pre-training与它有比较大的差别。</p><p>MAML效果好的假设有两个：1）初始参数可以很快地找到最好模型（rapid learning）；2）初始参数原本就与最终最好的模型参数非常接近了（feature reuse）。有研究认为是后者：</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/maml.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>大幅度简化运算的变形FOMAML（First order maml）。后续改进版本还有Reptile。</p><h4 id="2-优化器"><a class="markdownIt-Anchor" href="#2-优化器"></a> 2. 优化器</h4><p>自动根据训练任务学习直接学习最佳优化器。</p><p>《Learn to Learn by gradient descent》NIPS 2016</p><h4 id="3-nasnetwork-architecture-search"><a class="markdownIt-Anchor" href="#3-nasnetwork-architecture-search"></a> 3. NAS（Network architecture search）</h4><p>训练得到网络架构。</p><p>无法计算微分，可以用RL硬做，agent的输出是各种网络架构选项。或者使用evolution algorithm。</p><p>另外有一个经典的算法叫DARTS，硬要网络架构参数可以微分。</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/lft.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="4data-augmentation"><a class="markdownIt-Anchor" href="#4data-augmentation"></a> 4.Data Augmentation</h4><p>模型自动寻找数据增强方法。</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/da.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="5sample-reweighting"><a class="markdownIt-Anchor" href="#5sample-reweighting"></a> 5.Sample reweighting</h4><p>如何决定边界线上的数据点的权重问题，是提升权重，还是当做噪声点而降低权重？</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/sw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="6beyond-gradient-descent"><a class="markdownIt-Anchor" href="#6beyond-gradient-descent"></a> 6.Beyond gradient descent</h4><p>放弃梯度下降，直接通过数据学习参数。</p><p>《Meta-learning with latent embedding optimization》ICLR 2019</p><h4 id="7-metric-based-approach"><a class="markdownIt-Anchor" href="#7-metric-based-approach"></a> 7. Metric-based approach</h4><p>把训练、测试数据都喂给数据集，看看学习效果如何。</p><p>一般元学习用在few shot learning上，N way k shot。</p><p>当下MAML也用到NLP、语音识别、<strong>知识图谱</strong>（<a href="http://speech.ee.ntu.edu.tw/~tlkagk/meta_learning_table.pdf%EF%BC%89%E7%AD%89%E8%BE%83%E5%A4%8D%E6%9D%82%E4%BB%BB%E5%8A%A1%E4%B8%8A%E3%80%82" target="_blank" rel="noopener">http://speech.ee.ntu.edu.tw/~tlkagk/meta_learning_table.pdf）等较复杂任务上。</a></p><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><h2 id="相关研究"><a class="markdownIt-Anchor" href="#相关研究"></a> 相关研究</h2><h3 id="1-终身学习b_i设置"><a class="markdownIt-Anchor" href="#1-终身学习b_i设置"></a> 1. 终身学习<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>设置</h3><p>EWC, <a href="https://arxiv.org/abs/1612.00796" target="_blank" rel="noopener">https://arxiv.org/abs/1612.00796</a></p><p>SI, <a href="https://arxiv.org/abs/1703.04200" target="_blank" rel="noopener">https://arxiv.org/abs/1703.04200</a></p><p>MAS, <a href="https://arxiv.org/abs/1711.09601" target="_blank" rel="noopener">https://arxiv.org/abs/1711.09601</a></p><p>RWalk, <a href="https://arxiv.org/abs/1801.10112" target="_blank" rel="noopener">https://arxiv.org/abs/1801.10112</a></p><p>SCP, <a href="https://openreview.net/forum?id=BJge3TNKwH" target="_blank" rel="noopener">https://openreview.net/forum?id=BJge3TNKwH</a></p><h3 id="2-如何处理增加了标签类别"><a class="markdownIt-Anchor" href="#2-如何处理增加了标签类别"></a> 2. 如何处理增加了标签类别</h3><p>Learning without forgetting（LwF）</p><p>iCARL：Incremental Classifier and Representation Learning</p><p>Life long learning的三个情景，<a href="https://arxiv.org/abs/1904.07734" target="_blank" rel="noopener">https://arxiv.org/abs/1904.07734</a></p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅《机器/深度学习》2021笔记（二）</title>
    <link href="/2021/06/16/lhy-2021-b/"/>
    <url>/2021/06/16/lhy-2021-b/</url>
    
    <content type="html"><![CDATA[<h2 id="自督导式学习"><a class="markdownIt-Anchor" href="#自督导式学习"></a> 自督导式学习</h2><p>从资料中抽取两部分，一部分作为模型输入，一部分作为目标输出。Self-supervised可视为无监督学习方法的一种，这个概念最早于2019年提出。</p><p>相关模型有：ELMo（94M参数），BERT（340M参数），ERNIE，Big Bird，GPT-2（1500M参数），Megatron（ 8B），T5（11B），Turing NLG（17B），GPT-3（10倍图灵），Switch Transformer（1.6T）。</p><h3 id="1-bert"><a class="markdownIt-Anchor" href="#1-bert"></a> 1. Bert</h3><p>主要结构是<strong>Transformer Encoder</strong>。训练是主要进行两个任务：1）随机mask输入的token，可以替换为特殊符号或者随机值；2）下一句子预测（进阶版ALBERT使用的是SOP，sentence order prediction）。</p><p>使用上述“填空题”训练BERT，在经过Fine-tune之后，可应用到下游任务中。而之前的BERT训练过程称为Pre-train。整个过程合起来算是一种semi-supervised方法。</p><p>评估自监督模型的常用任务集，GLUE（General Language Understanding Evaluation），包含九个任务。</p><p>下游任务包括：情感分析（Linear模型部分随机初始化，BERT部分在之前参数基础上更新）、词性标注、自然语言推理（前提与假设）、问答系统。</p><p>BERT可以视为Deep版的CBOW，而且可以根据上下文情况得到多义字不同的表征向量，Contextualized word embedding。</p><h3 id="2-扩展工作"><a class="markdownIt-Anchor" href="#2-扩展工作"></a> 2. 扩展工作</h3><p>BERT训练过程需要耗费非常多的资源，所以大家致力于研究&quot;BERT胚胎学&quot;，研究训练中模型的发展过程。</p><p>Pre-train seq2seq模型，将一些故意“弄坏”的输入给Encoder，要求Decoder输出原本完好的数据。</p><p>论文BART研究使用了很多种“弄坏”输入的训练方法，表明组合起来效果比较好。</p><p>论文T5，在C4数据集上训练。</p><p>多语言Bert，multi-bert，可以进行zero-shot阅读理解。</p><h3 id="3-gpt系列模型"><a class="markdownIt-Anchor" href="#3-gpt系列模型"></a> 3. GPT系列模型</h3><p>预测下一个token，类似于Transformer的Decoder。所以GPT有生成能力（独角兽形象，因为写了一个有关独角兽的假新闻）。</p><p>直接有Few-shot learning、one-shot learning、zero-shot learning。</p><p>另外还有很多类型的自监督学习模型，应用在更多不同领域中，如下图所示：</p><div align="center">  <img src="/2021/06/16/lhy-2021-b/self.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="自编码器"><a class="markdownIt-Anchor" href="#自编码器"></a> 自编码器</h2><p>李老师认为autoencoder可以视为self-supervised中pre-train的一种，因为同样也是没有应用到标签数据。</p><p>Autoencoder流程和之前提到的Cycle GAN类似，常见作用是降维。</p><p>De-noising Autoencoder，设计思想和Bert中的mask很类似。Bert可以视为一种De-noising Autoencoder。</p><p>Feature Disentangle技术，即有可能知道autoencoder中每个维度代表了什么信息。可以应用到语者转换（变声器）等领域。比如可以支持双方语者间不需要相同语料。最代表性的工作是<strong>VQVAE</strong>，可以学习到最基本的发音单位，也可以做到文章摘要。</p><p>Decoder可以视为一个生成器，比如VAE</p><p>Autoencoder可以用来做压缩与解压，lossy compression。</p><p>重点应用场景还有<strong>异常检测</strong>，比如欺诈检测（<a href="https://www.kaggle.com/mlg-ulb/creditcardfraud/home%EF%BC%89%EF%BC%8C%E7%BD%91%E7%BB%9C%E5%85%A5%E4%BE%B5%E6%A3%80%E6%B5%8B%EF%BC%88http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html%EF%BC%89%EF%BC%8C%E7%99%8C%E7%97%87%E7%BB%86%E8%83%9E%E6%A3%80%E6%B5%8B%E7%AD%89%E3%80%82%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E5%A4%A7%E5%A4%9A%E6%95%B0%E6%95%B0%E6%8D%AE%E9%83%BD%E6%98%AF%E6%AD%A3%E5%B8%B8%E6%95%B0%E6%8D%AE%E3%80%82" target="_blank" rel="noopener">https://www.kaggle.com/mlg-ulb/creditcardfraud/home），网络入侵检测（http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html），癌症细胞检测等。数据集中大多数数据都是正常数据。</a></p><h2 id="对抗攻击"><a class="markdownIt-Anchor" href="#对抗攻击"></a> 对抗攻击</h2><h3 id="1-攻击方式"><a class="markdownIt-Anchor" href="#1-攻击方式"></a> 1. 攻击方式</h3><p>超出范围的噪声值直接拉回范围内fix就好。</p><p>代表方法FGSM，只使用一次迭代，在这次迭代中并不直接使用梯度进行更新，而是取更新方向（一击必杀）。也有iterative FGSM。</p><p>白盒攻击</p><p>黑盒攻击，可以训练一个proxy模型，模仿原本的攻击对象。如果连训练资料都没有的话，就自备数据，获取其检测结果。实验中黑箱攻击效果还不错，target attack会相对比较难一点。</p><p><strong>Ensemble Attack</strong>技巧</p><p>为什么攻击很容易成功？《Adverserial Examples are not bugs, they are features》部分人认为可能问题不是出现在模型上而是数据上。</p><p>One pixel attack，只修改图片里的一个像素点就完成攻击。</p><p>Universal attack，所有的图片都可以用着一个噪声完成攻击。</p><p>Adversarial Reprogramming，操控模型去做本来不是他想做的事情。</p><p>Backdoor，在模型中开后门，从训练阶段就开始攻击</p><h3 id="2-防御方式"><a class="markdownIt-Anchor" href="#2-防御方式"></a> 2. 防御方式</h3><p><strong>被动防御</strong>，比如smoothing（模糊化），compression（压缩后解压），Generator（重新产生输入数据）、Randomization等。</p><p>但攻击者了解这些防御措施后，这些措施就会失效。</p><p><strong>主动防御</strong>，adversarial training（可以视为一种data augmentation），这是一种比较吃运算资源的方法，提升效率是一个研究点。</p><h2 id="可解释性"><a class="markdownIt-Anchor" href="#可解释性"></a> 可解释性</h2><p>explainable vs interpretable</p><p>是否有解释性又高，能力又强的模型呢？比如决策树。</p><p>可解释性ML的目标如何设定？</p><p><strong>Local explaination</strong>，比如“为什么认为这张图片是一只猫？”</p><p>使用SmoothGrad，随机加入噪声，生成saliency map。但是这种方法会有Gradient Saturation的问题，为解决这一问题提出Integrated gradient分析。</p><div align="center">  <img src="/2021/06/16/lhy-2021-b/noise.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>可以取中间层向量可视化，也可以在中间层加入probe。</p><p><strong>Global explainable</strong>，比如“什么样的图片叫做一只猫？”</p><p>寻找使经过卷积层后产生的feature map的元素加和最大的输入X。但这样通常会找到一堆噪声，所以还要加上一些约束条件，比如限制X中的非零元素个数。如果想要产生清晰地图片，可以加入一个生成器。</p><p>目前explainable AI其实更多的是倾向于产生人类比较喜欢的结果。</p><p><strong>使用比较简单的模型</strong>，模仿复杂深度学习模型。比如LIME（Local Interpretable Model-Agnostic Explanations）。</p><h2 id="领域自适应domain-adaptation学习一下助教课"><a class="markdownIt-Anchor" href="#领域自适应domain-adaptation学习一下助教课"></a> 领域自适应（Domain Adaptation）&lt;学习一下助教课&gt;</h2><p><strong>Domain Shift（概念漂移）</strong>，训练集和测试集上的数据分布不同，比如输入数据的变化，输出的分布有变化（即标签分布的变化）以及输入和输出关系的变化。[ 课程专注于输入数据的变化部分 ]</p><p><strong>领域自适应</strong>可以视为迁移学习其中的环节，根据目标领域中的数据分为以下几种情况：</p><ul><li>有少量带标签数据；使用原始数据训练模型后利用目标领域数据进行微调，但注意不要过拟合。</li><li>有大量无标签数据；训练一个模型提取器，以及一个领域分类器；Feature Extractor的目标是骗过Domain Classifier，思路和GAN类似，原始论文中的目标函数设计如下。另外还有<strong>DIRT-T，Maximum Classifier Discrepancy</strong>，它们使用的思想是，将Feature Extractor的训练目标设定为让目标数据最终提取出来的特征分布距离当前分类界限越远越好。</li></ul><div align="center">  <img src="/2021/06/16/lhy-2021-b/da.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><ul><li><p>只有少量无标签数据；使用<strong>Testing Time Training（TTT）</strong></p></li><li><p>对目标领域一无所知（<strong>Domain Generalization</strong>）<a href="https://ieeexplore.ieee.org/document/8578664%EF%BC%9Bhttps://arxiv.org/abs/2003.13216" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/8578664；https://arxiv.org/abs/2003.13216</a></p></li></ul><h2 id="网络压缩"><a class="markdownIt-Anchor" href="#网络压缩"></a> 网络压缩</h2><p>因为需要考虑终端算力及隐私问题，所以需要进行模型压缩。本课程中只介绍了和软件相关的部分。</p><h3 id="1-network-prunning"><a class="markdownIt-Anchor" href="#1-network-prunning"></a> 1. Network Prunning</h3><p>首先训练一个大模型，之后将网络中的一些参数/神经元删减掉，之后经过fine-tune让效果回升一下。以上过程可以反复迭代。</p><p>以参数为单位（weight prunning）进行裁剪，会出现形状不规则的神经网络模型不太好实现，GPU加速也不容易的问题。所以很多人是直接把参数设为0，但这样做并没有真的把网络变小。而以神经元为单位（neuron pruning）做，比较容易编程实现。</p><p><strong>大乐透假说</strong>，但也有人反对这一假说。</p><p>后续有研究调研各种prunning策略的效果，并试图解释其中的原因。</p><p>《Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask》</p><p>《Weight Agnostic Neural Networks》</p><h3 id="2-knowledge-distillation"><a class="markdownIt-Anchor" href="#2-knowledge-distillation"></a> 2. Knowledge distillation</h3><p>知识蒸馏，大的模型为teacher network，之后据此学习一个小的模型 student network。</p><p>比如目前打比赛的时候用ensemble的方法比较好，但是实际应用中可以简化一个小的模型去学习多个模型的ensemble结果。</p><p>temperature for softmax</p><h4 id="3-parameter-quantization"><a class="markdownIt-Anchor" href="#3-parameter-quantization"></a> 3. Parameter Quantization</h4><p>Less bit represent: 可以用更少的bit存储参数。</p><p>Weight clustering，对参数进行分区，每个分区选取一个代表。</p><p>Binary weights，参数只有+1，-1两种可能。</p><h4 id="4-depthwise-separable-convolution"><a class="markdownIt-Anchor" href="#4-depthwise-separable-convolution"></a> 4. Depthwise Separable Convolution</h4><p>Depthwise convolution, filter数目和channel数目相同，每个filter只管一个channel，channel间没有任何交互。</p><p>Pointwise convolution，加入多个1x1的filter，作用是考虑不同channel之间的关系。</p><p>这个思路的来源是low rank approximation，矩阵分解的感觉，将一层拆成两层减少参数。</p><p>另外还有很多设计思路，SqueezeNet、MobileNet、ShuffleNet、Xception、GhostNet等等。</p><h4 id="5-dynamic-computation"><a class="markdownIt-Anchor" href="#5-dynamic-computation"></a> 5. Dynamic Computation</h4><p>希望模型可以自由调整其需要的运算资源。</p><p>可以在每一层额外加输出层，运算资源不够的时候就提前输出结果。</p><div align="center">  <img src="/2021/06/16/lhy-2021-b/dd.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>也可以让调整网络模型宽度（Slimmable Neural Networks）。</p><p>或者可以让模型自己决定，调整深度和宽度，比如有些样本非常容易判断，不需要经过特别多层。</p><div align="center">  <img src="/2021/06/16/lhy-2021-b/cp.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>以上这些方法可以结合使用。</p><h2 id="相关研究方向"><a class="markdownIt-Anchor" href="#相关研究方向"></a> 相关研究方向</h2><h3 id="1-feature-disentangle关于autoencoder特征的解释性"><a class="markdownIt-Anchor" href="#1-feature-disentangle关于autoencoder特征的解释性"></a> 1. Feature Disentangle（关于Autoencoder特征的解释性）</h3><p><a href="https://arxiv.org/abs/1904.05742" target="_blank" rel="noopener">https://arxiv.org/abs/1904.05742</a></p><p><a href="https://arxiv.org/abs/1804.02812" target="_blank" rel="noopener">https://arxiv.org/abs/1804.02812</a></p><p><a href="https://arxiv.org/abs/1905.05879" target="_blank" rel="noopener">https://arxiv.org/abs/1905.05879</a></p><h3 id="2-tree-as-embedding"><a class="markdownIt-Anchor" href="#2-tree-as-embedding"></a> 2. Tree as Embedding</h3><p><a href="https://arxiv.org/abs/1806.07832" target="_blank" rel="noopener">https://arxiv.org/abs/1806.07832</a></p><p><a href="https://arxiv.org/abs/1904.03746" target="_blank" rel="noopener">https://arxiv.org/abs/1904.03746</a></p><h3 id="3-模型攻击"><a class="markdownIt-Anchor" href="#3-模型攻击"></a> 3. 模型攻击</h3><p>《Adverserial Examples are not bugs, they are features》</p><p>文字领域对抗攻击，加入一个短句后，所有问答都失效，<a href="https://arxiv.org/abs/1908.07125" target="_blank" rel="noopener">https://arxiv.org/abs/1908.07125</a></p><p>CCS’16 考虑物理世界特性的CV模型攻击，Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition</p><p>Adversarial Reprogramming，<a href="https://arxiv.org/abs/1806.11146" target="_blank" rel="noopener">https://arxiv.org/abs/1806.11146</a></p><p>Backdoor，<a href="https://arxiv.org/abs/1804.00792" target="_blank" rel="noopener">https://arxiv.org/abs/1804.00792</a></p><p>Adversarial Training for free，<a href="https://arxiv.org/abs/1904.12843" target="_blank" rel="noopener">https://arxiv.org/abs/1904.12843</a></p><h3 id="4-概念漂移比较重要"><a class="markdownIt-Anchor" href="#4-概念漂移比较重要"></a> 4. 概念漂移（比较重要）</h3>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习笔记</title>
    <link href="/2021/05/22/pytorch/"/>
    <url>/2021/05/22/pytorch/</url>
    
    <content type="html"><![CDATA[<p>感觉相较于Tensorflow，Pytorch更适合科研人员使用，这边是一些关于Pytorch学习的资源与笔记。</p><h2 id="五小时官网教程"><a class="markdownIt-Anchor" href="#五小时官网教程"></a> 五小时官网教程</h2><p>教程中从numpy开始一步步自动化至调用pytorch模型的教学方式非常好。</p><p>B站资源，<a href="https://www.bilibili.com/video/BV1MU4y1p74U" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1MU4y1p74U</a></p><p>名词解释：epoch、batch、iteration</p><p>如何构造数据集，Dataset、DataLoader，可以直接分batch，直接shuffle，可以设置n_workers提升速度。</p><p>Transformers，ToTensor、MulTransform，__call__函数</p><p>Pytorch中的CrossEntropy层已经加入了softmax，接受的输入，Y是标签而非one-hot编码，预测值是logits而不是softmax之后的。</p><p>BCELoss函数，需要自己在神经网络设计中加入sigmoid层。</p><p>激活函数：step function、Sigmoid（常用于二分类最后一层）、TanH、ReLU（常用于隐藏层）、Leaky ReLU（尝试解决梯度消失问题）、softmax（常用于多分类最后一层）。</p><p>迁移学习，只在新任务数据上微调最后几层。pretrained = True</p><pre><code class="hljs python">model = models.resnet18(pretrained = <span class="hljs-literal">True</span>)num_frs = model.fc.in_featuresmodel.fc = nn.Linear(num_frs, <span class="hljs-number">2</span>)model.to(device)<span class="hljs-comment"># scheduler</span>step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size =<span class="hljs-number">7</span>, gamma=<span class="hljs-number">0.1</span>)model = train_model(model, criterion, optimizer, scheduler, num_epoches=<span class="hljs-number">20</span>)<span class="hljs-comment">#for epoch in range(100):</span><span class="hljs-comment">#    train()</span><span class="hljs-comment">#    evaluate()</span><span class="hljs-comment">#    scheduler.step()</span><span class="hljs-comment"># freeze all the previous parameters</span>model = models.resnet18(pretrained = <span class="hljs-literal">True</span>)<span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():    param.requires_grad = <span class="hljs-literal">False</span></code></pre><p>TensorBoard，可以支持pytorch，使用torch.utils.tensorboard中的SummaryWriter即可，官方材料有（<a href="https://pytorch.org/docs/stable/tensorboard.html%EF%BC%89%E3%80%82" target="_blank" rel="noopener">https://pytorch.org/docs/stable/tensorboard.html）。</a></p><p>保存和加载模型推荐使用下面的方式：</p><pre><code class="hljs python">FILE = <span class="hljs-string">'model.pth'</span>torch.save(model.state_dict(), FILE)loaded_model = Model(n_input_features = <span class="hljs-number">6</span>)loaded_model.load_state_dict(torch.load(FILE))loaded_model.eval()<span class="hljs-comment">#checkpoint</span>checkpoint = &#123;    <span class="hljs-string">"epoch"</span>:<span class="hljs-number">90</span>,    <span class="hljs-string">"model_state"</span>:model.state_dict(),    <span class="hljs-string">"optim_state"</span>:optimizer.state_dict()&#125;torch.save(checkpoint, <span class="hljs-string">'checkpoint.pth'</span>)loaded_checkpoint = torch.load(<span class="hljs-string">'checkpoint.pth'</span>)epoch = loaded_checkpoint[<span class="hljs-string">'epoch'</span>]</code></pre><p>将GPU上训练的模型加载到CPU上需要进行一点调整，写map_location参数。</p><h2 id="相关资源"><a class="markdownIt-Anchor" href="#相关资源"></a> 相关资源</h2><p>官网教程：</p><p>《Dive into deeplearning》pytorch版本：</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅《机器/深度学习》2021笔记（一）</title>
    <link href="/2021/05/12/lhy-2021/"/>
    <url>/2021/05/12/lhy-2021/</url>
    
    <content type="html"><![CDATA[<h1 id="背景介绍"><a class="markdownIt-Anchor" href="#背景介绍"></a> 背景介绍</h1><p>官网 <a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html" target="_blank" rel="noopener">https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html</a></p><p>数学基础能力：微积分、线性代数和概率论。使用Python和Google Colab，也使用Kaggle。</p><p>这门课程主要是针对深度学习并且涉及到比较前端的技术。如果希望了解机器学习基础可以尝试林轩田《机器学习基石与技法》。</p><p>[ 这类在线课程重点还是要把作业好好完成哦~ ]</p><p>本篇包括深度学习基础、CNN、注意力机制和生成对抗网络部分。</p><div align="center">  <img src="/2021/05/12/lhy-2021/good.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h1 id="深度学习"><a class="markdownIt-Anchor" href="#深度学习"></a> 深度学习</h1><p>机器学习可大约视为一个“找方程的过程”。</p><p>在分类和回归两类问题之外，还有很大一部分内容，叫做<strong>Structed Learning</strong>，即机器生成某些结构型数据。</p><p>Error Surface，不同参数下的损失值等高线图。</p><p>Model Bias，来自于模型自身表达能力的限制，比如线性模型无法分类“异或问题”</p><p>这次课中引入Sigmoid的、Hard Sigmoid以及神经元、神经网络的思路有点意思~</p><p>两个ReLU叠起来就可以合成一个Hard Sigmoid。</p><div align="center">  <img src="/2021/05/12/lhy-2021/sigmoid.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>深度神经网络最开始应用在CV领域，2015年Residual Net有152层（训练有特殊操作，skip connection）。<p>为什么选择深而不是选择“胖”呢？Fat network 哈哈哈。</p><h2 id="一-神经网络训练问题"><a class="markdownIt-Anchor" href="#一-神经网络训练问题"></a> 一. 神经网络训练问题</h2><p>通用的模型训练方法如下：</p><div align="center">  <img src="/2021/05/12/lhy-2021/road.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>训练集上loss偏大到底是因为1）model bias还是因为2）优化策略不合适呢？</p><p>可以通过比较不同的模型解决这一问题，如果深层模型在训练集上的效果没有浅层模型好，那么一定是优化策略的问题。</p><p>测试集结果比训练集结果差才可能是<strong>过拟合（overfitting）</strong>，还有一种可能是mismatch，由于训练资料和测试资料的分布不同导致。</p><p>最根本解决overfitting的办法其实是<strong>增加训练资料，或者叫Data Augmentation</strong>。比如CV领域将图片翻转、放大等。但Augmentation不要乱做，要有比较说得过去的理由，比如CV领域很少会把图片上下颠倒…</p><p>另一种方式就是限制模型复杂度，比如加入专家经验限制模型只能是二次函数、减少DNN的神经元、共享参数、减少特征、早停、正则化、Dropout等。</p><div align="center">  <img src="/2021/05/12/lhy-2021/over.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="1-局部最小值local-minima与鞍点saddle-point"><a class="markdownIt-Anchor" href="#1-局部最小值local-minima与鞍点saddle-point"></a> 1. 局部最小值（local minima）与鞍点（saddle point）</h3><p>这两种梯度为0的位置统称为“critical point”，如何区分二者呢？</p><p>使用泰勒级数展开近似当前位置的损失函数形状，具体来说，在以上两种位置一阶项均为零，可以根据二阶项区分。如果二阶项都大于零则当前为局部最小，如果二阶项都大于零则当前为局部最大，否则是鞍点。使用线性代数里的技巧，我们不需要将当前点周围的点都带入去判断二阶项是否大于零，<strong>只需要关注H即可</strong>。如果H是正定矩阵，即所有特征值都是正数，则假设1成立。</p><div align="center">  <img src="/2021/05/12/lhy-2021/critical.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>如果处于鞍点可以沿着特征值小于零的特征向量方向更新参数，可以继续减小损失值。</p><p>[ 但上述方法<strong>实际很少使用</strong>，因为二次微分计算量很大，而且还要算出特征向量 ]</p><p>是否维度越高可以走的路越多呢？定性来解释也许局部最小值位置是很少的。</p><h3 id="2-批次batch与动量momentum"><a class="markdownIt-Anchor" href="#2-批次batch与动量momentum"></a> 2. 批次（Batch）与动量（Momentum）</h3><p>为什么要区分批次？（Minibatch和batch通用）</p><p>批量更新概念厘清，epoch，batch，update的含义：</p><div align="center">  <img src="/2021/05/12/lhy-2021/batch.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>GPU可以实现batch内资料的并行运算，但如果batch过大的话计算时间还是会增加的。正是由于有并行运算的能力，如果batch size设定过小的话，计算完所有数据所用的时间反而会较大些batch设置下的长。</p><p>但有时正是由于small batch的更新过程比较noisy，反而会在最后有更好的效果。</p><p><strong>有很多paper研究如何均衡batch size从而结合两方面的优势，加快模型训练过程。</strong></p><div align="center">  <img src="/2021/05/12/lhy-2021/smallb.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>缓解SGD陷入局部最小值问题，加入momentum向量。动量有两种解读方法：1）前一步方向减去当前梯度方向；2）之前所有梯度方向的加权和。</p><h3 id="3-adaptive-learning-rate自适应学习率"><a class="markdownIt-Anchor" href="#3-adaptive-learning-rate自适应学习率"></a> 3. Adaptive learning rate（自适应学习率）</h3><p>训练卡住不一定是局部最小/全局最小/鞍点等critical points，也可能是学习率不合适一直在震荡。</p><p><strong>Adagrad</strong>使用了RMS（Root Mean Square），计算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span>参数记录之前梯度值，直观上理解，针对梯度较大的参数采取小的学习率，反之亦然。</p><p><strong>RMSProp</strong>支持（没有论文，直接在讲课时推导出来），第一步与Adagrad相似，但后面计算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span>的时候调整当前梯度权重。带有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>参数。更多强调针对同一参数，当其处于大梯度阶段会使用小学习率。</p><p>当前最常用的优化器为<strong>Adam</strong>，即RMSProp+Momentum。</p><p>当前还使用Learning Rate Scheduling技术，比如Learning rate decay（随着训练进行学习率要减小），Warm up（学习率要先变大后逐渐变小，现在训练Bert的时候常用，之前在Residual network，transformer等都有提及）等。</p><p>为什么需要Warm up？给出的一个解释是，自适应的学习率调节是以以往梯度值统计信息为基础的，而刚开始训练的时候数据量不足，所以让学习率小一些。</p><div align="center">  <img src="/2021/05/12/lhy-2021/schedule.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>自适应学习率总结如下图所示。直观上理解，momentum是直接对以往梯度的累加，而<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span>只关注大小，所以即便它们分别处于分子和分母的位置，也不会将效果抵消。</p><div align="center">  <img src="/2021/05/12/lhy-2021/adaptive.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="4-分类问题-损失函数"><a class="markdownIt-Anchor" href="#4-分类问题-损失函数"></a> 4. 分类问题 &amp; 损失函数</h3><p>由回归问题修改引入分类问题。</p><p><strong>softmax函数</strong>处理结果后得到预测标签。<strong>为什么要使用softmax？</strong></p><p>softmax的效果：1）将结果归一化，0、1之间加和为1；2）扩大值之间的差距</p><p>softmax的输入称为logit。</p><p>二分类问题直接去<strong>sigmoid</strong>就可以，二者是等价的。</p><p>损失函数选择：1）均方误差，MSE；<strong>2）cross-entropy，交叉熵</strong>。最小化交叉熵等价于最大似然。</p><p>Pytorch里面将cross-entropy和softmax内建在一起，不如需要在设计网络时手动添加。</p><h3 id="5-批次标准化batch-normalization"><a class="markdownIt-Anchor" href="#5-批次标准化batch-normalization"></a> 5. 批次标准化（Batch Normalization）</h3><p>帮助训练时使梯度下降更快收敛。因为这样使得error surface没有那么崎岖。</p><p>在深度学习模型中，对隐藏层的输入（下图中的z向量）也进行标准化，而这个过程使得训练样本间不再独立。再考虑到GPU资源限制，我们在一个batch中进行这样的归一化。</p><div align="center">  <img src="/2021/05/12/lhy-2021/batchn.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>但在测试阶段不存在上述batch概念，Pytroch会基于训练阶段不同batch计算出的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">μ</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span>，计算moving average直接应用在测试样本中。</p><p>Batch Normalization的作者提出了internal covariate shift概念，但之后又有一篇文章《How dose batch normalization help optimization》打脸了这个观点…但他从实验和理论上解释了为什么BatchNorm效果会比较好，但同时还存在其它方式也能达到相同效果。</p><p>此外还有很多归一化方法比如，Batch Renormalization，Layer Normalization，Instance Normalization，Group Normalization，Weight Normalization， Spectrum Normalization等。</p><h2 id="二-神经网络架构设计cnn为例"><a class="markdownIt-Anchor" href="#二-神经网络架构设计cnn为例"></a> 二. 神经网络架构设计（CNN为例）</h2><p>图片分类问题几点观察和对应的简化方法：</p><ul><li>识别关键模式，而并不需要看整张完整图片。因此，设置<strong>receptive field</strong>，限制每个神经元关注的范围；不同神经元的receptive field可以相同，可以重叠，可以调整不同大小，或者限制channel，可以设置成长方形（但通常不用）。一般会设计为，会看全部channel，所以只用设置kernel size即可（比如3 X 3, 7 X 7等），同一个receptive field会对应有一组神经元，不同receptive field之间会有重叠（步长 stride），在边缘处会补全（padding）。</li><li>同样的关键模式可能出现在图片的不同区域。因此，提出<strong>共享参数</strong>思想，让不同神经元具有同样的权重参数。</li><li>对一张图片进行subsampling并不影响识别。因此，提出pooling操作，其中没有需要学习的参数。而随着运算力的增强，大多数网络架构设计中不再使用pooling层而追求更高的效果。</li></ul><p>简化过程如下图所示，将CNN用在图像外的其它领域中时需要<strong>仔细分析一下是否具有上述特性</strong>。比如alpha go并没有使用pooling。</p><div align="center">  <img src="/2021/05/12/lhy-2021/cnn.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>另外一种思路是由X个不同的fiter扫过整个图片（是参数共享的另一种面向），得到X channels的feature map（可以视为另外一张新的图片，只不过channel不再代表RGB）。</p><div align="center">  <img src="/2021/05/12/lhy-2021/cnnl.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>经过CNN之后的结果要经过Flatten，拉直成向量，再加一层全连接层，加softmax之后得到最终的结果。</p><p>CNN自己并不能够处理影像的放大、缩小或者旋转的问题，我们<strong>需要数据增强</strong>。<strong>Spatial Transformer layer</strong>可以处理这个问题。</p><p>CNN当前也应用在语音识别、图像识别等领域。</p><h2 id="三-自注意力机制"><a class="markdownIt-Anchor" href="#三-自注意力机制"></a> 三. 自注意力机制</h2><h3 id="1-self-attention"><a class="markdownIt-Anchor" href="#1-self-attention"></a> 1. Self-attention</h3><p>假设模型输入是一个<strong>长度可变</strong>的向量集合会如何？比如一段声音信号，社交网络等。</p><p>以输出向量数量可以区分三类任务：1）N个输入N个输出，比如词性标定；2）N个输入1个输出，比如情感分析；3）N个输入不确定个数的输出，seq2seq，比如机器翻译。</p><p>Self-attention会接收一整个sequence的信息（N个输入），并返回N个输出，它们都是考虑一整个sequence的内容而得到的，之后再经过FC得到最终结果。如下图所示的结构可以叠加多层，self-attention和FC交替使用。最知名的相关文章为《Attention is all you need》。</p><div align="center">  <img src="/2021/05/12/lhy-2021/self.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>计算输出向量的步骤（针对每个输入并行进行如下操作）：</p><ol><li>找出其他向量与<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>a</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">a^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>的关联程度<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>；这里有不同的做法，常见的是<strong>dot-product</strong>，additive等。所有结果会经过一个类似softmax的非线性层；</li><li>将输入向量乘上<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mi>v</mi></msup></mrow><annotation encoding="application/x-tex">W^v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span></span></span></span></span></span></span>，得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>向量；</li><li>b向量即为各v向量带<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>加权。</li></ol><p>相关术语：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span></span></span>为query，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>为key，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span>为attention score。</p><div align="center">  <img src="/2021/05/12/lhy-2021/dot.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>另外可以从矩阵运算角度总结如下，self-attention中需要学的参数是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mi>q</mi></msup><mo separator="true">,</mo><msup><mi>W</mi><mi>k</mi></msup><mo separator="true">,</mo><msup><mi>W</mi><mi>v</mi></msup></mrow><annotation encoding="application/x-tex">W^q, W^k,W^v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span></span></span></span></span></span></span>。</p><div align="center">  <img src="/2021/05/12/lhy-2021/selfm.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>进阶版本为多头自注意机制（<strong>Multi-head</strong> self attention），让不同的q负责不同种类的相关性，计算过程与上述类似，不同的head种类分别计算。</p><p>目前来讲，self-attention完全<strong>没有用到位置信息</strong>。可以使用<strong>positional encoding技术</strong>加入位置信息。最早是使用人工设定的位置信息，目前可以根据资料学习。</p><p>Self-attention应用在Transformer，Bert以及其它如语音辨识（Truncated self-attention，不要看一整句话，数据量太大了）、图像识别（可以将每个位置的pixel视为一个3维向量，比如self-attention GAN、DETR等）、图数据等任务中。</p><p><strong>Self-attention vs CNN</strong>：CNN可以视为简化版的self-attention，后者理解为receptive field是自动被学出来的而非人为划定的。《On the relationship between self-attention and convolutional layers》</p><p>**Self-attention vs RNN：**当前用到RNN的任务大多可以用self-attention取代。self-attention可以很轻易地利用远距离信息，而且可以平行处理。《Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention》</p><p><strong>self-attention for graph：</strong> 可以选择以图中的边确定节点向量间的关联性。也是GNN的一种</p><p>self-attention的最大问题在于运算量非常大，目前有很多变体<strong>致力于提升效率</strong>。《Long Range Arena: A Benchmark for efficient transformers》，《Efficient Transformers：A survey》。</p><h3 id="2-transformer"><a class="markdownIt-Anchor" href="#2-transformer"></a> 2. Transformer</h3><p>Seq2seq应用案例：1）是语法解析也可以视为是seq2seq，可以直接把语法的树状结构转换为sequence。《Grammar as a Foreign Language》；2）Multi-label分类；3）Object detection。</p><p>Transformer一个seq2seq的模型，分为Encoder和Decoder两个部分，每个部分有多个block，block中有self-attention层和FC层，且遵循residual架构，而且使用了layer normalization。Encoder架构如下图所示：</p><p>（但原始的Transformer的架构不一定是最优设计，有其它研究工作进行调整）</p><p>《On Layer Normalization in the Transformer Architecture》、《PowerNorm: Rethinking Batch Normalization in Transformers》</p><div align="center">  <img src="/2021/05/12/lhy-2021/trans.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>Decoder部分有两种，比较常见的是<strong>Autoregressive</strong>的形式。基础架构和encoder相似，但需要使用masked self-attention，还需要设置BEGINE和END符号。</p><p>Non-autoregressive模型（NAT），一次产生所有输出，可以控制输出长度，平行化是它的优势。由于self-attention与次序无关，所以现在NAT的decoder也是一个热门研究方向。</p><p>两种模型异同如下图所示，当前NAT的效果基本赶不上AT，主要是由于Multi-modality的问题，这也是一个热门研究方向。</p><div align="center">  <img src="/2021/05/12/lhy-2021/NAT.Jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>Decoder和Encoder连接的桥梁是<strong>cross-attention</strong>，Decoder提供q向量，k和v向量来自Encoder的输出最终生成v向量提供给FC。</p><p>训练过程中，Decoder的输入是正确答案，<strong>Teacher Forcing</strong>。</p><p>可以直接复制一些内容作为回答的模型，包括Pointer Network，Copying Network。（科研方向）</p><p><strong>Guided Attention</strong>，强迫模型的attention有固定的样子，比如语音合成中要求attention必须由左向右。（科研方向）</p><p><strong>Beam Search</strong>（偶尔有用），而非Greedy Decoding，</p><p>如果需要一些创造力的话，可以在训练decoder的时候加入一些杂讯。TTS领域，测试的时候也会加入一些杂讯。</p><p><strong>Scheduled Sampling</strong>，解决exposure bias，直接在训练时给Decoder一些错误的输入。原本的scheduled sampling会伤害到模型的平行化，所以针对transformer有所调整。</p><p>[ Accept that nothing is perfect. True beauty lies in the cracks of imperfection. ]</p><p>When you don’t know how to optimize, just use reinforcement learning (RL)！比如无法给出一个可微的损失函数。</p><h2 id="生成对抗网络"><a class="markdownIt-Anchor" href="#生成对抗网络"></a> 生成对抗网络</h2><h3 id="1-gan基础"><a class="markdownIt-Anchor" href="#1-gan基础"></a> 1. GAN基础</h3><p>Generator接收一个输入X和一个分布Z，输出一个分布Y。</p><p>训练时可以把生成器和判别器视为一个大网络，一部分fix，反复训练。</p><div align="center">  <img src="/2021/05/12/lhy-2021/gan.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>最终Generator可以根据不同的输入生成一些中间状态，比如从严肃到大笑。</p><p>本质上，GAN是突破了我们无法计算生成数据与真实数据分布差异的问题，divergence between distributions of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>G</mi></msub></mrow><annotation encoding="application/x-tex">P_G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{data}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。有了GAN以后，只要我们知道如何在G和data中采样样本，就可以<strong>完成divergence计算</strong>，即使用Discrimincriminator。</p><p>原始的GAN论文中关于判别器的训练貌似是从二分类任务中发散过来，比如训练目标为最大化负交叉熵，也即最小化交叉熵。经过推导发现其训练目标<strong>与JS divergence有关</strong>。</p><div align="center">  <img src="/2021/05/12/lhy-2021/js.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>所以可以把生成器的目标函数中的divergence项替换为判别器中的目标函数项，形成min-max形式。</p><div align="center">  <img src="/2021/05/12/lhy-2021/theory.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>其实也可以设计不同的目标函数得到不同的divergence计算，相关论文（<a href="https://arxiv.org/abs/1606.00709%EF%BC%89" target="_blank" rel="noopener">https://arxiv.org/abs/1606.00709）</a></p><h3 id="2-gan训练技巧"><a class="markdownIt-Anchor" href="#2-gan训练技巧"></a> 2. GAN训练技巧</h3><p>GAN以难训练而闻名。经常出现如果使用简单的二分类模型做判别器的话，出现百分百分辨出来的情况。</p><p>实际中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>G</mi></msub></mrow><annotation encoding="application/x-tex">P_G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{data}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>间可重叠/相交的部分非常少，可从以下两个角度解释：1）<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>G</mi></msub></mrow><annotation encoding="application/x-tex">P_G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{data}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>都是低维数据在高维空间的映射，重叠/相交的部分基本可以忽略；2）我们对于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>G</mi></msub></mrow><annotation encoding="application/x-tex">P_G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{data}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的了解仅来自于采样样本，可能没有采样到重叠区域。</p><p>所以JS divergence可能并不是很合适，因为对于两个没有重叠的分布，JS divergence永远都是log2，而没有中间值。</p><p><strong>WGAN（使用Wasserstein distance）</strong>，可以解决上述问题。其实计算W distance还是比较麻烦的，可以简单的视为解下面的优化问题。</p><div align="center">  <img src="/2021/05/12/lhy-2021/wgan.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>注意，这边要求判别器是平滑的。最开始是通过直接截断实现的，即限制D的输出值范围。之后有了<strong>Improved WGAN（使用Gradient Penalty）</strong>。在之后还有**SNGAN（Spectral Normalization）**真的做了对判别器的限制。</p><p>[ 实际训练过程和理论上有所出入，比如可能不会每轮都将判别器训到收敛 ]</p><p>另外，由于GAN中判别器和生成器是“相爱相杀，相辅相成”的，一旦其中某一个出问题，另一个也会无法训练。</p><p>相对而言，用GAN生成一段文字是最困难的，有很长一段时间没有人可以做到这一点，直到<strong>ScratchGAN</strong>出现。</p><p>其它有关生成模型的还有<strong>VAE</strong>，<strong>flow-based model</strong>等，但一般来讲，GAN生成的效果会比较好。</p><h3 id="3-gan评估"><a class="markdownIt-Anchor" href="#3-gan评估"></a> 3. GAN评估</h3><p>最直接的是直接找人来看…</p><p>如今的GAN也许还有mode dropping问题，即生成的图片多样性（diversity）不足。常用**Inception Score（IS）**定义生成结果的多样性。</p><p>也有使用**FID（Frechet Inception Distance）**衡量。</p><h3 id="4-cgan"><a class="markdownIt-Anchor" href="#4-cgan"></a> 4. CGAN</h3><p>Conditional GAN，比如输入“红头发、绿眼睛”产生相应的图片。</p><p>重点在于判别器还要接受条件输入，判断生成图片和条件是否匹配。</p><h3 id="5-在无成对数据条件下学习"><a class="markdownIt-Anchor" href="#5-在无成对数据条件下学习"></a> 5. 在无成对数据条件下学习</h3><p>Cycle GAN，比如用于图像风格迁移。网络可以设置为双向。类似的还有Disco GAN, Dual GAN都是这样的想法。</p><div align="center">  <img src="/2021/05/12/lhy-2021/cyclegan.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>另外还有进阶版StarGAN，可以在多种风格间进行转换。</p><p>另外的应用还有进行<strong>文字风格转换</strong>，长文转摘要，无监督翻译，无监督语音辨识等。</p><h2 id="相关研究点整理"><a class="markdownIt-Anchor" href="#相关研究点整理"></a> 相关研究点整理</h2><h3 id="1-均衡batch-size加速模型训练"><a class="markdownIt-Anchor" href="#1-均衡batch-size加速模型训练"></a> 1. 均衡batch size加速模型训练</h3><p>《Large Batch Optimization for Deep Learning: Training BERT in 76 minutes》</p><p>《Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes》</p><p>《Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well 》</p><p>《Large Batch Training of Convolutional Networks》</p><p>《Accurate, large minibatch sgd: Training imagenet in 1 hour》</p><h3 id="2-学习率warm-up"><a class="markdownIt-Anchor" href="#2-学习率warm-up"></a> 2. 学习率Warm Up</h3><p>RAdam，<a href="https://arxiv.org/abs/1908.03265" target="_blank" rel="noopener">https://arxiv.org/abs/1908.03265</a></p><h3 id="3-spatial-transformer-layer"><a class="markdownIt-Anchor" href="#3-spatial-transformer-layer"></a> 3. Spatial Transformer layer</h3><h3 id="4-positional-encoding"><a class="markdownIt-Anchor" href="#4-positional-encoding"></a> 4. Positional encoding</h3><p><a href="https://arxiv.org/abs/2003.09229" target="_blank" rel="noopener">https://arxiv.org/abs/2003.09229</a></p><h3 id="5-transformer-nat"><a class="markdownIt-Anchor" href="#5-transformer-nat"></a> 5. Transformer NAT</h3><h3 id="6-gan-training一些训练技巧和论文"><a class="markdownIt-Anchor" href="#6-gan-training一些训练技巧和论文"></a> 6. GAN training（一些训练技巧和论文）</h3><p><a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">https://github.com/soumith/ganhacks</a></p><p><a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">https://arxiv.org/abs/1511.06434</a></p><p><a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="noopener">https://arxiv.org/abs/1606.03498</a></p><p><a href="https://arxiv.org/abs/1809.11096" target="_blank" rel="noopener">https://arxiv.org/abs/1809.11096</a></p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cs224w《图机器学习》2021（三）知识图谱</title>
    <link href="/2021/05/10/cs224w3/"/>
    <url>/2021/05/10/cs224w3/</url>
    
    <content type="html"><![CDATA[<h2 id="相关论文"><a class="markdownIt-Anchor" href="#相关论文"></a> 相关论文</h2><p>RotateE，《RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space 》</p><h2 id="知识图谱"><a class="markdownIt-Anchor" href="#知识图谱"></a> 知识图谱</h2><p>常用的知识图谱如Google knowledge Graph、Amazon Product Graph、Facebook Graph API、IBM Watson、Microsoft Satori、Project Hanover/Literome、LinkeIn Knowledge Graph、Yandex Object Answer等。</p><p>开放的知识图谱有FreeBase、Wikidata、Dbpedia、YAGO、NELL等。它们的共同特征是1）节点、边数量巨大；2）不完整性，即很多边是缺失的。比如下图所指示的Freebase的情况：</p><div align="center">  <img src="/2021/05/10/cs224w3/freebase.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>知识图谱的应用场景主要在于，比如推荐系统、问答对话系统等。</p><h2 id="图谱补全"><a class="markdownIt-Anchor" href="#图谱补全"></a> 图谱补全</h2><p>给定头结点、关系类型信息，预测尾结点。[ 与之前介绍的链路预测任务略有不同 ]</p><p>使用shollow embeddings联合entities和relations，而不是训练一个GNN。</p><blockquote><p>Given a true triple (h, r, t), the goal is that the embedding of (h,r) should be close to the embedding of t.</p></blockquote><h4 id="1-transe"><a class="markdownIt-Anchor" href="#1-transe"></a> 1. TransE</h4><p>希望头结点和关系的表征向量加和与尾结点表征向量接近。</p><div align="center">  <img src="/2021/05/10/cs224w3/transe.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/10/cs224w3/transe_d.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>知识图谱中的关系类型：</p><ul><li>对称（symmetry）或antisymmetric，比如室友关系、上位词关系</li><li>逆（inverse），比如顾问与被顾问</li><li>传递（composition/Transitive），比如朋友关系，比如“我妈妈的丈夫是我爸爸”</li><li>一对多（1-to-N），比如“student of”</li></ul><p><strong>TransE是否可以有效表达以上这些关系？</strong></p><p>可以自然捕获反对称关系、逆关系、传递关系，但无法表达对称关系和1-to-N关系。</p><div align="center">  <img src="/2021/05/10/cs224w3/sy.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/10/cs224w3/oneN.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-transr"><a class="markdownIt-Anchor" href="#2-transr"></a> 2. TransR</h4><p>将实体和边映射到不同的向量空间，实体映射到空间<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">R^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span>，关系映射到空间<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">R^k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span>， 同时计算一个投影矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mo>∈</mo><msup><mi>R</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo>×</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">M\in R^{(k \times d)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">d</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>，计算时投影到空间<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">R^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span>，之后再按照TransE思路进行。</p><p>这样的操作下，TransR可以进一步支持对称关系、1-to-N关系（来源于投影矩阵M带来的灵活性）。但<strong>TransR不能支持传递关系</strong>，因为每种关系有自己的向量空间，而我们只有它和节点空间的投影关系M，而没有它们之间的映射关系。</p><h4 id="3-双线性建模bilinear-modeling"><a class="markdownIt-Anchor" href="#3-双线性建模bilinear-modeling"></a> 3. 双线性建模（Bilinear Modeling）</h4><p>TransE和TransR中都简单的使用预测值与目标值在嵌入空间的（L1/L2）距离作为代价函数。</p><p><strong>DistMult</strong>，使用映射后坐标乘积作为代价函数，如果存在此三元组则最终得分很高。这个方法也可以视为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mo>⋅</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">h\cdot r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span></span>与t的余弦相似度。</p><div align="center">  <img src="/2021/05/10/cs224w3/distmult.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>DistMult建模可以表达1-to-N、对称关系，但是不可以表示非对称关系、逆关系、传递关系。</p><h4 id="4-complex"><a class="markdownIt-Anchor" href="#4-complex"></a> 4. ComplEx</h4><p>基于DisMult，ComplEx将实体和关系映射到复数向量空间（Complex vector space），而只用实数部分作为评分函数。</p><div align="center">  <img src="/2021/05/10/cs224w3/complex.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>ComplEx可以支持对称、非对称、逆关系、1-to-N，但无法表达传递关系。</p><p>四种方法效果总结如下，需要依据不同的建模任务加以区分。一般来说用TransR和ComplEx的比较多。经常先用TransE快速建模看效果，之后使用更具表达性的模型比如ComplEx，RotatE（TransE in complex space）等。</p><div align="center">  <img src="/2021/05/10/cs224w3/com.jpg" srcset="/img/loading.gif" width="50%" height="40%" alt="oauth"></div><h2 id="知识推理"><a class="markdownIt-Anchor" href="#知识推理"></a> 知识推理</h2><p>不同类型的查询方式如下图，既可以表示为自然语言也可以通过图结构表达：</p><div align="center">  <img src="/2021/05/10/cs224w3/query.jpg" srcset="/img/loading.gif" width="50%" height="40%" alt="oauth"></div><p>知识图谱补全任务可以视为回答one-hop query任务。</p><p>在path query中有锚节点anchor node，比如上图中蓝色的节点。</p><p>注意，做这些查询的知识图谱可能是<strong>不完整的</strong>，需要我们自己补全，而不只是简单地遍历查询。</p><p>**那么我们是否可以先进行图谱补全，之后再遍历查询呢？不可以！**因为通过图谱补全的步骤我们会得到几乎全部三元组是否存在的概率，而几乎所有的三元组都会有非零几率，这会得到一个非常密集的图谱，而使遍历也非常困难，复杂度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msubsup><mi>d</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow><mi>L</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(d_{max}^L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">x</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>, L为path的长度。</p><p>我们希望进行<strong>一种“泛化的链路预测任务”</strong>，希望可以回答任意查询同时隐式估算丢失的信息。</p><h3 id="一-多跳查询"><a class="markdownIt-Anchor" href="#一-多跳查询"></a> 一. 多跳查询</h3><p>如何在一个知识谱图中预测出给定查询的答案？</p><p>我们需要<strong>嵌入查询</strong>，将TransE扩展到多跳推理任务中。因为TransE本身就可以支持传递关系，所以天然可以处理path queries，通过将各关系向量相加完成。</p><div align="center">  <img src="/2021/05/10/cs224w3/qe.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="二-query2box"><a class="markdownIt-Anchor" href="#二-query2box"></a> 二. Query2box</h3><p>如何完成连接查询？ （我们希望可以得到嵌入空间中两个实体集合的交集）使用box embeddings推理知识图谱。</p><div align="center">  <img src="/2021/05/10/cs224w3/box.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>龙虾教授《人格及其转变》笔记（上）</title>
    <link href="/2021/05/07/lobster/"/>
    <url>/2021/05/07/lobster/</url>
    
    <content type="html"><![CDATA[<p>乔治*皮特森是加拿大多伦多大学教授，因其在《人生十二法则》一书中提出人与人的交往关系与龙虾间有相似之处，而被网友们亲切地称呼为“龙虾教授”。</p><p>我在2020年下半年学习了《人格及其转变》课程，收获颇丰。比如，我们应生活在阴（混沌）与阳（秩序）的交界、各种神话故事中包含的“英雄出走”的内核、人与父母间的关系、大五人格（在第二部分着重介绍）等。</p><p><strong>请把自己想象成人类灵魂工程师，我们在同时塑造自己和他人的人格，我们一同努力想要达成的是什么？</strong></p><p><strong>Be what you can be; Be yourself !</strong></p><p>另外，作为土生土长的中国人，我认为在了解西方哲学思想的同时还需要学习我国传统思想文化。理性至上无法提供安身立命之精神，如<strong>王德峰</strong>教授所说，即“感性大我之重建”。</p><h2 id="英雄-神话与萨满启蒙"><a class="markdownIt-Anchor" href="#英雄-神话与萨满启蒙"></a> 英雄、神话与萨满启蒙</h2><h3 id="一-元故事-meta-story"><a class="markdownIt-Anchor" href="#一-元故事-meta-story"></a> 一. 元故事 meta-story</h3><p>各种圣经故事、神话、小说、电影等作品其实包含了统一的精神内核，即“元故事”，它们在告诉人们如何在这个世界上生存。</p><p>人类的主旨故事之一，是走出去探索前人未知的领域，直面未知带来的恐惧，获得宝物凯旋而归。比如约翰从鲸鱼肚子里出来、哈利波特与密室、狮子王、漫威系列等，总是经过<strong>乐园、失乐园、复乐园</strong>（paradise, paradise lost, paradise regained）的过程。这个过程大多不顺利，因为life is hard，我们需要有意义的东西支撑下去，履行自己的使命。</p><div align="center">  <img src="/2021/05/07/lobster/whale.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p><strong>一旦掉入“地下城”该怎么办？</strong></p><ul><li>意识到也许自己正是一切不幸的源头；</li><li>审视自己的假设和行为方式，看清楚它们是怎样阻拦着你，搞清楚要怎么改变。你在担心什么？逃避什么？你无法发展出的是什么？你要怎么重塑自己？</li><li>摊开分析，弄懂它们，然后彻底放下；</li><li>决定做该做的事，重新塑造自己，从而以一种更正确的方式生活</li></ul><p>以“哈利波特与密室”为例具体说明：要直面未知的恐惧，虽然可能会死掉，但这是最好的选择和出路；其中有一段邓布利多的凤凰重生（transformation），学习是<strong>寻找自己正在做错的事，同时失去自己的这一部分</strong>（即使是错误的），这会很痛苦，你需要让自己很大一部人人格死去，像凤凰一样。另外，哈利波特包括很多其它故事也都体现出“拥有<strong>至善人格</strong>的人被施以残酷的惩罚”的原型。</p><div align="center">  <img src="/2021/05/07/lobster/hp.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>扩展开来，人类之间也有很多共同点。</p><ul><li><p>人类的自我意识也和自我局限有关，大猩猩视频实验表示，人类无法关注到世界上的所有事。人类其实是目光狭隘的，只是通过快速移动点亮每个区域，所能看到的世界其实很小。我们活在某种感知框架里，这个框架由我们的认知决定。正如佛家思想所讲，你的价值观决定了你的认知方向。我们的注意力很大程度上是自主的，即使你认为可以掌控自己，但并不一定（比如潜意识）；</p></li><li><p>人类天生有共同的恐惧和脆弱，比如死亡焦虑，害怕蛇、黑暗、混沌等。致力于一些有意义的事可以在一定程度上抵挡脆弱；</p></li><li><p>使用了致幻剂之后，被试们描绘着相同的超体验或濒死体验；</p></li><li><p>各地古老的关于世界的描述十分相似；</p></li><li><p>我们处于社群之中是会受到血清素系统影响的（与龙虾们相似），一个理想的人是爬到某个优势等级顶端的人。我们的环境并不是自然，而是文化，是其他人。人有本性，根深蒂固，我们只能依据本性构建社会（荣格），否则这类文化就会瓦解。</p></li></ul><h3 id="二-隐喻"><a class="markdownIt-Anchor" href="#二-隐喻"></a> 二. 隐喻</h3><h4 id="1已知隐喻"><a class="markdownIt-Anchor" href="#1已知隐喻"></a> 1.已知隐喻</h4><p>一般以父亲、太阳等形象隐喻已知，它代表了文化与社会秩序等，它具体、明确、确定，会给人一定的安全感。但同样也会以为这极权、暴君，比如斯大林、纳粹党等。</p><p>彼得潘的故事告诉我们，我们无法一直缩在已知的舒适区中。如果你不在成熟的过程（自然岁月）中使用潜能的话，潜能也会自我消耗。你会自然长大，所以最好将潜能塑造成某个东西。</p><p>让某人做某事的最好方法，是禁止他做这件事而且不告诉他原因。这里也体现出人类会打破秩序、探索未知的本能。</p><h4 id="2未知隐喻"><a class="markdownIt-Anchor" href="#2未知隐喻"></a> 2.未知隐喻</h4><p>通常以黑暗之地、禁果、潘多拉魔盒、蛇等形式展现，<strong>表现为女性或阴性</strong>。它代表了潜意识、带着酒神力量的本我（弗洛伊德）、潜伏的怪兽、万物的来源与归宿。还包括伟大母亲、女王、基体（the matrix）、容器、深处、山谷、月亮等符号，是<strong>超越性的</strong>。</p><p>以迦利神像为例，它是毁灭或恐惧的具象符号。我们需要给“社会/自然/母亲”她想要的东西，与未来做交易，做出正确的牺牲，以今天的愉悦为筹码，交换明日的优势。</p><div align="center">  <img src="/2021/05/07/lobster/jl.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>我们需要主动探索未知，如果知道外面正酝酿着极大的危险，主动前往比不小心掉入要好很多。因为主动挑战威胁，身体会被激活，否则你会进入猎物模式，所以<strong>最好睁大双眼，警惕萌生的威胁，及早行动</strong>。而每次你学到人生中的一课，学习到的瞬间，你的世界都地动山摇。</p><h4 id="3-二者结合"><a class="markdownIt-Anchor" href="#3-二者结合"></a> 3. 二者结合</h4><p>我们存在其中的世界充满了动机和感情，充斥着恐怖、痛苦、喜悦与挫败，以及其他人，包含一切已知和未知，你希望在可预料当中有一些不可预料。<strong>未知中蕴含着宇宙的混沌与未来的潜能，我参与其中，将可能转变为现实</strong>。</p><p>借助<strong>太极图</strong>，我们要理解到，已知的部分可能在一瞬间变为未知，一生中可能经历无数次这样的转折。我们注定会经历混沌（chaos），也许会在某次混沌中一蹶不振，但<strong>一定要坚信混沌中蕴含着向秩序的转变</strong>。</p><div align="center">  <img src="/2021/05/07/lobster/taiji.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>你要尽力活在<strong>秩序和混沌的边界（meta-place）</strong>，主动探索未知，强化秩序。现实中，有些人可以容忍大范围的混沌，比如自由派，但有些人更喜欢现存结构的稳定性，如保守派，环境瞬息万变，没有永远的对错，这就需要<strong>双方的沟通交流</strong>。当你处于混沌与秩序的最佳边界时，你的大脑会告诉你，它会制造出一种全情投入、富含意义的感受；<strong>你足够稳定，也有充足的兴趣，达到这种完美的平衡</strong>。</p><p><strong>要如何在混沌与秩序之间斡旋呢？</strong></p><p>1）Heaven is in the unknown。有些人带着幻想面对未知，未来，梦境，创造力从这里萌发，具有很高的经验开放性。但很多患者也这样，他们具有强烈的自我批判，他们聪明，具有创造力，通过梦境与艺术探索未知。许多19、20世纪被普遍认为很伟大的人（如尼采、达尔文、陀思妥耶夫斯基、托尔斯泰、弗洛伊德、荣格等）都经历过这种创造类疾病，那是长期、深刻的、心理上的不安与不确定。<strong>通往更高智慧的道路便是要经过恐怖的地域之门。</strong></p><p>2）文化、艺术、幻想、音乐、戏剧、故事，会起到缓冲作用。我们被文化和幻想包围着，也被保卫着，我们应当尊重秩序。对历史了解的太少才会更偏爱混沌，而不是秩序。应心存感激，感谢明君，但同时也要足够聪明，知道他的另一面是邪恶暴君，这才是<strong>对世界的完整认知</strong>。必须对人性有了解，并不是作恶者在一边而受害者在另一边，<strong>人性之邪恶，人性之崇高都是你的中心元素</strong>。</p><p>3）在发展初期（比如事不过三，你要记得人们会狡辩，你要坚持住自己的观点），你最好把自己从压迫中释放出来，<strong>把真实想法讲出来</strong>（不一定正确），你要与敌人或者不同观点的人<strong>真诚交流，探索未知</strong>。</p><p>4）<strong>怨恨是一种有毒的情绪</strong>。如果你怨恨某事这意味着你应该有所成长，承担责任，不再四处抱怨，哭哭啼啼；或者这意味着正有人压迫你、欺辱你，你有东西要说出来或做出来，但你没有（因为那可能在短期内让你惹上危险）。日复一日，虽然逃避会在短期内可以保护你，但这会把你挤压变形，随着怨恨积累，疯狂滋长，成为复仇的欲望。你会成为压迫者，你在背后说坏话，他们让你做什么你会很马虎或者很勉强地去做。如同陀思妥耶夫斯基在《地下室手记》中描写的主人公一样，住在<strong>心理地下室</strong>里，想着这个世界多么邪恶，别人如何针对你、拒绝你，那么你会觉得活着本身就是有毒的，你会想要走出去做尽极恶之事。</p><p>在此也可以引出<strong>科学技术与人文艺术的关系</strong>：</p><ul><li><p>科学用来描述“世界是什么”，而对于【真实】的定义，即使是科学也只是一种工具，是一种人类用来探索世界的工具；</p></li><li><p>神话、戏剧、梦境、潜意识以及其他美学的、艺术的、幻想方面的文学作品是在告诉我们事情应该是怎样的，告诉我们如何行动，传授为人处事之道。经过多少个世纪，提炼出<strong>至善、至恶</strong>让我们理解，它们就<strong>像圣经故事中的两兄弟，也在我们体内时常斗争</strong>；</p></li><li><p>人文类学科像魔法，让人意识到，我不只是父母的孩子，还是自然的孩子，人类文化的孩子；由于人生艰难，你必须做一些真的有意义的事，你肩负着道德责任，做正确的事；<strong>真正意义上的好人，单纯遵守纪律是不够的，需要理解恶，并经受住恶</strong>；我们必须要理解自己身上的恶和阴暗面，这些可以通过读史来实现，比如《古拉格群岛》、《南京大屠杀》、《奥斯维辛》等，读的时候想象自己是集中营的守卫、是卡波而不是所谓解放人民的大英雄。<strong>只有认识到自己是个魔鬼之后，你才能对自己有足够的尊重，会对自己的行为有所控制</strong>。</p></li></ul><h3 id="三-狮子王中的隐喻"><a class="markdownIt-Anchor" href="#三-狮子王中的隐喻"></a> 三. 《狮子王》中的隐喻</h3><h4 id="1-性欲与侵犯欲"><a class="markdownIt-Anchor" href="#1-性欲与侵犯欲"></a> 1. 性欲与侵犯欲</h4><p>与饥饿和口渴不同，性欲与侵犯欲这两种驱动力通常被人类社会排除在外，需要个体有意识地自我整合。</p><p><strong>整合好自身的攻击性</strong>是一件非常重要的成长课题。</p><p>早期的辛巴像一只瞪大了眼睛的无辜的鹿，它软弱而幼稚，具有天真的脸庞。它接受一切信息，但没有反应和产出，牺牲了自己而成为环境的出气筒。</p><p>牺牲自己迎合别人，永远不制造冲突，这并不会让你更受欢迎或成为一个好人。因为没有能力伤害别人不代表你就是道德的，<strong>你需要长出獠牙，这样会保证你更少用到它们</strong>。背后的愤恨或怨恨其实变相地展示了你的不成熟。另外，如果无法拒绝加入群众病态的（某些情况下）行动中，恶意的企图会把你的邪恶（你一直逃避、摒弃、压抑的邪恶）勾引出来。</p><p>荣格认为，<strong>应该将恐怖的一面整合进自身而不是一直摒弃它</strong>，鄙视它，妄图彻底清除它，因为你做不到。而且即使做到了，你只能得到一个软弱的自己。圣人不是纯白的，是阴阳统一的。整合好你的阴影和侵犯性，你的面庞会变得更加坚定，如同成年辛巴。</p><p>通过<strong>查验是否还存在比较持久的（18个月以上）活跃的负面记忆</strong>，可以判断自己的感知价值结构中是否还存在漏洞。具体可以通过<strong>自我写作</strong>的形式来做。在你的自传中，仔细考虑过去发生在你身上的那些不好的事，弄清楚到底发生了什么，如何避免未来再次发生，梳理并表述自己的负面情绪。</p><h4 id="2-自性"><a class="markdownIt-Anchor" href="#2-自性"></a> 2. 自性</h4><p>《狮子王》中的狒狒象征了<strong>自性，也即潜能，未来可能成为的你</strong>，如果你以获取最大化信息的方式与世界交互，那个你能成为的你。辛巴受到召唤，在地下深水泉（潜意识）中看到自己未来的模样。</p><p>你会受到睿智的召唤，重新发现孩童时期与太阳相连的部分，并且相信它。当然这会需要一个长期的过程：</p><ul><li>第一阶段，开始意识到自己的无用，伴随着对自己的辩解。生气而毫无力量，爱发牢骚，满怀愤恨，拥有可悲而欠揍的面容。刚刚唤醒自我意识，伴随着非常痛苦的自我反思；</li><li>第二阶段，意识到自身价值结构的不足，希望找到“父亲”，也即权威。但“父亲”已经死了，我该如何转化自己，我该如何变好？但是没有人能够给出答案。</li><li>第三阶段，真诚希望改变。一旦你意识到自己错了，并且打开了潜在改变的大门，你的一部分自我会做出回应。你的深层自我还保存着潜在的可能性，它会改变你对事情的看法，改变你的回应方式。</li></ul><div align="center">  <img src="/2021/05/07/lobster/feifei.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>如果你真的想让情况好转，让自己振作，并且你承认现阶段的不足，你考虑当前的问题，想弄明白下一步究竟应该怎么办，<strong>那么你会找到内心中有某种东西在你发展的过程中，指引着你，那就是自性，是更高阶的自我，是转变中的不变</strong>。<strong>心理治疗是可以被高境界的道德努力取代的</strong>，一定要在内心深处，深层潜意识中，看到自己可能成为的人。</p><p><strong>如何解决心理问题？</strong></p><p>1）陷入混乱，如何改善当下？即<strong>自我认知模型的重建</strong>。你如今的生活不如意，那么你期待一年（或三五年）以后的生活（或者只是某些方面）是什么样的？确定下来，作为目标，筹划一下（诚实地），考虑会遇到哪些阻碍（实际的，心理的）。制定策略，试着朝向理想状态迈步。<strong>一定要对自己真诚，协调自我，记住，自己和自己是一伙的</strong>。</p><p>2）<strong>在多个维度照顾好自己</strong>，避免陷入混沌。每个人都需要<strong>日常惯例（routine）<strong>比如固定的作息，在物理上照顾好自己；你需要</strong>一份事业（career）</strong>，生产你认为有意义的东西，制定职业计划；需要家庭、朋友、<strong>亲密关系</strong>；<strong>学会如何谈判，学会表达需求</strong>；学会<strong>讲真话</strong>；学会<strong>倾听</strong>；<strong>运动是最好的阻止智力下滑的方法</strong>。</p><p>3）其实，很多去找心理治疗的来访并不是心理问题，而是生活真的出了问题亟待解决。心理治疗是一段真诚的关系。而本质上，<strong>自愿接触你不愿面对的东西是有疗愈性的</strong>，注意这必须是自愿的面对的。</p><p>4）<strong>存在式焦虑</strong>；正常人在安全的地方是冷静的，是把自己整合的很好的。但是现实中常见的地方并不安全，所以<strong>感到焦虑、抑郁是正常的，并不需要理由</strong>，我们需要关注的是你如何达到安全协调。我们可以通过待在自己的领地，维持稳定与安全感。但我们<strong>必须在意外界</strong>。你确实需要<strong>整合好自己的思想、精神，将每一种构成要素转换成功能性的存在</strong>，并且让这个<strong>功能性存在与外界图景具有一致性，即融入社会</strong>。内在、外在整合兼备才能真正调整好你的情绪。</p><h4 id="3-其它隐喻元素"><a class="markdownIt-Anchor" href="#3-其它隐喻元素"></a> 3. 其它隐喻元素</h4><ul><li><p><strong>阳光照耀</strong>；如果真正想在某处感到舒适，想主导并融入这个地方，你需要<strong>花心思让每个角落都被（你的）光照耀到</strong>，主动探索而非恐惧地缩在一角。这里的“某处”包括自身、亲密关系、陌生环境等；</p></li><li><p><strong>将特权与能力混为一谈</strong>；在大学里、在和平国度里应当心存感激，而不是认为这一切理所当然；</p></li><li><p>要<strong>允许自己是傻瓜</strong>；如果不允许自己时这种状态，而且一味完美主义，你会觉得自己是个“冒名顶替者”。尝试新事物的时候，你确实会像个傻瓜，但如果因此不去尝试，你就是个更糟糕的傻瓜。<strong>允许自己犯错才能进步</strong>。</p></li><li><p><strong>刀疤的帝国</strong>；二战时期的德国，极具条理性和极权主义，是过度的文明。人们信奉一套理论体系，甚至不惜欺骗自己，整个系统中充满了谎言（比如德国、苏联、1984）。</p></li><li><p><strong>辛巴逃离到了一篇沙漠</strong>；当你离开一个国度（不管那里是怎样的残暴），你都会<strong>陷入混乱中</strong>。如戒烟戒酒的时候，你抛弃了旧的价值体系（也许是因为它不够好），但<strong>并不是马上迎来提升</strong>。</p></li><li><p><strong>国王在国外培养</strong>；哈利波特、亚瑟王、狮子王等都会包含这类元素。你会在一定程度上疏离于你的文化，可能是因为原本的文化滞后腐朽，也可能由于你并没有发挥出你的潜力践行价值，在现有的评价体系中，你并不成功。比如在电影中辛巴被陷害了，但它并不是完全无辜的。</p></li><li><p>睿智的部分<strong>没有对邪恶的部分有足够的戒心</strong>；辛巴的父亲被弟弟打败的场景表明，明君有一个邪恶的兄弟，而没有足够留意，它选择逃避或视而不见。人生的洪水也是如此，这些灾难有上天的随机因素，也有一些部分是你会责备自己过去的，<strong>因为当时的你短视或选择视而不见</strong>。</p></li></ul><h2 id="皮亚杰与构成主义"><a class="markdownIt-Anchor" href="#皮亚杰与构成主义"></a> 皮亚杰与构成主义</h2><p>皮亚杰，知识巨匠，一位杰出的发展心理学家，知识渊博。</p><p>经典经典科学观会假设知识本身是事实而不是过程，但当代科学更多认识到<strong>知识是过程而不是静态事实</strong>。</p><h4 id="1-同化与顺应"><a class="markdownIt-Anchor" href="#1-同化与顺应"></a> 1. 同化与顺应</h4><p>皮亚杰理论中重点有<strong>同化与顺应</strong>的概念。我们需要将知识视为工具，而不是客观独立的现实，<strong>世界不是所有等待发现的客观事实的合集</strong>，世界是更为复杂的。不随时间改变的才可以称为事实，比如人们产出事实的方式、知识（世界观）的获取与转换过程等，而不是事实本身。<strong>终极现实，是经历这些阶段的过程</strong>。</p><p>皮亚杰理论与萨满启蒙有共通之处，也即起初存在有序状态，之后意外发生，陷入混沌，原有体系瓦解，最终重组新生，形成一个更加完整的状态。根据这个理论可以解释<strong>孩子在商场中与家长分开会恐慌</strong>的现象。因为，原本家长将外界的复杂性与孩子隔绝，而家长离开，混沌和不确定性向孩子涌来，在新世界中学习速度过快，会带来巨大痛苦。而如果学习速度适中的话，你会从可能性中获益，而不是被不确定因素淹没。</p><div align="center">  <img src="/2021/05/07/lobster/pyj.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><h4 id="2-程序记忆与表现记忆"><a class="markdownIt-Anchor" href="#2-程序记忆与表现记忆"></a> 2. 程序记忆与表现记忆</h4><p>皮亚杰理论整体在研究人们如何表现世界并学习。他在<strong>试图填补科学与价值之间的裂缝</strong>。</p><blockquote><p>大卫休谟提出，你无法从“是什么”中推理出“应该是什么”，你知道这件事不代表你可以毫无差错地处理这件事；</p><p>哈里斯相信价值可以嵌于科学中；</p></blockquote><p>提出<strong>程序记忆与表现记忆</strong>，人类在婴儿时期还没有表现记忆，而社会结构隐形地嵌于程序记忆的系统结构中，你所出生于的这个社会结构会被编入你的行为，而你并不知道规则。</p><h4 id="3人生游戏"><a class="markdownIt-Anchor" href="#3人生游戏"></a> 3.人生游戏</h4><p>人类间的社会互动来源于一个有限制的空间，我们永远都在玩游戏。而通过研究哺乳动物的脑回路研究，我们发现公平游戏的感觉是生物天生的。</p><p>小孩子的游戏嵌在大人游戏中（所以，在2-4岁时，要让孩子学会如何和其他人，尤其是孩子，玩耍）；之后，人们会随着发展，更加有意识地玩游戏，并开始在行为上表现游戏，开始学习显示游戏规则；最终，在道德发展的最高阶段，人们意识到自己不仅仅是游戏的玩家，还是规则的制定者，也可以发明游戏。</p><p>如何毁掉一个孩子：</p><ul><li>在他做好事或尝试做好事时，惩罚他或忽略他；忽略比惩罚更可怕，因为惩罚他的时候至少你的注意力还在他身上</li><li>玩游戏时不要越过让孩子筋疲力尽的那个点[ 其实“培养”自己也是同理]</li></ul><p>在人生游戏中，克服痛苦的方式之一是创造意义：</p><ul><li>在技术层面，从挫折中学习到什么，真的会改变的微观生物构造，可以类比于冲浪；</li><li>我们在死亡（稳定或混沌都是死亡状态，你生活在阴阳交界）和生存之间保持着精妙的平衡。学习的过程也是这样，你需要杀死部分已知（生物结构），才能学到未知，虽然过程会带来痛苦；</li><li>在这个过程中，我们学习到游戏的“元玩法”</li><li>意义的重要性之一就在于，它会改变你看待世界、回应世界的方式</li></ul><h4 id="4-高级抽象认知"><a class="markdownIt-Anchor" href="#4-高级抽象认知"></a> 4. 高级抽象认知</h4><p>如图所示，要从小事做起、从多方面做起，从底层的行为感知序列开始，从实际的微观行为到高层抽象。</p><div align="center">  <img src="/2021/05/07/lobster/pyj_high.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><h2 id="弗洛伊德与潜意识"><a class="markdownIt-Anchor" href="#弗洛伊德与潜意识"></a> 弗洛伊德与潜意识</h2><p>潜意识的概念表明了，人们可以在无法解释的时候做出行动，比如孩子无法描述游戏规则，但他可以玩这个游戏。</p><p>人类深层的潜意识和其他哺乳动物、爬行动物等也有很多共通之处。</p><p>我们并不是完全掌握这些“后台运行的程序”，人格越没有整合好，就越容易失去对这类意识的掌控。</p><p>弗洛伊德提出我们的防御机制包括：压抑、否认、反向、转移、认同、合理化、理智化、升华、投射。</p><p><strong>JP教授对弗洛伊德的理论有几处不认同的地方</strong>：</p><p>首先是<strong>学习成长模型</strong>，教授更倾向认为在健康状态下是皮亚杰模型。病态发展中，你并没有把攻击冲动和性冲动整合到你的人格中去，而是让超我直接压抑了它们，没能称为活跃的一部分。你不去展示它们，所以成为了所谓的“好人”。你对自己非常专制，成为一个超严厉超我的受害者。你把父母、祖父母及内化的“父母”集合成了一个超级严厉的法官，一直在注视你，评判你。</p><p>其次是有关<strong>记忆与遗忘</strong>，弗洛伊德理论认为人类有连续录影带式的记忆，某些由于太过痛苦或其他原因而被压抑，导致遗忘。而教授则认为，人的记忆并不是这样有条理的，过去的信息是杂乱的，待清除的，所以人们没有注意到，而不是有意识（自我意识或潜意识）地去压抑。</p><p>最终有关<strong>梦境解析</strong>，弗洛伊德认为梦展现了被压抑的东西，梦尽力地像掩盖所要表达的内容。而教授更倾向于荣格的观点，即认为梦在尽可能清晰地表达，它不属于语义记忆系统，而更像是探索未知的触角，所以使用象征符号，而并不是想要向做梦者隐藏不愉悦的内容，而是用它唯一可以使用的语言体系。</p><h2 id="卡尔罗杰斯与现象学"><a class="markdownIt-Anchor" href="#卡尔罗杰斯与现象学"></a> 卡尔罗杰斯与现象学</h2><p>科学将主观剥离世界，它将任何主观视为偏见或错误，希望尽可能摆脱。但问题在于每个人都是一种主观。</p><p>在科学发展的当今，人成为了冷漠的客观事实中一种孤立的存在，容易导致存在的虚无。</p><p>尼采提出，上帝已死，带入虚无主义。海德格尔则希望从头构建体系，重新思考现实，解决这一问题。将现实看作我们经历/体验到的一切，抛弃主观和客观的划分，但这类研究并没有实质性进展。</p><p>荣格后半生转向现象学，提出如下三个必要层面，也可视为扩展了皮亚杰的道德模型：</p><ul><li><strong>将思想与情绪结合 [男女结合]</strong>；可以通过<strong>未来自我写作训练，让焦虑成为你的助手</strong>；想象如果你不去处理某个你目前尽量逃避的问题，会发生什么；也可以<strong>进行坚定性训练</strong>，想象你具体想要什么，不要因为当下得不到就不去想象，想象你不去这个做的代价，整合攻击性（如果你没有力量，你就无法谈判；而愤怒是一种有毒的情绪，会加剧身体负担，长期下来会影响健康）。</li><li>将思想与情绪的结合体，<strong>再与身体结合 [行动起来]</strong>；一定要知行合一，保证认知与行动的连贯性。从皮亚杰高层抽象模型的最底层做起，从多个方面的小事做起。没有肩负起存在的重任会导致神经质的愧疚和恐惧，所以从清扫你的房间做起。</li><li>最终消除<strong>自己与世界的区分[天人合一]</strong>；世界会随着你的目标而改变。</li></ul><p>人与人之间的沟通要做到<strong>真正的倾听</strong>，我在这里是为了一起达到更好的你的一部分；我希望了解你的观点，而不是输出我的然后只希望你赞同。人们很难找到真正倾听自己的人，真正的倾听是我们给别人的非常宝贵的礼物。</p><h2 id="存在主义"><a class="markdownIt-Anchor" href="#存在主义"></a> 存在主义</h2><p>存在主义相关人物包括尼采、陀思妥耶夫斯基、克尔凯郭尔等。</p><p>存在主义产生的背景为“上帝已死”。现代生活中不可避免地承担着很多焦虑，这可能是人们对科技的提升与意识觉醒付出的代价。人类暴露于一种无意义且痛苦的存在，如果你的价值系统瓦解了，那么你就会没有目标，没有积极情绪（虚无），这种情况下，人们也许会飞奔至极权主义的怀抱，牺牲理智与智力换取秩序与确定性。</p><blockquote><p>虚无主义(物质世界观中隐含)，陷入其中，你会一无所有，无法应对生活中的挑战或苦难</p><p>理性极权主义(激进意识形态)，过度信奉，你变成了一堆理论抽象中的提线木偶，走向毁灭</p></blockquote><p>存在主义<strong>悲观又极度乐观</strong>，它承认人是脆弱的、有限的，但一旦你<strong>直面恐惧</strong>，又会激发出我们无法估量的力量。这力量就是自性或称为内在潜能。不要低估自己的内在潜能，对于你自己，你还有很多不知道的事，<strong>去新的环境</strong>，会改变你的微观生物结构。如果你<strong>恰当地把自己推进世界</strong>，你会开启新的能力，并在探索过程中获得信息。<strong>自愿地以更多方式挑战自己</strong>，可以促进这种转变。不要针对死亡焦虑构建虚假的抵抗，而是积极学会如何应对这个世界。</p><p>存在主义包括三个基本理念：</p><ul><li><strong>行动比语言更有力</strong>。知行合一，身心协调，行胜于言。如果想了解别人或自己的信念，最好去看做了什么，而非说了什么。</li><li><strong>麻烦和痛苦是人类经历的固有元素</strong>。人难以摆脱苦难，即便这个人本身没什么问题。很多人生中的苦难没有原因，没有由来，并不只是因为童年/经历的锅，并不是你这个人哪里出了问题，也不要认为这些问题仅限于你自己或这一小部分人。<strong>人的生活本身就有问题，你要做的是面对和解决</strong>。合理程度的痛苦是正常的，类似圣经故事中所说，人们从美好的天堂掉落，总是处于残缺的状态，认为自己哪里出了问题，需要被纠正。</li><li><strong>存在主义包含一些浪漫主义</strong>。反对理性与智力的至高地位，理性并不是指引人们的根本原则。<strong>理性是需要与其它主观因素相辅相成的</strong>，而非如20世纪以来所呈现出的一家独大。科学是一种需要被合理利用的工具，而不是描述存在的方式。<strong>关注个体</strong>，个体才是心理分析的恰当层面，将每个患者视为独特的个体，有独特的问题，而不仅是使用精神分析那种成体系的框架。</li></ul><p>由此JP引到恐旷症的逐步治疗：</p><ul><li>四处嗅探的小白鼠；</li><li>早期阶段的回归与固化，可以看到患者退行回小孩子阶段，认为自己没有能力；</li><li>对于权威的态度，自己总处于次级、奴隶的状态。家庭成员对心理治疗的抵制（包括沉默的抵制），你真的希望伴侣变得更好吗，那意味着她会更坚定，而难以掌控；</li><li>一些心理分析师不赞成类似行为主义的治疗手段，认为对于电梯的恐惧并不针对电梯，而是象征着其它东西，如果治好了电梯，这恐惧还会从其它地方冒出来；JP认为不然，治好电梯，患者会自行挑战出租车，这是在教患者学会勇敢，而<strong>勇敢会扩大化</strong>；</li><li><strong>表现得弱小无用会成为一种武器</strong>；</li><li>[ 题外话 ] 联想到刚开始做科研时，自己对自己的批判，导致了自己的罢工，所以很喜欢DDL，因为那时候你必须专注，没心思再批判什么。发论文也是如此，直面恐惧，真的做到之后，就觉得没有那么困难。</li></ul><p>另外，人并不是理性的，这也是存在主义对乌托邦主义的批判。陀思妥耶夫斯基的作品也表明，<strong>人具有自由意志</strong>，人所做的事情，是为了时时刻刻证明他是人，不是钢琴键，不是可以计算、推理的事物，就算这可能会损害他自己的皮肤，就算要以自相残杀为代价。尼采也表示，也许感到不满足其实也是一种满足，也许你必须受限，这是你想要的，也许这些才会给你生活的意义。</p><h2 id="现象学"><a class="markdownIt-Anchor" href="#现象学"></a> 现象学</h2><p>相关人物，马丁海德格尔（哲学家）。</p><p>现象学认为人们生活在一个自我定义的感知框架之中。</p><p>JP认为<strong>临床心理学是有价值导向的，并不是纯粹的科学学科</strong>。</p><p>客体是非常复杂的，比如波粒二象性。即便以科学方法定义某个客体，也并非真正在定义，你只能说，这是个多维度物体，如果我以这样的方式接触它，也就是采取这样的过程或方法，它就会显现出那样的特质，但还有很多种其它的可能。</p><p>我们需要限制自己的感知范围，达到一个可以处理的范围，<strong>在这个收窄的现实中</strong>生活。这就意味着，你<strong>需要有一个目标（视野的焦点）</strong>，这代表了你的价值体系，以及世界将如何在你面前展开。目标启示着你的世界，组织着你的情感，并让你准备好做行动。目标包含了很多，比如内驱力、目的、动机等等，它是你人格的一部分。但同时也要了解到，我们正<strong>处于收窄的视野范围内</strong>。</p><p>人们为什么对某些事物好奇？要去追寻某些意义？</p><p><strong>宾斯旺格</strong>认为我们最先感知到的，不是味道、声调或触感印象，也不是物体或客体，而是意义；<strong>美丽是主观的</strong>，由于你的感知‘滤镜’产生的。<strong>鲍斯</strong>则认为，<strong>美丽固有于客体本身</strong>，显现了自身，向外发光，你追求那些向外发光的东西（比如《哈利波特》中的金色飞贼隐喻）。JP认为是这两种观点的结合，你无法完全掌控你的好奇心，但它也并不是完全随机的，因为没有主体能脱离结构去感知。同时，被感知的那个客体也带着它自己的潜能向外发光。</p><p>我们面前不是一个固定的客观世界，而是<strong>一个充满潜能的世界</strong>。你能和这种潜能的任何方面进行互动，在互动中，你将一些以前不存在的东西拉进了现实。这些潜能并不是无限的，因为你本身就是有限的，但不管你有什么目标打算，它都足够了，因为<strong>它永远比你所需的潜能更多</strong>。</p><p>你探索着某个新的东西，你从这次探索中生成了什么呢？首先，会产生一个新的你，物质的也是精神的，因为探索时你在学习，这个过程会改变你。而同样从你的探索中也生成了一个世界。</p><p>开放的想象力：</p><ul><li>梦境处于思考的前沿，走在你的前面</li><li><strong>艺术超越了语言能表达的东西，否则它就不是艺术，而是宣传</strong></li><li>发源于未知世界，并提供给你一些信息</li></ul><h2 id="索尔仁尼琴"><a class="markdownIt-Anchor" href="#索尔仁尼琴"></a> 索尔仁尼琴</h2><p>他是一位二战上了前线的苏联士兵，是存在主义作家，相关人物还包括撰写《活出生命的意义》的维克克多弗兰克、哈维尔等。</p><p>其著作《伊凡德尼索维奇的一天》是苏联时期第一本公开描述集中营的书，他还凭借《古拉格群岛》获得诺贝尔奖。</p><h4 id="1古拉格群岛"><a class="markdownIt-Anchor" href="#1古拉格群岛"></a> 1.《古拉格群岛》</h4><p>《古拉格群岛》于1973年出版，之后Samizdat地下传阅，1989年再次公开出版。古拉格，即纠正性劳动营主管部门，因为当时认为人们会犯罪是由于沙皇俄国体制的压迫，所以让一些罪犯（强奸犯、抢劫犯、小偷等）管理集中营，而集中营中关押着的是与“特权”有关的人士，带着基于阶级和种族的罪名。<strong>这本书书例证劳动人民的乌托邦可以实现的想法的幻灭</strong>。《昨日的世界》中也描绘了苏联邀请西方知识分子去参观的情景。1930年开始，《1984》、《动物农场》也都开始揭露一些现象。</p><p>这本书包括了压迫性苏联体制的产生，斯大林统治下全面展开和共产系统。当时人们把苏联解体归咎于斯大林的个人崇拜扭曲了最初准确的主义，认为如果列宁活得就一些，乌托邦就可以实现了。索尔仁尼琴从根本上反驳的了个观点。他梳理了主义与列宁定制的某些法律之间的问题，比如清除异己、个人崇拜、专制权力、无处不在的监控，KGB等。当时的情况下，即使是坚定的党员也无法幸免。毫无缘由地，即便他们没有对D犯下任何错误。<strong>人类的心无法承受被心爱的斧头所伤，却还要证明那把斧头是智慧的。</strong></p><p><strong>帕累托分布</strong>掌管了金钱分布、公司关系等情况，支配了几乎所有创造性生产的领域。这是<strong>一个根本性原则</strong>，而目前没人知道该怎么有效且持续地把资源从几乎掌握一切的人手里撒到下层几乎什么都没有的人那里（虽然顶端的人会变化）。<strong>即使是通过抛硬币决胜负</strong>，财富也会最终集中到少数人手里，时间足够长后，甚至会集中到一个人手里。</p><p>社会的病态和个体的病态间根本联系在于，<strong>个体倾向于欺骗自己</strong>，从而无法以真实真诚的方式行事。最终个体变为虚无主义者，或由于品格被逐渐削弱，不真诚称为了生活的一部分，转向意识形态和极权主义的解决方案，放弃恰当生活，放弃个体责任。而无法<strong>真诚真实地行事会导致走向虚无主义或极权主义</strong>。</p><p>如何分辨一个被意识形态控制住的人？其实一旦你掌握了他们意识形态底层结构的五六个公理，你<strong>甚至可以预测</strong>他会说什么（比如休蒙格斯采访视频的例子）。人们选择被意识形态控制，因为这减少了他们思考的负担，也让他们相信自己完全掌握了世界上所有的知识，而且相信自己不需要思考就可以分辨出谁在善的一边。</p><h4 id="2圣经故事"><a class="markdownIt-Anchor" href="#2圣经故事"></a> 2.圣经故事</h4><p>《巴别塔》，极权主义大厦或乌托邦，越建越好，要容纳更多的因素，更多不同的人，最终会成为一盘散沙。洪水一般隐喻是来自神的惩罚，包括事物瓦解的趋势、人类对罪的趋向、熵增原理等。</p><p>《失乐园》，理性思维产生的政治的、意识形态的理性建构与引导着人类组织的超然神话间有一种紧张的关系。<strong>上帝的最高天使撒旦就是这种经理性思维的拟人化象征</strong>。这类思维倾向于产生极权系统，并爱上极权系统，系统之外的东西都不允许存在，而最终将自己投入地狱。</p><p>艺术家、诗人、哲学家先后了解到未知。</p><h4 id="3箴言"><a class="markdownIt-Anchor" href="#3箴言"></a> 3.箴言</h4><p><strong>苦难是存在的一部分</strong>，这是存在主义的基础观点之一，大部分伟大宗教体系也拥抱这一观点。</p><p><strong>苦难的三大来源</strong>如下：</p><ul><li>自然界的人性，人生而有限</li><li>社会结构的武断审判（无论你处于哪个社会中都会有不同程度的专断）但你需要和他人生活在一起，你要找到你适合的地方</li><li>我们自身也对某些不必要的痛苦负有责任。你本可以做某些事，让你的生活以及别人的生活得到改善；你有未承担起来的责任，而你的良知明明知道。<strong>人心真的有良知，听一听来自良知的劝告（conscience）</strong>。</li></ul><p>正确度过人生的方式是：<strong>真诚地存在，拒绝参与说谎和欺骗，让你的语言/行动尽可能真实，为你的生活（也许还有其它人的生活）负起责任</strong>。这样做是有意义的、负责任的、高尚的。这样做有助于减轻痛苦，否则痛苦会带来虚无主义，或让人逃入极权主义的怀抱。</p><p>你需要一些东西来抵抗你自己的脆弱性，你可以采纳<strong>别人给你制定好的</strong>对现实的综合描述，有种描述把世界简洁地分为天真的无辜受害者和犯了罪的苦难制造者，而且他们都和你无关，这不是评估世界的合理方法，<strong>苦难是与生俱来的。即使你精神/心理没有出现问题</strong>，事情也向糟糕的方向发展了，但我们仍然有前进的方向，选择活得高尚一些，让你可以忍受你自己，甚至可能尊敬你自己，因为你能直面那可怕的脆弱和痛苦。</p><p>[ 避免欺骗、承担责任、试图改善 ]</p><p>什么是真实？ 你看到什么，听到什么，做什么，和谁在一起。有一种从心灵深处满溢出来的不懊悔，也不羞耻的和平与喜悦。爱你所爱，行你所行，听从你心，无问西东。</p><p><strong>成为你自己</strong></p><p>你很清楚你没有完全实现自己的潜能，你造成了部分的苦难，也许你可以换一种看待世界的方式，换一种行动的方式，不要再浪费眼前的机会！<strong>人心真的有良知</strong>，我们并不知道那是什么，但你要听从良心的劝告。以你真实自我建立起来的亲密关系会更强健、更愉快，一个完整的你，通过和伴侣协商，活出真实的生活，这是也是养育孩子的基础。</p><p>实际上人们不仅不做他们应该做的、让情况变好的事，他们还积极地把事情搞糟，因为他们**（我目前是他们的一员）**怀恨在心、怨气冲天、狂傲自满、尔虞我诈甚至杀人如麻，所有这些病态都纠缠在一起。</p><p>你振作起来，多大程度上活出真实的自我（不要误解为放浪形骸，而是存在主义中恰当生活的三个步骤）不仅关乎你一个人的命运，而是关乎所有与你产生联系的人的命运：</p><ul><li>不要低估自己的影响力，它如同涟漪</li><li>你的所作所为非常重要，大多数事情都是有意义的，这同时意味着你需要承当由此而来的责任</li><li>如果你过着病态的生活，你病态化了整个社会，如果有足够多的人这样做，社会会变成什么样子？</li><li>虚无主义者很痛苦，<strong>但他们的优势在于不用承担责任</strong>，是你自己你放手让你的价值体系崩溃了，于是你无需承担责任，代价是时常痛苦，但你可以一直哭哭啼啼，人们会为你感到难过</li></ul><p><strong>人们需要在生命中的某阶段时间全身心参与某项游戏</strong>，从某时刻起，你得在某一方面有所成就，即使你牺牲掉了其他所有的可能，但你必须做出选择（某种职业、某种价值观），否则会徒增年岁而依旧混乱。</p><h2 id="大脑边缘系统"><a class="markdownIt-Anchor" href="#大脑边缘系统"></a> 大脑边缘系统</h2><h3 id="一-人格神经科学相关人物"><a class="markdownIt-Anchor" href="#一-人格神经科学相关人物"></a> 一. 人格神经科学相关人物</h3><p><strong>汉斯艾森克</strong></p><p><strong>格雷</strong>，对动物行为学、神经解剖学和神经精神药理学都非常熟悉，极大拓展了我们对至少两个人格特质（外向性和情绪不稳定性）的生物学和演化学基础，著有《焦虑神经心理学》。与之相关的还有<strong>控制论</strong>，MIT的诺布特维纳，是早期人工智能科学家。智能个体是目标导向型的，在达成目标的过程中，以减少与目标的偏差来组织自己的行为。</p><p><strong>勒杜</strong>是情感神经科学家，给格雷（多集中于海马体）的研究做了很多补充。</p><p><strong>斯万森</strong>研究发育解剖学，对大脑在胚胎发育以及之后的阶段如何逐渐成熟感兴趣。大脑是一个动态、发育、不断变化的系统，心理学家一般只关注成熟的大脑，与皮亚杰理论有偶然性的相符。</p><h3 id="二-人类的目标范围有限"><a class="markdownIt-Anchor" href="#二-人类的目标范围有限"></a> 二. 人类的目标范围有限</h3><p>现实异常复杂，大脑也是非常复杂的。即使在做及其简单的事情，我们都需要屏蔽掉几乎所有的其它信息。作为人类感知能力非常受限，基本对任何事物都是“瞎的”。比如以猩猩实验为例，表明如果专注于目标的话，我们的注意力并不容易被打扰。</p><p>在实际生活中，我们需要首先构建框架，之后才能和世界互动。我们有很多恐惧管理的要素，比如爱国主义、文化认同、宗教信仰，死亡焦虑、等级制度，社会体制等。社会制度帮我们阻挡了复杂性，我们的身体也帮忙屏蔽了一些事情。</p><p>弗洛伊德提出人类有几项基本动机，比如口渴、饥饿、痛苦（心理上的抑郁与孤独、生理痛苦等）、愤怒（攻击性、掠夺攻击性、防御攻击性 &lt;与情绪不稳定性有关&gt;）、体温调节、恐慌、归属感和关怀、性冲动、探索与玩耍（大部分哺乳动物需要通过玩耍来社会化、父亲与孩子的打闹游戏、人们天生有很强的探索性，受下丘脑管控）。</p><p>这些动机的作用在于，有了动机才方便设定目标，情感会根据目标来引导你，让你准备行动并建立与世界互动的感知框架。</p><p>多个动机组成整个人格，可以将动机视为“微人格”，这和精神分析学派相符。<strong>大脑皮质是无法战胜下丘脑的</strong>，俗称“恼羞成怒”。下丘脑是一个古老的区域，是赖以生存的根本，是处理紧急状况的主宰，只有当你不受任何（在各种维度上）干扰的时候，大脑皮质才是掌管者。</p><h3 id="三-你不只在大脑里"><a class="markdownIt-Anchor" href="#三-你不只在大脑里"></a> 三. &quot;你&quot;不只在大脑里</h3><p>你的神经系统遍布全身，相关实验比如盲人实验表明你不仅仅是用眼睛看的，你的大多数感觉会映射到神经系统的不同层面，思考的过程太慢了，有一些过程并不经过“思考”这个中间解释过程。</p><p>“大脑”是一个连续体，所以要在身体各部分层面都注意照顾好自己。</p><p>皮亚杰认为从婴儿开始，人的意识是从下到上发展的，首先是脊柱相关的意识，最后才到大脑皮层的高级意识，设计机器人的科学家也是这样做的。</p><h3 id="四-大脑"><a class="markdownIt-Anchor" href="#四-大脑"></a> 四. 大脑</h3><p>两个半脑中，左脑更多的负责已知、秩序和集中注意力，右脑则负责感知未知。大概在做梦的时候右脑将新知识传递给左脑，所以<strong>一定要保证睡眠！！</strong></p><p><strong>下丘脑</strong>：整个感官-运动-感知-行动的等级系统的构建方式，需要让你的基础动机状态维持满足，所以<strong>你必须管理自己的行为</strong>。在基础动机被满足的限制下，恰当地排列组合。同时你要注意到社会环境！！！下丘脑等级组织需要满足许多参数条件，考虑到他人，同时考虑到未来。意识可能存在于丘脑和大脑皮层之间（正反馈过程）的<strong>交互过程中</strong>，而不是仅仅产生于某个区域。</p><p><strong>海马体和杏仁体</strong>：海马体会监视那些根据你的理解，你希望发生的事（动机状态），同时它会观察世界（你自己诠释后的世界），评估这两者是否一致，如果匹配（即你知道你在干什么、你在哪儿）则会抑制杏仁体和下丘脑，让网状激活系统保持冷静状态。如果出现偏差，会激活网状回路，启动情绪，特别是消极情绪，同时也会准备积极情绪系统以及下丘脑的一些神经回路。</p><p>我们并没有熟悉大脑结构，大脑是未知区域。<strong>我们对大脑部位的命名并不一一对应，这只是为了解剖学上的方便</strong>。人类是一种极其有趣的动物，极其发达的大脑皮层，复杂的运动输出系统。你不能直接将动物视为一种简单的“刺激-响应”机械系统，这是简单的行为主义，但大脑还有其它层次，比如行为主义、认知心理、心理动力等学科研究的都是大脑不同的层面（盲人摸象？）。随着进化与发展，人类既是简单机器（古老的反射），也是有些复杂的机器，同时也是及其复杂的机器，<strong>以及另外一些你根本不知道的东西</strong>。（谦卑虔诚一些，不要自大）</p><h3 id="五-基础情绪"><a class="markdownIt-Anchor" href="#五-基础情绪"></a> 五. 基础情绪</h3><h4 id="1-积极情绪"><a class="markdownIt-Anchor" href="#1-积极情绪"></a> 1. 积极情绪</h4><p><strong>不是所有的激励奖赏机制都是后天习得的</strong>，这也是行为主义的盲点，它忽略了古老的无条件奖赏回路。比如躲避蛇、血、牙齿、低频大声嚎叫等不需要后天习得，而是自动反应。</p><p>第一种积极情绪，比如吃了三明治后你不再饿了的饱足感（西西弗推石），无条件反应，无需后天学习，完成性奖赏。这与血清素含量相关，满足感，没有过多的积极或消极情绪，没有过多的担心和兴奋，就是满足。</p><p>第二种积极情绪，包括好奇、期盼、希望等，激励奖赏或条件性奖赏，那种朝着所向往的奖赏前进的动力。它们根植于下丘脑的多巴胺能系统，“那儿有好东西，我要去拿到它！”。这可以后天养成与激活，但也有部分是天生的。一半下丘脑制定具体目标，另一半进行探索。多巴胺系统激活，会产生探索、外向、开心、玩耍、热情，会自信，新奇事物积极的一面。</p><p>通常人们对即将到来的完成性奖赏所产生的情绪比真正获得奖赏时更强烈。我们的探索欲非常强，新奇事物本身跟现象学主要关心的内容相关，<strong>即意义的揭示</strong>。积极情绪推动你向理想目标迈进，消极情绪保护你，如果发生负面情况，你会停下来，重新理清你的行为或行为框架（这里有漏洞），越过障碍（有的时候你直接抛弃了问题）。<strong>一个情绪稳定、非神经质的人</strong>会很快将烦恼抛之脑后，重新开启探索回路，尤其是外向开放性还很高的时候。而<strong>也有人会产生奇怪的联想，并且认为自己是个“坏人”，掉入漏洞后，焦虑害怕，难以走出来</strong>。</p><h4 id="2-消极情绪"><a class="markdownIt-Anchor" href="#2-消极情绪"></a> 2. 消极情绪</h4><p>与防御和回避相关，痛苦、悲伤、沮丧、失望还有恐惧，恐惧、厌恶等都有自己独立的回路。血清素可以抑制消极情绪，血清素能系统是古老的系统，就像大脑中的管弦乐队指挥。GABA缓解焦虑，内生的，安定和巴比妥药物、酒精等激活gaba。对于某些人来说，酒精是非常容易成瘾的，它影响gaba，多巴胺系统，奖惩机制。</p><h4 id="3综合"><a class="markdownIt-Anchor" href="#3综合"></a> 3.综合</h4><p>两个情绪系统是独立的系统，哪一个力量更强大一些会造成情绪稳定性和外向性的不同表现，而<strong>这种倾向性大多是天生的</strong>。</p><p>女性相对男性更加情绪不稳定（半个标准差），而且越是男女平等的地区越明显，社会越平等，两性之间的差异不是减小，而是增大，所以并不是所有的特征都是社会化的结果。可能的原因在于，女性体型较小，上身较弱，在争夺优势地位的冲突中，体力不如男生；性方面更容易受伤；需要照顾婴儿，可能是为了适应女性+婴儿的系统，让女性的消极情绪系统更强，保护婴儿。</p><blockquote><p><strong>意料之外的事情发生后，你需要多大程度的担心？</strong></p><p>可能是你没有正确认知/诠释世界；</p><p>可能由于你不切实际的计划；</p><p>这里受情绪的影响很大，不同的人有不同的情绪稳定程度，没有绝对的优势，都是视情况而定</p></blockquote><p><strong>人们不喜欢承认自己对世界的高级抽象出了问题</strong>，因为整个复杂性会涌上来。所以在恋爱中不要这么做。抽象化和范畴化是一种高级能力。</p><p><strong>为什么人会受精神创伤？</strong></p><p>首先二元抽象也带来危险，过度泛化，在某件事情上容易得出，“我不是一个好人，所以我就是一个坏人”，但这其实是在价值结构中逐步抽象，环环相扣的过程。</p><p>另外，抑郁症患者容易从一个很小的，难以参数化的所谓“错误”中，迅速归纳出最高级别的抽象意义（虽然在另一个角度抽象是个很棒的能力），比如我犯了错误A，所以我无用/坏人，这种思维很不好，忽略了差异性。</p><h2 id="大五人格"><a class="markdownIt-Anchor" href="#大五人格"></a> 大五人格</h2><h3 id="一-概述"><a class="markdownIt-Anchor" href="#一-概述"></a> 一. 概述</h3><h2 id="相关资源"><a class="markdownIt-Anchor" href="#相关资源"></a> 相关资源</h2><p>《人生十二法则》、《人格及其转变》课程 [B站，<a href="https://www.bilibili.com/video/BV1AW411M7vL" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1AW411M7vL</a>]</p><p>《哈佛幸福课》</p><p>王德峰教授讲座系列 [B站可搜索到，但被屏蔽了好多，喜马拉雅有音频资源]，包括王教授讲的《坛经》、《传习录》、《资本论》等</p><p>欧丽娟老师讲《红楼梦》[喜马拉雅有完整音频，B站有一些精彩节选]</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人文社科</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>哈佛《正义课》笔记</title>
    <link href="/2021/05/07/fair-havard/"/>
    <url>/2021/05/07/fair-havard/</url>
    
    <content type="html"><![CDATA[<h2 id="背景材料"><a class="markdownIt-Anchor" href="#背景材料"></a> 背景材料</h2><p>迈克尔.桑德尔教授是哈佛最受欢迎的教授之一，他说过“我的目标不是试图用什么理念去说服学生，而是把他们训练成有头脑的公民”。他在29岁时就成为社群主义向自由主义发起挑战的标志性人物。刘擎教授在《刘擎西方现代思想讲义》中有专门章节介绍。</p><p>课程视频：<a href="https://www.bilibili.com/video/BV1ct4y167fM%EF%BC%88B%E7%AB%99%EF%BC%89%E5%8F%A6%E5%A4%96%E5%8C%85%E6%8B%AC%E8%AE%B2%E5%BA%A7%E3%80%8A%E9%87%91%E9%92%B1%E4%B8%8D%E5%8F%AF%E4%B9%B0%E7%9A%84%E4%B8%9C%E8%A5%BF%E3%80%8B" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1ct4y167fM（B站）另外包括讲座《金钱不可买的东西》</a></p><p>课程讲稿发表为书籍《公正》，相关书籍还推荐《洞穴奇案》。</p><p>[ 但个人阅读体会觉得富勒写的第一部分比较精彩，萨伯补充的后九个观点略显逊色些 ]</p><p><em>对于哲学问题没有一劳永逸的解决方式，也许我们会产生怀疑论的回避，但永远无法平息内心渴望理性思考的不安</em></p><h2 id="功利主义与道德主义"><a class="markdownIt-Anchor" href="#功利主义与道德主义"></a> 功利主义与道德主义</h2>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人文社科</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cs224w《图机器学习》2021（二）图神经网络</title>
    <link href="/2021/05/06/cs224w2/"/>
    <url>/2021/05/06/cs224w2/</url>
    
    <content type="html"><![CDATA[<h2 id="相关论文"><a class="markdownIt-Anchor" href="#相关论文"></a> 相关论文</h2><p>GCN《<a href="https://arxiv.org/abs/1609.02907" target="_blank" rel="noopener">Semi-supervised classification with graph convolutional networks</a>》</p><p>GraphSAGE 《<a href="https://arxiv.org/abs/1706.02216" target="_blank" rel="noopener">Inductive representation learning on large graphs</a>》</p><p>GAT《<a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener"><strong>Graph attention networks</strong></a>》</p><p>GNN with skip connection_1 《<a href="https://arxiv.org/abs/1605.06431" target="_blank" rel="noopener"><strong>Residual networks behave like ensembles</strong> of <strong>relatively shallow networks</strong></a>》</p><p>GNN with skip connection_2 《<a href="http://proceedings.mlr.press/v80/xu18c.html" target="_blank" rel="noopener">Representation learning on graphs with jumping knowledge networks</a>》</p><p>DiffPool 《<a href="https://arxiv.org/abs/1806.08804" target="_blank" rel="noopener">Hierarchical graph representation learning with differentiable pooling</a>》</p><p>GIN《<a href="https://arxiv.org/abs/1810.00826" target="_blank" rel="noopener">How powerful are graph neural networks?</a>》</p><h2 id="信息传播与节点分类"><a class="markdownIt-Anchor" href="#信息传播与节点分类"></a> 信息传播与节点分类</h2><p>半监督节点分类问题（<strong>semi-supervised</strong> node classification）</p><p>消息传递框架（message passing framework），关键思想在于，<strong>同类/同标签的节点间倾向于有连接</strong>，也即correlations。</p><p>集体分类（collective classification），节点根据其邻居的标签更新其自身标签。</p><p>相关场景包括比如恶意网页检测、垃圾邮件、欺诈用户检测等等。</p><h3 id="一-基础概念"><a class="markdownIt-Anchor" href="#一-基础概念"></a> 一. 基础概念</h3><p>相关性（correlation）具体体现在以下两个方面：</p><ul><li><p><strong>同构/同质性（Homophily）</strong>：以社交网络为例，具有相似特征的人倾向于相互联系（社会学同质性概念）。具体定义为“The tendency of individuals to associate and bond with similar others”。</p></li><li><p>影响力（Influence）：以社交网络为例，社会关系会影响我们自己的特征或行为；</p></li></ul><p>考虑节点的属性及其邻居节点的标签和属性，确定某节点v标签，方法可以表达为Guilt-by-association。</p><p>可以使用概率框架，依照一阶<strong>马尔科夫假设（Markov Assumption）</strong>，即节点v的标签只取决于其邻居节点们的标签。这里的“一阶”表示我们只考虑当前节点的一跳邻居。</p><p>集体迭代分类包括三个步骤：</p><ul><li>局部分类器（Local Classifier）：为节点<strong>分配初始标签</strong>；这是一个基于节点属性预测标签的标准分类器，与网络结构信息无关。</li><li>关系分类器（Relational Classifier）：捕获节点时间的<strong>相互关系</strong>；此分类器应用到网络结构信息，基于邻居节点属性/标签预测当前节点标签。</li><li>集体推理（Collective Inference）：在网络中<strong>传递相关性/信念（belief）</strong>，直到出现标签并收敛；迭代地将关系分类器应用于每个节点知道相邻节点间的预测结果趋向一致。</li></ul><h3 id="二-经典方法"><a class="markdownIt-Anchor" href="#二-经典方法"></a> 二. 经典方法</h3><h4 id="1关系分类-relational-classification"><a class="markdownIt-Anchor" href="#1关系分类-relational-classification"></a> 1.关系分类 Relational Classification</h4><p>基本思想：节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>的标签概率是其<strong>邻居节点</strong>概率的平均值/边权重加权。有标记节点使用其真实标签，未标记节点，标签初始化为0.5。以随机的顺序更新全部节点概率直到到达迭代最大次数或结果收敛。</p><p>[ 这个方法没有应用到节点属性，也不保证收敛。 ]</p><h4 id="2迭代分类-iterative-classification"><a class="markdownIt-Anchor" href="#2迭代分类-iterative-classification"></a> 2.迭代分类 Iterative Classification</h4><p>基本思想：针对节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>，基于其属性和其节点集合的标签确定它的标签。</p><p>组成：训练两个分类器：1）基础分类器，基于节点属性预测其标签；2）两个输入的分类器：基于节点属性和邻居节点标签（a label summary vector z）预测节点v标签。</p><p>方法：1）训练阶段，完成上述两个分类器的训练；2）在测试集合（unlabeled nodes）中迭代直到收敛，首先使用基础分类器得到初始标签，计算出邻居向量z，使用分类器二得到预测结果；之后进入分类器二的迭代。</p><div align="center">  <img src="/2021/05/06/cs224w2/iteration.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>[ 这个方法不保证收敛。]</p><h4 id="3循环置信传播-loopy-belief-propagation"><a class="markdownIt-Anchor" href="#3循环置信传播-loopy-belief-propagation"></a> 3.循环置信传播 Loopy Belief propagation</h4><p>Loopy表示其适用的图数据中可能有环（cycles）。核心在于potential function/matrix。</p><div align="center">  <img src="/2021/05/06/cs224w2/notion.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>收集下游节点信息，然后结合自己的标签本性，决定其向节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span>传递的信息。</p><div align="center">  <img src="/2021/05/06/cs224w2/bp.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>即便图数据中有环（子图中的各节点不再独立，而是相互影响），我们也可以随意挑选起始节点，然后沿着边传递信息。</p><p>但是有环的时候可能无法收敛，但是实践中效果依然很好，<strong>环不是问题</strong>。最糟糕的情况是下面这样，但在实际中不常见：</p><div align="center">  <img src="/2021/05/06/cs224w2/bpw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>总结一下有关信念传播方法：</p><ul><li>容易代码实现，也容易并行化</li><li>易于应用于各种图模型，可以基于各种潜在矩阵（potential matrix）不一定是上述的label-label矩阵；不只考虑了同构性而是加入了更复杂的关系</li><li>这个方法也是不保证收敛的</li><li>potential matrix需要一定的估算</li><li>是一种非常强大、有效的半监督节点分类方法</li></ul><h2 id="gnn基础"><a class="markdownIt-Anchor" href="#gnn基础"></a> GNN基础</h2><p>之前学习的浅层encoder-decoder节点嵌入方法的局限性在于：</p><ul><li>每个节点有自己的embedding向量，即需要训练<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(|V|)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mord">∣</span><span class="mclose">)</span></span></span></span>个参数</li><li>属于转导学习（transductive），无法应对在训练阶段未出现的节点</li><li>没有考虑节点属性</li></ul><p>Deep Graph Encoders，encoders是基于图结构信息的多层非线性转换。这些编码器可以和之前学到的所有节点相似度计算方法结合。</p><p>这些方法可以应用于节点分类、链路预测、社区发现、网络相似度计算等任务。</p><h3 id="一-深度学习基础"><a class="markdownIt-Anchor" href="#一-深度学习基础"></a> 一. 深度学习基础</h3><div align="center">  <img src="/2021/05/06/cs224w2/sgd.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>常用优化器包括Adam、Adagrad、Adadelta、RMSprop等</p><p>使用深度学习框架使得训练时的梯度计算变得十分容易！</p><p><strong>批量归一化（Batch Normalization）</strong>，用于稳定模型训练过程。给定一批数据，通过变换将其移到均值为0，方差为1的情况。</p><div align="center">  <img src="/2021/05/06/cs224w2/nor.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>Dropout</strong>，防止过拟合。在GNN中，dropout被应用于消息传递即message部分的线性层。</p><div align="center">  <img src="/2021/05/06/cs224w2/dropout.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>激活函数</strong>，非线性化，常用ReLU、Sigmoid、Parametric ReLU（实际场景中基本效果比ReLU要好）等。</p><h3 id="二-图的深度学习"><a class="markdownIt-Anchor" href="#二-图的深度学习"></a> 二. 图的深度学习</h3><p>当图数据中没有节点属性时，一般会选择使用1）指示向量，即节点的one-hot编码或 2）全1向量。</p><p>最简单的方案是直接将邻接矩阵和节点属性矩阵拼接起来，直接加入一个深度神经网络中，类似视为一张图片加入CNN中。但这样做的问题在于：</p><ul><li>需要训练的参数非常多</li><li>无法使用与各种大小的图数据</li><li>对节点次序信息敏感</li></ul><p>将CNN的思想扩展到GNN上，将图片上的邻域扩展成图数据中的<strong>节点邻居集合</strong>。GNN变为两个关键步骤：1）确定某个节点的计算（子）图；2）在其上进行信息传递。</p><p>每个节点都会基于其邻居节点得到它的计算图。</p><div align="center">  <img src="/2021/05/06/cs224w2/graph.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>模型可以有多层，每一层都会有节点的表征信息，第0层节点向量是其输入特征向量，第k层的嵌入向量中考虑了K跳之外的邻居节点信息。</p><p>不同模型的邻居节点<strong>特征汇总方式</strong>会有所不同。注意，邻居聚合函数需要<strong>与节点次序无关</strong>。一般常用Average、sum等。重要的是该模型中所有的节点共享参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span>。</p><div align="center">  <img src="/2021/05/06/cs224w2/gnn.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>以上模型也可以写为矩阵分解的形式，最终的矩阵是比较稀疏的。[ 当聚合方式比较复杂时，这种GNN无法写为矩阵形式]</p><div align="center">  <img src="/2021/05/06/cs224w2/matrix.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/06/cs224w2/rewrite.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>可以基于节点标签进行有监督训练<strong>或者基于图结构信息进行无监督学习</strong>。无监督学习中关于节点相似性的定义可以使用之前介绍过的随机游走、矩阵分解等形式。</p><div align="center">  <img src="/2021/05/06/cs224w2/unsupervised.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>使用GNN模型获取节点表征向量的总结：</p><ul><li>定义邻域聚合函数，确定当前节点邻居节点集合及聚合方式 [key distinctions are in how different approaches aggregate information across the layers]</li><li>定义表征向量损失函数</li><li>在一个集合上完成模型训练</li><li>所有节点共享相同的聚合参数，所以模型参数规模与网络规模是<strong>次线性</strong>的（sublinear），而且模型<strong>具有归纳能力</strong>（inductive），可以扩展到训练集中未出现的节点上，所以可以应用到新的图数据或新发展出的节点上 [注意一下<strong>动态图模型</strong>的研究主题]。</li></ul><h2 id="gnn模型设计"><a class="markdownIt-Anchor" href="#gnn模型设计"></a> GNN模型设计</h2><p>GNN层 = <strong>Message + Aggregation</strong>，如何定义（1）消息和（2）聚合时区分不同GNN模型的关键。另外还有（3）<strong>如何进行层堆叠、（4）如何确定计算图</strong>，以及（5）如何进行模型学习。</p><div align="center">  <img src="/2021/05/06/cs224w2/framework.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="一-单层gnn"><a class="markdownIt-Anchor" href="#一-单层gnn"></a> 一. 单层GNN</h3><p>首先进行“消息计算”，定义一个message function，生成的消息会传递给其它节点。</p><p>之后是聚合，每个节点会聚合来自其邻居的消息，比如使用Sum、Mean、Max等，注意要与节点次序无关（order invariant）。将来自邻居的信息和来自上一层的节点自身的信息结合起来。</p><div align="center">  <img src="/2021/05/06/cs224w2/single.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>最后一般加一个激活函数，比如ReLU，sigmoid等，用来提升（针对节点属性的）表达能力。<div align="center">  <img src="/2021/05/06/cs224w2/gcn_layer.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="1-gcn"><a class="markdownIt-Anchor" href="#1-gcn"></a> 1. GCN</h4><p>节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>在第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span>层的表征是其邻居节点表征的均值。按照message+aggregation的形式可以写成下面的样子：</p><div align="center">  <img src="/2021/05/06/cs224w2/GCN.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2graphsage"><a class="markdownIt-Anchor" href="#2graphsage"></a> 2.GraphSAGE</h4><p>扩展了GCN的聚合函数形式（比如Mean、Pooling、LSTM（需要特殊操作消除次序信息的影响）等），而且使用<strong>CONCAT</strong>的方式把自身消息和邻居消息结合起来。</p><div align="center">  <img src="/2021/05/06/cs224w2/graphsage.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>另外，GraphSAGE在每一层都<strong>加入了L2归一化</strong>，如果没有的话节点向量取值范围不同，加入L2归一化可以提升效果。</p><h4 id="3-gat"><a class="markdownIt-Anchor" href="#3-gat"></a> 3. GAT</h4><p>对于每个邻居加入一个权重，体现针对当前节点，其不同邻居的不同重要性。</p><p>GCN和GraphSage直接使用了平均即权重为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>N</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">1/N(v)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mclose">)</span></span></span></span>，即每个邻居节点都同样重要。</p><p><strong>Attention</strong>是受认知注意力启发而来，注意力系数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>v</mi><mi>u</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{vu}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span><span class="mord mathdefault mtight">u</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>强调输入数据中重要的部分而忽略其他内容。在模型训练过程中学习到哪部分数据是重要的。</p><div align="center">  <img src="/2021/05/06/cs224w2/GAT.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>注意力机制</strong></p><p>在节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>u</mi><mo separator="true">,</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">u,v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>之间计算注意力系数（attention coefficients <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>e</mi><mrow><mi>u</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">e_{uv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>），表示来自节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">u</span></span></span></span>的信息对节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>的重要程度。之后对<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>e</mi><mrow><mi>u</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">e_{uv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>进行归一化（比如softmax）得到最终的权重系数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mrow><mi>u</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">a_{uv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。最终这个参数和其它参数如权重矩阵W一起训练，模型是一个端到端的效果。</p><div align="center">  <img src="/2021/05/06/cs224w2/attention.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>加入注意力机制后的模型可能很难训练，很难收敛。通过使用多头注意力机制（multi-head attention）稳定其学习过程。本质思想是设计多个注意力函数，聚合之后一起计算，增加模型鲁棒性。每一个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mrow><mi>u</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">a_{uv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>使用不同的函数预测，而且每个函数都随机其初始值。</p><div align="center">  <img src="/2021/05/06/cs224w2/multi.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>加入注意力机制的好处：</p><ul><li>区分了不同邻居节点对当前节点的重要程度</li><li>计算高效，参数的计算可以并行。注意力系数可以按不同边并行；聚合操作可以在节点间并行</li><li>存储高效，稀疏矩阵运算符，参数数目固定（与图大小无关）</li><li>局部化，只关注局部网络邻居信息</li><li>可扩展，具有归纳能力，是一种shared edge-wise机制，不依赖全局图结构</li></ul><h3 id="二-gnn堆叠"><a class="markdownIt-Anchor" href="#二-gnn堆叠"></a> 二. GNN堆叠</h3><h4 id="1-经典方式"><a class="markdownIt-Anchor" href="#1-经典方式"></a> 1. 经典方式</h4><p>直接将GNN层sequentially叠加起来。</p><p>GNN的深度表示我们在得到某一节点表征的时候考虑了它的几跳邻居，而GNN模型的表达能力/复杂度是取决于单层GNN层的设计。</p><p>直接堆叠起来会带来<strong>over-smoothing</strong>的问题，无法构造比较深层的GNN模型。到达一定深度，所有节点的表征向量会收敛到一起。</p><p>可以通过<strong>Receptive field</strong>（感受野）解释这个问题，GNN中可定义为为获取某节点表征向量而需要考虑的节点集合。在一个K层GNN模型中，每个节点的感受野就是它的K跳（以内的）邻居。</p><p>堆叠多层GNN层 ——&gt; 各节点的感受野重合程度过大 ——&gt; 得到的节点表征向量过于近似  ——&gt; 造成过平滑问题</p><p>最直接的解决办法是在构造模型是注意堆叠的GNN层数。首先分析解决任务必要的感受野（比如<strong>首先计算一下图数据的半径</strong>），GNN的层数可以比这个感受野稍大一些。</p><p><strong>如何让浅层GNN模型更具表达力？</strong></p><p><strong>解决方法1，提升每层GNN的表达能力</strong>；之前介绍的模型中每层里的聚合或变换都只使用了线性层，我们可以改成使用深度学习网络，比如换成3层MLP。[ 这里也可以看出，GNN中的层概念和DNN中有所不同 ]</p><p><strong>解决方法2，添加不进行消息传递的层</strong>；GNN模型中不一定只包含GNN层，在GNN层之前/之后都可以加入MLP层进行预处理或后处理。</p><div align="center">  <img src="/2021/05/06/cs224w2/MLP.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-skip-connection"><a class="markdownIt-Anchor" href="#2-skip-connection"></a> 2. Skip connection</h4><p>如何构建深层GNN模型？可以在GNN层之间加入一些跳过连接（skip connection）。</p><p>研究发现有时候比较前面的GNN层得到的节点表征结果更有利于区分节点，所以可以在最终结果中提升这些层的重要性。skip connection可以构建mixture of models，即上一层和当前层信息的加权组合。由此我们得到了一个深度GNN模型和一些浅层GNN模型（如果有N个跳过连接，会得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mi>N</mi></msup><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2^N-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.924661em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>个浅层模型）的混合模型。</p><p>跳过的操作可以如下图所示，也可以直接跳到最后一层（如ICML 2018中写到的），有很多种方式。</p><div align="center">  <img src="/2021/05/06/cs224w2/skip.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="三-图征增强-graph-augmentation"><a class="markdownIt-Anchor" href="#三-图征增强-graph-augmentation"></a> 三. 图征增强 Graph Augmentation</h3><p>很多情况下原始的图数据不适合直接作为计算图数据，而是需要一些改进。</p><h4 id="1特征增强"><a class="markdownIt-Anchor" href="#1特征增强"></a> 1.特征增强</h4><p>原始的图数据可能缺少某些特征信息  ——&gt; feature augmentation</p><p><strong>问题1，输入图数据中不含有节点特征</strong></p><p>方法1：直接赋予常量值作为特征</p><p>方法2：给每个节点赋予独特的ID，并且进行独热编码；但这个方法问题在于无法很好地扩展到其它图数据上</p><div align="center">  <img src="/2021/05/06/cs224w2/aug.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>问题2，有些结构对于GNN来说很难学</strong></p><p>比如，如下图所示，节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>处于某个环形结构中，GNN很难学习到这个结构的周长，区分这两个结构。</p><div align="center">  <img src="/2021/05/06/cs224w2/feature.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>方法，可以把这个结构信息编辑到节点特征中去，比如左图中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>v</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">v_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的独热编码在第三位（处于周长为3的环形中），右图中的在第四位。</p><p>其它方法还包括使用节点度、聚合系数、PageRank、中心度等之前介绍过的特征。</p><h4 id="2结构增强"><a class="markdownIt-Anchor" href="#2结构增强"></a> 2.结构增强</h4><p>原始图数据结构可能：</p><ul><li>过于稀疏，影响消息传递   ——&gt; 添加虚拟节点或边</li><li>过于密集，消息传递花费过大 ——&gt; 消息传递时进行邻居采样，降低计算成本</li><li>图太大，无法装载如显存 ——&gt; 计算时进行子图采样</li></ul><p><strong>添加虚拟边</strong></p><p>使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><mo>+</mo><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">A + A^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>进行计算，而非单独使用邻接矩阵A表示图，这样我们给2-hop的邻居加入了虚拟边。</p><p>比如在author-paper的二部图中，使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">A^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>就构造了一个共同作者网络。</p><p><strong>添加虚拟节点</strong></p><p>给稀疏图中加入一个节点，这个虚拟节点与图中所有的节点都相连。</p><p><strong>节点采样</strong></p><p>可以采样当前节点的邻居节点进行消息聚合，而非考虑其所有邻居的消息。</p><p>如何选取邻居子集呢？是一个可以优化研究的问题</p><h2 id="gnn模型训练"><a class="markdownIt-Anchor" href="#gnn模型训练"></a> GNN模型训练</h2><div align="center">  <img src="/2021/05/06/cs224w2/GNNt.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="一-预测任务"><a class="markdownIt-Anchor" href="#一-预测任务"></a> 一. 预测任务</h3><h4 id="1node-level-prediction"><a class="markdownIt-Anchor" href="#1node-level-prediction"></a> 1.Node-level prediction</h4><p>直接把GNN得到的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span>维表征向量映射到为K类标签向量。</p><div align="center">  <img src="/2021/05/06/cs224w2/node.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-edge-level-prediction"><a class="markdownIt-Anchor" href="#2-edge-level-prediction"></a> 2. Edge-level prediction</h4><p>以节点对表征向量为输入映射到K类标签向量。</p><p>处理节点对向量常用方法有两类：1）Concat + Linear（GAT中使用过）；</p><div align="center">  <img src="/2021/05/06/cs224w2/2d.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>2）向量内积，一般会得到一个标量结果，如果想做成K向预测，可以使用类似多头注意力机制的处理方法。</p><div align="center">  <img src="/2021/05/06/cs224w2/kway.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="3-graph-level-prediction"><a class="markdownIt-Anchor" href="#3-graph-level-prediction"></a> 3. Graph-level prediction</h4><p>需要将节点的表征向量聚合来得到表示整个图的向量。可以使用很多pooling方法，比如Mean、Max、Sum等。</p><p>但有时候简单的全局池化会损失太多信息，尤其当图规模比较大的时候。</p><p>改善这个问题可以使用<strong>分层池化</strong>（hierarchical global pooling）。可以加入聚类方法确定每层中可以聚合的节点集合。这里的两个GNN模型是独立的，每层的模型可以并行训练。</p><div align="center">  <img src="/2021/05/06/cs224w2/diffpool.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="二-预测和标签"><a class="markdownIt-Anchor" href="#二-预测和标签"></a> 二. 预测和标签</h3><p>图上的监督学习：标签来自外部；图上的无监督学习：标签来自图数据自己，比如链路预测问题。但实际上二者之间的界限比较模糊。</p><p>在无监督学习中，可以使用图数据自身产生的标签信息：</p><ul><li>Node-level，使用节点统计数据，比如聚类系数、PageRank等</li><li>Edge-level，比如在链路预测任务中隐藏两节点间的边</li><li>Graph-level，使用图特征，比如是否两个图是同构的</li></ul><h3 id="三-损失函数与评估"><a class="markdownIt-Anchor" href="#三-损失函数与评估"></a> 三. 损失函数与评估</h3><p>GNN可以进行分类任务，也支持回归任务（标签<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span></span>具有连续值）</p><p>分类问题中常用交叉熵，重点强调了K向预测时的交叉熵。</p><div align="center">  <img src="/2021/05/06/cs224w2/entropy.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>回归问题一般使用MSE，K向回归计算如下：</p><div align="center">  <img src="/2021/05/06/cs224w2/kreg.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>评估GNN时一般也是使用sklearn中的常规指标。评估回归问题的时候常用RMSE、MAE等。</p><h3 id="四-数据切分"><a class="markdownIt-Anchor" href="#四-数据切分"></a> 四. 数据切分</h3><p>图数据中有的时候无法明确区分出测试集，因为图中的节点是相互连接的，彼此之间不独立。</p><p>方案1：transductive setting，只根据标签区分，训练阶段可以看到所有数据，我们只需要区分不同节点的标签</p><div align="center">  <img src="/2021/05/06/cs224w2/tran.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>方案2：inductive setting, 将图数据中的某些边截断，区分成多个子图</p><div align="center">  <img src="/2021/05/06/cs224w2/ind.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>两个方法的区别主要在于：</p><div align="center">  <img src="/2021/05/06/cs224w2/vs.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>在各类任务中注意一下inductive setting的<strong>链路预测任务</strong>，一般说起链路预测的话是transductive setting的任务。</p><p><strong>链路预测任务</strong>中，训练时可以看到message edge而supervision edge不喂入GNN模型。</p><p>inductive setting设置如下：</p><div align="center">  <img src="/2021/05/06/cs224w2/indl.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>transductive setting为：</p><div align="center">  <img src="/2021/05/06/cs224w2/tranl.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>【注意，以上是教授提出的认为正确的区分方法，不同论文中的划分方法有所不同】</p><h2 id="gnn表达能力"><a class="markdownIt-Anchor" href="#gnn表达能力"></a> GNN表达能力</h2><p>目前已经有很多GNN模型被提出来，大家的区分点主要是在消息处理和聚合时使用的网络不同。</p><p>例如GCN使用element-wise mean pooling+ Linear + ReLU。GraphSAGE使用MLP + max-pool。</p><div align="center">  <img src="/2021/05/06/cs224w2/model.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>GNN模型的表达能力主要体现在它的计算图（或叫做subtree rooted around each node）上，如下图所示，GNN得到的映射要尽可能做到<strong>单射（injective）</strong>。如果每层GNN上的聚合都是单射的，那么这个GNN模型就可以完全区分不同的子树模型。</p><div align="center">  <img src="/2021/05/06/cs224w2/emb.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>GCN和GraphSAGE并不是单射的，这部分内容在《图神经网络的局限》博文中有总结和介绍。</p><p>由此比较，SUM pooling具有较强表达/判别能力，其次是Mean pooling，再次是Max pooling。</p><h3 id="一-设计具有强表达能力的gnn模型"><a class="markdownIt-Anchor" href="#一-设计具有强表达能力的gnn模型"></a> 一. 设计具有强表达能力的GNN模型</h3><p>通过设计单射的邻居聚合方法构建具有最佳表达能力的基于消息传递的GNN模型。</p><div align="center">  <img src="/2021/05/06/cs224w2/inject.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>根据“含有一层足够多神经元隐藏层的MLP可以拟合任意函数”的定力，作者使用MLP设计<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">ϕ</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span>。实验表明，大概用100~500个神经元，可以训练出很好的单射函数。由此作者提出GIN模型。</p><p>[ GIN is the most expressive GNN n the class of message-passing GNNs! ]</p><p>[ The key is to use element-wise sum pooling. ]</p><h3 id="二-使用wl测试解释gin模型"><a class="markdownIt-Anchor" href="#二-使用wl测试解释gin模型"></a> 二. 使用WL测试解释GIN模型</h3><p>WL测试中可以使用color refine算法，如果两个图模型拥有相同颜色集合，则表示它们同构。</p><div align="center">  <img src="/2021/05/06/cs224w2/color.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>GIN模型使用MLP模仿color refine算法中的单射哈希方法.</p><div align="center">  <img src="/2021/05/06/cs224w2/gin.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>两种方法的对比如下，<strong>两模型的表达能力相当</strong>。GIN的优势在于：1）它可以得到低阶表征向量；2）方程的参数训练可以利用到下游任务中的信息。WL已于1992年被证实可以区分大多数实际图模型。</p><div align="center">  <img src="/2021/05/06/cs224w2/com.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>### 三. 当前挑战<p>目前还有一些GNN无法区分的基础图结构，比如之前提到的不同节点数的环形。</p><p>已有工作在这方面开始努力。 [You et al. AAAI 2021, Li et al. NeurIPS 2020]</p><h2 id="异构图"><a class="markdownIt-Anchor" href="#异构图"></a> 异构图</h2><h3 id="一-关系型gcnrgcn"><a class="markdownIt-Anchor" href="#一-关系型gcnrgcn"></a> 一. 关系型GCN（RGCN）</h3><p>加入不同类型连接下的消息转换机制。</p><div align="center">  <img src="/2021/05/06/cs224w2/rGCN.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>可扩展性问题</strong>：随着关系类型数量的增多，RGCN的参数量上涨非常快，可能导致过拟合。可以采用以下两种技术减少模型参数量：</p><p><strong>方法一，块对角矩阵</strong></p><p>我们希望消息传递矩阵W是稀疏的，可以使用它的块对角矩阵，但这也造成只有相邻的节点间可以彼此传递信息，所以整个GCN模型可能要更深一些。</p><div align="center">  <img src="/2021/05/06/cs224w2/wGCN.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>方法二，基准/词典学习</strong></p><p>在不同类型的连接之间共享参数，所有的消息传递矩阵W都表示成不同基础矩阵V的组合。</p><div align="center">  <img src="/2021/05/06/cs224w2/bGCN.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>之后介绍了在RGCN下的节点分类与链路预测（重点介绍）问题。</p><p>基于RGCN的思想，可以很方便地得出RGraphSAGE、RGAT等。</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>联邦图模型论文与开源框架</title>
    <link href="/2021/05/05/fedgraphNN/"/>
    <url>/2021/05/05/fedgraphNN/</url>
    
    <content type="html"><![CDATA[<p>目前的论文课题需要用到图模型联邦学习技术，将调研结果总结为本文。</p><p>文章主要包括联邦学习内容梳理（依据FedML论文、谷歌的巨长综述等）、图模型联邦学习相关论文介绍、联邦学习框架、FedGraphNN代码等部分。</p><h2 id="联邦学习"><a class="markdownIt-Anchor" href="#联邦学习"></a> 联邦学习</h2><p>传统学习方法为<strong>集中式学习</strong>，将数据收集至服务端进行训练和预测，结果发送至客户端，但这会造成延时高、浪费终端设备资源、数据隐私风险大等问题。而如果将模型放到<strong>端侧</strong>，终端根据本地数据完成模型训练和预测又存在数据量少且无法利用其它用户数据信息的问题。故2016年由谷歌提出联邦学习。联邦学习（FL）是一种分布式学习框架，许多客户端（如移动设备、组织）在中央服务器（如服务提供商）的协调下<strong>共同训练模型</strong>，同时保护<strong>本地数据隐私</strong>。广泛来讲，联邦学习是为了解决<strong>数据孤岛</strong>问题。</p><p>目前联邦学习已成为一个比较热门的研究方向。作为一个研究方向，“联邦学习”概念很宽泛，经过这几年的研究其边界得到极大扩大。比如学习过程中的数据不均衡、数据非独立同分布（non-I.I.D）、设备不可靠、有限通信带宽等挑战。</p><p>联邦学习和一般<strong>分布式学习</strong>的主要区别？[ 主要还是数据的问题 ]</p><table><thead><tr><th></th><th><strong>分布式训练</strong></th><th><strong>联邦学习</strong></th></tr></thead><tbody><tr><td>数据分布</td><td>集中存储，但可以任意打乱、平衡地分配给所有客户端</td><td>分布式存储，数据无法互通、可能存在数据的Non-IID</td></tr><tr><td>节点数量</td><td>1~1000</td><td>1~10^10</td></tr><tr><td>节点状态</td><td>所有节点稳定运行</td><td>节点可能不在线</td></tr></tbody></table><p>联邦学习理论研究主要集中于针对优化算法的收敛性问题。</p><p>下表为FedML作者何朝阳总结的FL领域相关研究方向：</p><div align="center">  <img src="/2021/05/05/fedgraphNN/challenge.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="1-fedavg"><a class="markdownIt-Anchor" href="#1-fedavg"></a> 1. FedAvg</h4><p>Server端将client端传送过来的模型（参数）进行加权求平均，之后发送会client端，反复迭代，直到模型收敛（已经证明）。如下图划线部分所示，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>n</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">n_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>代表第K个客户端所拥有的样本数量。</p><div align="center">  <img src="/2021/05/05/fedgraphNN/fedavg.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2-fedopt"><a class="markdownIt-Anchor" href="#2-fedopt"></a> 2. FedOPT</h4><p>FedOPT将本地的优化器和服务器端的优化器加以区分，两个优化器互相配合。</p><p>可作为一个通用框架理解FL中的一些挑战（针对下图框架中<strong>各个参数进行更现实的优化</strong>）：</p><ul><li>根据每个client端的计算能力设置其不同的迭代次数K</li><li>在Server可以将梯度视为伪梯度进行优化</li><li>每个client端的数据是异构的如何解决（这是区分于distributed learning的最大的点）</li><li>如何在端侧训练大模型</li><li>每一轮采样client的策略，传统使用uniform sampling，没有区分用户之间信息重要程度的差异</li><li>服务端聚合算法研究</li></ul><div align="center">  <img src="/2021/05/05/fedgraphNN/fedopt.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-non-iid问题"><a class="markdownIt-Anchor" href="#二-non-iid问题"></a> 二. Non-IID问题</h3><p>各客户端之间数据分布的不一致性，包括特征、标签、数量等方面的分布偏差，甚至还有<strong>概念漂移</strong>问题。</p><p>非独立同分布主要有三个方面：</p><ul><li><strong>不同客户端数据分布不同</strong><ul><li>特征分布倾斜：P(x)不同；比如不同人的笔迹不同</li><li>标签分布倾斜：P(y)不同；比如企鹅在南极、北极熊在北极</li><li>标签相同特征不同：P(x|y)不同；概念飘移</li><li>特征相同标签不同：P(y|x)不同；比如点头表示yes / no?</li><li>数量不平衡</li></ul></li><li><strong>数据偏移</strong>：训练集测试集不同分布；比如以狗在草地上奔跑的照片训练，但使用狗在海里游泳的照片测试识别能力</li><li><strong>非独立</strong>：可用节点大多在附近的时区，图数据</li></ul><p>文章<a href="https://arxiv.org/pdf/1806.00582.pdf" target="_blank" rel="noopener">《Federated Learning with Non-IID Data》</a>重点研究了这个现象，定义了Weight divergence指标观察全局模型与本地模型之间的差异，发现FedAvg聚合策略下有non-IID的数据集的Global和Local模型间差异很大。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>d</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mo>=</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msup><mi>W</mi><mrow><mi>F</mi><mi>e</mi><mi>d</mi></mrow></msup><mo>−</mo><msup><mi>W</mi><mrow><mi>S</mi><mi>G</mi><mi>D</mi></mrow></msup><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">/</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msup><mi>W</mi><mrow><mi>S</mi><mi>G</mi><mi>D</mi></mrow></msup><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">Weight divergence = ||W^{Fed} - W^{SGD}||/||W^{SGD}||</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault">e</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">h</span><span class="mord mathdefault">t</span><span class="mord mathdefault">d</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">c</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.149108em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">F</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord">∣</span><span class="mord">/</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord">∣</span></span></span></span></span></p><div align="center">  <img src="/2021/05/05/fedgraphNN/wd.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>后续后很多方法致力于改进此类问题，一般有三种思路：1）修改现有算法；2）创建一个可全局共享的小数据集；3）为不同客户端提供不同模型。</p><p><a href="https://arxiv.org/pdf/2102.02079.pdf" target="_blank" rel="noopener">Federated Learning on Non-IID Data Silos: An Experimental Study》</a>和<a href="https://arxiv.org/abs/2102.09743" target="_blank" rel="noopener">《Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques》</a>均有所总结。</p><div align="center">  <img src="/2021/05/05/fedgraphNN/non.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="1-fedprox"><a class="markdownIt-Anchor" href="#1-fedprox"></a> 1. FedProx</h4><p>FedAvg中固定时间内没有完成E轮迭代的客户端会被drop掉，而实际中有些设备的计算/通信性能较差，这会严重影响模型效果。</p><p>针对**用户间数据异构（non-IID）和系统异构（设备间通信和计算能力差异）**两大挑战。</p><p>修改Client端损失函数，通过加入<strong>proximal term修正项</strong>，确保本地更新不要太够远离初始的global模型，即如下图所示，客户端损失函数由<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F_k(*)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">∗</span><span class="mclose">)</span></span></span></span>变为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_k(*)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">∗</span><span class="mclose">)</span></span></span></span>。</p><p>定义<strong>inexact solution项</strong><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>y</mi><mi>k</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">y_k^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0766639999999998em;vertical-align:-0.2831079999999999em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-2.4168920000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831079999999999em;"><span></span></span></span></span></span></span></span></span></span>，通过对本地模型的非精确求解，动态调整本地迭代次数，弥合系统中不同client端的性能差异性。</p><div align="center">  <img src="/2021/05/05/fedgraphNN/prox.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2-skewscout"><a class="markdownIt-Anchor" href="#2-skewscout"></a> 2. SkewScout</h4><h2 id="图模型联邦学习"><a class="markdownIt-Anchor" href="#图模型联邦学习"></a> 图模型联邦学习</h2><h3 id="fedgnn"><a class="markdownIt-Anchor" href="#fedgnn"></a> 《FedGNN》</h3><p>使用元学习解决non-IID数据问题</p><h3 id="fedgraphnna-federated-learning-system-and-benchmark-for-graph-neural-networks"><a class="markdownIt-Anchor" href="#fedgraphnna-federated-learning-system-and-benchmark-for-graph-neural-networks"></a> 《FedGraphNN：A Federated Learning System and Benchmark for Graph Neural Networks》</h3><p>文章没有相关算法创新，偏重于工程，是基于FedML框架的针对GNN算法的实现，针对Graph-level FL，即每个客户端有自己的图模型。</p><p>实验结果表明：</p><ul><li>直接将现有方法应用于GNN无法达到良好效果，基本相对于集中式学习都有所折扣ROC-AUC下降大概0.05~0.1。</li><li>可能由于non-IID影响，集中式表现更好的模型在联邦模式中比一定表现最好，比如文中实验里GAT效果下降明显</li></ul><p>主要是看看里面的代码实现。</p><h3 id="fedglfederated-graph-learning-framework-with-global-self-supervision"><a class="markdownIt-Anchor" href="#fedglfederated-graph-learning-framework-with-global-self-supervision"></a> 《FedGL：Federated Graph Learning Framework with Global Self-Supervision》</h3><p>定义了<strong>在图数据上</strong>进行联邦学习会面临的两大问题：<strong>heterogeneity和complementarity</strong>。如下图所示，<strong>异构性</strong>即每个客户端的图数据上节点数、边数和标签情况分布不同；<strong>互补性</strong>即每个客户端上的节点以及边的情况可能有重合/补充，比如client A这里节点5和节点7之间没有边，但client B这边是有边的。</p><div align="center">  <img src="/2021/05/05/fedgraphNN/ques.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>针对这两个问题，作者在传统FL步骤基础上加入了对于<strong>client端预测结果和节点表征结果</strong>传送和处理的步骤，在server端形成<strong>全局伪标签（non-IID问题）<strong>和</strong>全局伪图（互补性问题）</strong>。</p><h4 id="1-全局伪标签"><a class="markdownIt-Anchor" href="#1-全局伪标签"></a> 1. 全局伪标签</h4><p>处理各client端的预测结果，加权组合形成全部节点的预测结果<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>1</mn></msub><mi mathvariant="normal">，</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>P</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">P_1，...,P_K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">，</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。在每个预测结果中选取概率值较高的维度作为标签。</p><p>Server端将这些伪标签数据下放，提升client端模型训练效果。</p><h4 id="2全局伪图"><a class="markdownIt-Anchor" href="#2全局伪图"></a> 2.全局伪图</h4><p>处理各client端的节点表征结果，加权组合形成全局节点表征<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>H</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">H_1,...,H_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, 即矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>H</mi><mo>˙</mo></mover></mrow><annotation encoding="application/x-tex">\dot H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9201900000000001em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9201900000000001em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.08333000000000002em;">˙</span></span></span></span></span></span></span></span></span>, 将此矩阵乘以其转置得到整个图的伪邻接矩阵，下放至client端提升模型效果。</p><p>client接收到server端的信息由三种：模型信息、伪标签信息和伪全图信息。伪全图信息被直接用来完善当前client上的图结构（补全边但是不增加节点），伪标签则被用来构建自监督学习（在损失函数中加入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi>S</mi><mi>S</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{SSL}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>项）。</p><p><strong>【问题就在于，这些也把client端的信息暴露的差不多了啊…】</strong></p><p>以上操作需要server知道每个client上到底有哪些节点且每个节点有单独索引的基础上，这样才能处理client上传的预测值和表征向量。这样联邦学习只保护了节点的属性、图结构信息以及本身的标签，但是暴露了节点信息（可能某些情况下不重要？）</p><h2 id="联邦学习框架"><a class="markdownIt-Anchor" href="#联邦学习框架"></a> 联邦学习框架</h2><p>当前常见的联邦学习框架包括Tendorflow Federated（Google），LEAF benchmark（CMU），FedML以及工业界提供过的PySyft和FATE。做科研一般不会选择使用工业界框架，而是直接单机模拟。</p><p>之前有博客总结了以下几个框架：</p><ul><li>TensorFlow Federated专门针对研究用例，提供大规模模拟功能以及灵活的编排来控制采样。</li><li>PySyft 是用于安全的私有深度学习Python库。 PySyft使用PyTorch中的联邦学习，差分隐私和多方计算（MPC）将私人数据与模型训练分离。</li><li>Leaf 提供了多个数据集以及模拟和评估功能。</li><li>FATE（Federated AI Technology Enabler）是一个开源项目，旨在提供安全的计算框架来支持联邦AI生态系统。</li><li>PaddleFL 是基于PaddlePaddle 的开源联邦学习框架。在PaddleFL中，通过应用程序演示提供了几种联邦学习策略和训练策略。</li><li>Clara培训框架包括基于服务器客户端方法和数据隐私保护的跨孤岛联邦学习的支持</li></ul><p>本文主要介绍<strong>FedML框架</strong></p><h3 id="一-背景介绍"><a class="markdownIt-Anchor" href="#一-背景介绍"></a> 一. 背景介绍</h3><p>一些工业界主推的框架比较偏重分布式，对科研人员的学习成本较高；而另一些框架使用单机模拟，串行模拟不同client端的训练，耗时长，FedML希望弥补这里的gap。FedML是一个<strong>以科研为主要导向</strong>的开源框架，目前包括FedML-IoT、FedNLP、FedCV等部分，完整细节介绍可以参看其官网（<a href="https://fedml.ai/%EF%BC%89%E5%92%8C%E8%AE%BA%E6%96%87%E3%80%8AFedML:" target="_blank" rel="noopener">https://fedml.ai/）和论文《FedML:</a> A Research Library and Benchmark for Federated Machine Learning》。</p><p>FedML基于Pytorch，目前支持单机、分布式和边缘计算模式，并且方便自己的代码、数据集快速嵌入框架。FedML自身实现了不同FL场景下的很多种算法，如FedAvg、FedOPT、FedNAS等，另外其中包含了多种benchmark数据集。</p><p>FedML有如下几个<strong>值得注意的重要功能</strong>：</p><ul><li>WorkerManager中使用预定义的API：<code>register_message_receive_handler</code>和<code>send_message</code>可以定义任何类型可传送的数据，比如除梯度之外的一些辅助信息（auxiliary information）。</li><li><code>TopologyManager</code>可以支持多种联邦学习架构，比如常见的集中式，分布式（无server节点）、层次式、纵向联邦（VFL，又名特征分割FL）、Split Learning、FedNAS、Turbo-Aggregate等。</li></ul><div align="center">  <img src="/2021/05/05/fedgraphNN/topo.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><ul><li>在底层API即<code>FL-core</code>中包含有关FL的安全性保障机制，如secret sharing，key agreement，digital signature等。同时也包含了最新的鲁棒性聚合方法，如DP，RFA，KRUM等。</li><li>相较于使用standalone的一些框架如PySyft、LEAF、TTF等（串行模拟），支持分布式训练（多GPU，以及同一个GPU上的多进程），速度更快。比如作者在CIFAR-10上训练ResNet时，是在8块GPU上模拟了112个worker。</li></ul><p>建议<strong>从FedAvg入手</strong>本框架。</p><h3 id="二-源代码分析"><a class="markdownIt-Anchor" href="#二-源代码分析"></a> 二. 源代码分析</h3><p><strong><a href="http://FedAvgAPI.py" target="_blank" rel="noopener">FedAvgAPI.py</a></strong>文件中封装了FedAvg算法。</p><p><code>process_id</code>参数仿照MPI架构实现（对应其区分不同进程的rank_id），取值为0时表示为server端，其他值为client端。Server端创建Aggregator、ServerManager（aggregator会作为它的一个参数）；client端创建Trainer和ClientManager。</p><p>整个模型的统领过程由ServerManager完成</p><ul><li>ServerManager中提供的register_message_receive_handler用于接收client端传来的参数并callback其中handle_message_receive_model_from_client函数，其中的aggregator会记录客户端发送过来的参数以及客户端的ID，当所有客户端发送完毕后（异步调用）会启动aggregate，结束后会进行下一轮客户端采样，将合并后的模型发送回客户端。</li><li>send_message_init_config，初始化需要发送的消息</li><li>send_message_sync_model_to_client，服务器发送消息到客户端</li><li>在Aggregator中完成测试任务</li></ul><p>客户端这边要接收到服务器端的两种消息：</p><ul><li>初始化消息与合并后的模型消息</li><li>模型更新（handle_message_receive_model_from_server，update_model），round次数更新（达到设定值后销毁进程）</li><li>__train，模型更新</li></ul><p>注意其中实现<strong>采样的机制</strong>是通过换数据的方式进行的，为了尽力模拟现实中大量用户的情况。</p><p><code>Message_define.py</code>中可以自定义需要传递的消息。</p><h3 id="三-环境配置与调试"><a class="markdownIt-Anchor" href="#三-环境配置与调试"></a> 三. 环境配置与调试</h3><p>两种配置方式，一种参考CI流程（用于线上验证），另一种是参考FedML博文（<a href="http://doc.fedml.ai/#/installation-distributed-computing%EF%BC%89%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%EF%BC%8C%E6%8E%A8%E8%8D%90%E4%BD%BF%E7%94%A8%E5%89%8D%E4%B8%80%E7%A7%8D%E3%80%82" target="_blank" rel="noopener">http://doc.fedml.ai/#/installation-distributed-computing）中的分布式环境搭建，推荐使用前一种。</a></p><p>分布式实验环境中NFS可以方便管理。</p><p>CI-install.sh中pyflakes检查脚本是否有误，使用miniconda加快CI速度。</p><p>standalone单机版适合小数据集下的浅层模型，规模变大后推荐distributed模式。</p><p>FedML_mobile，采用Flask+Pytorch+RabbitMQ实现，对科研人员的学习成本较低。首先架设FedML服务器…这一部分没有仔细看，后续如果用到的话可以参考B站视频。</p><h2 id="fedgraphnn"><a class="markdownIt-Anchor" href="#fedgraphnn"></a> FedGraphNN</h2><p>以FedML为基础打造</p><h2 id="参考材料"><a class="markdownIt-Anchor" href="#参考材料"></a> 参考材料</h2><ul><li>《Advances and Open Problems in Federated Learning》Google主导综述</li><li>《FedML: A Research Library and Benchmark for Federated Machine Learning》</li><li>《FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks》</li><li>FedAvg：《Communication-Efficient Learning of Deep Networks from Decentralized Data》</li><li>FedOPT（ICML 2020）:《Adaptive federated optimization》</li><li>FedProx：《[Federated Optimization in Heterogeneous Networks》</li><li>non-IID：《Federated Learning with Non-IID Data》</li><li>non-IID：《Federated Learning on Non-IID Data Silos: An Experimental Study》</li><li>non-IID：《Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques》</li><li>non-IID：《Survey of Personalization Techniques for Federated Learning》</li><li><a href="https://fedml.ai/" target="_blank" rel="noopener">https://fedml.ai/</a></li><li><a href="https://github.com/chaoyanghe/Awesome-Federated-Learning" target="_blank" rel="noopener">https://github.com/chaoyanghe/Awesome-Federated-Learning</a></li><li>B站视频，FedML联邦机器学习框架视频教学全集</li><li>B站视频，联邦学习综述FAQ</li><li>B站视频，联邦学习FATE课程系列</li><li>知乎专栏：《联邦学习论文分享》</li></ul>]]></content>
    
    
    <categories>
      
      <category>知识梳理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
      <tag>联邦学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cs224w《图机器学习》2021（一）经典方法</title>
    <link href="/2021/05/05/cs224w/"/>
    <url>/2021/05/05/cs224w/</url>
    
    <content type="html"><![CDATA[<h2 id="学习资料"><a class="markdownIt-Anchor" href="#学习资料"></a> 学习资料</h2><p>相比于2019年的课堂录播，本年度直接使用线上课程形式，更加方便理解学习。</p><div align="center">  <img src="/2021/05/05/cs224w/dagang.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>课程官方网站：<a href="http://web.stanford.edu/class/cs224w/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224w/</a></p><p>课程视频链接：Youtube（<a href="https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn%EF%BC%89%E3%80%81B%E7%AB%99%EF%BC%88https://www.bilibili.com/video/BV1RZ4y1c7Co%EF%BC%89" target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn）、B站（https://www.bilibili.com/video/BV1RZ4y1c7Co）</a></p><p>参考书目：《Graph Representation Learning 》by Will Hamilton</p><p>编程工具：<a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html" target="_blank" rel="noopener">Pytorch Geometric（PyG）</a>、DeepSNAP、GraphGym、<a href="http://SNAP.PY" target="_blank" rel="noopener">SNAP.PY</a>、<a href="https://networkx.org/documentation/stable/tutorial.html" target="_blank" rel="noopener">NetworkX</a></p><h3 id="相关论文"><a class="markdownIt-Anchor" href="#相关论文"></a> 相关论文</h3><ul><li>PinSAGE，《Graph Convolutional Neural Networks for Web-Scale Recommender Systems》KDD，2018</li><li>DeepWalk, 《Online Learning of Social Representations》KDD 2014</li><li>node2vec，《node2vec: Scalable Feature Learning for Networks.》KDD 2016</li><li>Graph Embedding Survey，《Graph Embedding Techniques, Applications, and Performance: A Survey》2017</li><li>子图表征，引入虚拟节点，《Gated Graph Sequence Neural Networks》</li><li>匿名游走，《Anonymous Walk Embeddings》ICML 2018</li><li>矩阵分解与节点表征，《Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec》WSDM 2018</li></ul><h2 id="背景"><a class="markdownIt-Anchor" href="#背景"></a> 背景</h2><p>为什么使用图模型？</p><p>不仅考虑数据本身，还要考虑实体间的复杂关系。How do we take advantage of relational structure for better prediction?</p><p>多种图模型样例：计算机网络、疾病传播、食物链网络、交通网络、社交网络、论文引用、神经元连接、知识图谱、代码、化学分子、3D建模等等。</p><p>当前深度学习模型一般应用于序列、图片等简单的数据结构。Graphs are the new frontier of deep learning.</p><p>图模型常见任务：</p><ul><li>节点分类：DeepMind提出AlphaFold解决生物学领域蛋白质折叠问题；蛋白质序列中的氨基酸为节点，氨基酸（残基）之间的接近度为边；</li><li>链路预测：推荐系统（users-items）、多种药物一起的副作用预测</li><li>图/子图分类：交通流量预测、药物发现</li><li>聚类</li><li>生成图：新分子发现</li><li>图进化：物理仿真</li></ul><p>选择节点和连接，构成何种图模型这一步非常重要。</p><p>图模型基础概念：</p><ul><li>有向、无向图  -&gt; 度、节点平均度（2E/N）</li><li>Bipartite Graph（二部图）-&gt; Folded network（映射图）</li><li>邻接矩阵（Adjacency Matrix），现实网络的邻接矩阵通常非常稀疏</li><li>边缘列表（Edge list），在深度学习工程实现时十分常用</li><li>邻接表（Adjacency list）</li><li>其它可用属性：边权重、排名、类型、标签以及其它与场景契合的属性等</li><li>自环（self-loops）</li><li>多图（multigraph）：一对节点间有多个边</li><li>连通性、强连接（有向图中每对节点可以相互访问，SCCs，Strongly connected components，强连接部分）、弱连接</li></ul><h2 id="经典图机器学习方法"><a class="markdownIt-Anchor" href="#经典图机器学习方法"></a> 经典图机器学习方法</h2><p>经典机器学习方法重点在于<strong>提取有效特征</strong>（hand-designed features）。</p><h3 id="一-特征提取"><a class="markdownIt-Anchor" href="#一-特征提取"></a> 一. 特征提取</h3><h4 id="1-node-features"><a class="markdownIt-Anchor" href="#1-node-features"></a> 1. Node features</h4><p><strong>节点度（node degree）</strong>[importance, structure]，相同度数的节点无法区分</p><p><strong>节点中心度（node centrality）</strong>[importance]，考虑了图中不同节点的重要程度，包括engienvector centrality，betweenness centrality, closeness centrality等。<strong>engienvector centrality</strong>，某节点的重要程度是其邻居节点重要程度的归一化和（递归计算）；<strong>betweenness centrality</strong>，存在于其它节点对间最短路径上的节点更重要；<strong>closeness centrality</strong>，与其它各节点间最短路径长度和越短的节点越重要。</p><div align="center">  <img src="/2021/05/05/cs224w/ec.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/05/cs224w/bc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/05/cs224w/cc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>聚类系数（clustering coefficient）</strong>[structure]，节点附近的局部结构，衡量某节点的邻居节点间的联通程度如何。基本上是在计算自网络（ego-network）中的<strong>三角形</strong>个数。</p><div align="center">  <img src="/2021/05/05/cs224w/coefficient.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>GDV(graphlet degree vector)</strong> [structure]，将上述三角形概念扩大，Rooted connected non-isomorphic subgraphs。以下概念中节点的位置也很重要。GDV即该节点在某种graphlet出现的次数。</p><div align="center">  <img src="/2021/05/05/cs224w/graphlets.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/05/cs224w/gdv.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-link-prediction-features"><a class="markdownIt-Anchor" href="#2-link-prediction-features"></a> 2. Link Prediction features</h4><p>链路预测任务，比如随机丢失了当前图模型中的某些连接信息，或需要预测下一时间窗口中的节点连接信息。</p><p><strong>Distance-based features</strong>，如两节点之间的最短距离</p><p><strong>Local neighborhood overlap</strong>，两节点间的共有邻居数，Jaccard系数、Adamic-Adar index等</p><p><strong>Global Neighborhood Overlap</strong>，katz index计算一对节点间的全部路径数目（使用邻接矩阵A计算）；邻接矩阵的N次幂表示了每对节点间长度为N的路径的数量。</p><div align="center">  <img src="/2021/05/05/cs224w/katz.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="3-graph-level-features"><a class="markdownIt-Anchor" href="#3-graph-level-features"></a> 3. Graph-level features</h4><p>核方法（kernel），核心是设计一个kernel而不是使用特征向量。两个图模型之间的Kernel衡量的是图数据之间的相似度。内核矩阵是测量每对数据点之间的相似度，它一定是正定的，即只有正数特征值。</p><p>当前存在很多Graph kernels，比如graphlet kernel，Weisfeiler-Lehman Kernel, Random-walk kernel，shortest-path graph kernel等等。</p><p>Graph kernel背后的思想是给图模型做词袋（Bag-of-words），将图中的节点视为词，比如Bag of node degrees。而graphlet kernel，Weisfeiler-Lehman Kernel是这类方法的延伸，即使用了比节点度更复杂一些的表示方法。</p><p>Graphlet kernel使用不同graphlet数目，这边的graphlet定义与node-level features那一节中的有所不同。首先这类graphlet中允许存在孤立节点，而且它们不是rooted。注意，计算两个图模型相似性的时候可以把向量f归一化一下。这类方法的问题在于graphlets计数非常困难，类似的subgraph isomorphism test问题是NP难的。</p><div align="center">  <img src="/2021/05/05/cs224w/graphlet.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>Weisefeiler-Lehman Kernel</strong>旨在缓解这个问题，使用<strong>color refinement</strong>。其中确定属性使用到了哈希函数。这个方法计算效率较高，是线性于两个图的边数目的。Weisefeiler-Lehman Kernel是衡量图相似度的一种非常有效的方式（很难被击败），也是与GNN是息息相关的。</p><h2 id="节点表征"><a class="markdownIt-Anchor" href="#节点表征"></a> 节点表征</h2><p>重点包括三部分内容：1）编解码器框架，encoder是一个简单的表征向量查询，decoder是基于表征向量计算与定义的节点相似度匹配程度；2）节点相似度衡量方法：基于随机游走；3）图级别表征方法，可以直接将节点表征向量聚合或者利用匿名游走。</p><h3 id="一-encoder-decoder-框架"><a class="markdownIt-Anchor" href="#一-encoder-decoder-框架"></a> 一. Encoder + Decoder 框架</h3><p>相较于传统的图学习方法，图表征学习省去了特征工程的步骤，直接自动学习图特征，之后应用于不同下游任务。</p><p>Efficient task-independent feature learning for machine learning with graphs. Encode nodes so that similarity in the embedding space (e.g., dot product) approximates similarity in the graph.</p><p>重点在于定义：1）什么是图上的节点相似性；2）节点到向量的映射函数。</p><p>一个最简单的encoder（shallow encoder）就是对于embeddings的查询。我们可以去直接优化每个节点的表征向量。但如果是大图的话，这个方法会很慢，因为嵌入矩阵Z会有很多列，要针对每一个进行表征向量估计。</p><div align="center">  <img src="/2021/05/05/cs224w/embedding.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>如何衡量图中节点相似度？这也是不同算法工作中比较大的差异点。大多流行方法会使用<strong>随机游走</strong>。</p><p>节点表征方法的特点：</p><ul><li>这类方法属于无监督或自监督学习，并没有利用到标签信息</li><li>没有利用节点属性，方法的目标是表征向量体现出图结构信息</li><li>表征结果是独立于下游任务的</li></ul><h3 id="二-基于random-walk的节点表征方法"><a class="markdownIt-Anchor" href="#二-基于random-walk的节点表征方法"></a> 二. 基于Random Walk的节点表征方法</h3><p>关于图中节点间的相似度衡量定义为两节点同时出现在图中统一随机游走记录中的概率。</p><p>使用随机游走的优势在于：1）可以同时考虑局部和高阶邻居信息；2）只需要考虑在随机游走中同时出现的节点对，而不是考虑图上的全部节点对。</p><div align="center">  <img src="/2021/05/05/cs224w/rw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>一般会在某些游走策略R下使用short fixed-length随机游走，收集某节点的邻居节点集合（此集合为multiset，即允许重复，因为某些节点可以被访问多次），之后调整嵌入向量z（参数）去优化最大似然概率。下图使用了softmax方法。</p><div align="center">  <img src="/2021/05/05/cs224w/rwe.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>但直接进行这样的计算代价很大，复杂度是图中节点数的平方。我们同样使用<strong>负采样（negative sampling）<strong>进行优化，只在负样本集合上计算而非在全部数据上计算。那么</strong>如何选取负样本</strong>呢？采样概率依照每个不同节点的度设定，共采样K个。K值越高得到的模型越鲁棒，但同时对负样本的偏向也越高，实际中<strong>一般选取K=5-20</strong>。</p><p>最后使用梯度下降方法优化即可。</p><p>最后一个问题是<strong>如何设定随机游走策略</strong>？最简单的方法是直接进行fixed-length, unbiased random walks（DeepWalk使用）。而node2vec认为更灵活地定义邻居节点可以获取包含信息量更大的节点表征，所以提出了biased 2nd order随机游走来生成节点邻居集合，这个方法可以综合权衡局部（BFS）和全局（DFS）信息。</p><div align="center">  <img src="/2021/05/05/cs224w/node2vec.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>因此node2vec提出return参数p（返回到之前的节点）和in-out参数q（BFS和DFS的比例）。关键在于<strong>记录了上一节点信息</strong>，对于处于某个节点的随机游走，下一节点有三种选择：1）退回上一节点；2）去和上一节点距离相同的节点；3）去距离上一节点更远的节点。</p><div align="center">  <img src="/2021/05/05/cs224w/pq.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>Node2vec算法总结如下，算法为<strong>线性复杂度</strong>，三个步骤可以并行处理。</p><ul><li>计算随机游走概率</li><li>对每个节点u模拟r次长度为l的随机游走</li><li>使用随机梯度下降法优化目标函数</li></ul><p>还有很多<strong>其它经典算法</strong>应用了不同的随机游走策略、优化策略和一些数据预处理技巧。</p><div align="center">  <img src="/2021/05/05/cs224w/others.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>截至目前，我们学习了三类图中<strong>节点相似度度量</strong>的方法：</p><ul><li>Naive：有连接的节点相似；</li><li>第二节中的Neighborhood overlap，即两节点间共有邻居情况</li><li>基于随机游走的节点向量表征</li></ul><p><strong>Must choose definition of node similarity that matches your application!</strong></p><h3 id="三-图表征"><a class="markdownIt-Anchor" href="#三-图表征"></a> 三. 图表征</h3><p>图或子图级别（即多个节点）的向量表征。</p><p>方法一：最简单是沿用节点向量表征方法，之后直接加和或平均，作为整个图的表征向量，虽然简单但是实际效果还不错。也可以使用层次聚类的方法逐步计算。</p><p>方法二：引入一个虚拟节点“virtual node”代表整个图/子图。</p><div align="center">  <img src="/2021/05/05/cs224w/subgraph.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>方法三</strong>：使用<strong>匿名游走（anonymous walks）</strong>，名字由来在于最终结果与具体图中节点信息无关。随着随机游走长度的增长，匿名游走数量呈指数型增长。可以按不同长度L匿名游走下不同游走类型的数量/概率作为表征向量。</p><div align="center">  <img src="/2021/05/05/cs224w/aw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>如何确定我们需要的匿名游走的数量？</p><div align="center">  <img src="/2021/05/05/cs224w/count_aw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>以同时出现在时间窗口T中的匿名游走为样本进行训练。这也是和DeepWalk之间的一大差异，即没有用节点集合左右邻居域。</p><div align="center">  <img src="/2021/05/05/cs224w/awe.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/05/cs224w/awg.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="pagerank一些链路分析方法"><a class="markdownIt-Anchor" href="#pagerank一些链路分析方法"></a> PageRank（一些链路分析方法）</h2><p>从矩阵角度进行图数据分析，由此可以进行：1）基于随机游走衡量节点重要程度（PageRank）；2）通过矩阵分解（Matrix factorization，MF）获取节点向量表征；3）将其它节点向量表征视为MF。</p><p><strong>Random walk，matrix factorization and node embeddings are closely related!</strong></p><div align="center">  <img src="/2021/05/05/cs224w/summary.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>之前的假设是把互联网中的（静态）网页视为节点，超链接视为边，去<strong>衡量网页的重要性</strong>。[ 虽然目前随着互联网的发展有了很多<strong>动态页面</strong>以及一些<strong>无法访问的生成页面</strong>。之前的网站链接多为navigational，而如今的更多是transactional。]</p><h4 id="1-pagerank"><a class="markdownIt-Anchor" href="#1-pagerank"></a> 1. PageRank</h4><p>将网页链接视为投票，使用in-links权衡，但每个连接的重要程度又不同，从而形成一个递归问题。</p><div align="center">  <img src="/2021/05/05/cs224w/page.jpg" srcset="/img/loading.gif" width="20%" height="20%" alt="oauth"></div><p>计算使用<strong>列随机矩阵M</strong>，最终得到<strong>秩向量r</strong>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>r</mi><mo>=</mo><mi>M</mi><mo separator="true">⋅</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">r = M · r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span></span>。可以将r视作<strong>随机游走</strong>收敛到的平稳分布，或者<strong>视为M的特征值为1对应的特征向量</strong>，即主特征向量。它是随机游走方程、基于流的方程式 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo separator="true">⋅</mo><mi>r</mi><mo>=</mo><mi>M</mi><mo separator="true">⋅</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">1·r = M · r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span></span>和线性代数的特征向量、特征值的完美融合。</p><p>[ 这里与前文中介绍的节点中心度中的<strong>eignvector centrality</strong>（针对无向图）和<strong>Katz centrality</strong>有一些梦幻联动。]</p><div align="center">  <img src="/2021/05/05/cs224w/pc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>使用幂迭代（power iteration）方法求解r，一般来讲迭代50次会大致收敛。[ 下图中左右两侧的迭代公式意义相同 ]</p><div align="center">  <img src="/2021/05/05/cs224w/pi.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>还有<strong>两个问题</strong>需要关注：</p><p>第一，死胡同问题（dead ends）[数学问题]，有的页面时没有out-link的。解决方法是，给没有out-link的页面所在的M列<strong>赋均值</strong>。</p><p>第二，蜘蛛陷阱（spider traps）[非数学问题]，有些页面相互连接，最终吸收了所有的“重要性”。解决方法是，引入参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>，表示继续沿着当前link游走的概率，而有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">1-\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>的概率**随机传送（teleport）**到任意页面，<strong>一般取值在0.8-0.9之间</strong>。</p><p>所以，Google的做法如下，或者写成矩阵形式<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mo>=</mo><mi>β</mi><mi>M</mi><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo><mo stretchy="false">⌈</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><msub><mo stretchy="false">⌉</mo><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msub><mo separator="true">,</mo><mi>r</mi><mo>=</mo><mi>G</mi><mo>∗</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">G = \beta M + (1-\beta) \lceil \dfrac{1}{N} \rceil_{N \times N},  r = G * r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">G</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mopen">⌈</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose"><span class="mclose">⌉</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">G</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span></span></p><div align="center">  <img src="/2021/05/05/cs224w/google.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>我们虽然从随机游走的角度来理解算法，但运行过程中并不实际进行游走，而是假定它游走了无限长时间。</p><h4 id="2-personalized-pagerankppr-random-walk-with-restarts"><a class="markdownIt-Anchor" href="#2-personalized-pagerankppr-random-walk-with-restarts"></a> 2. Personalized PageRank（PPR）&amp; Random walk with restarts</h4><p>PPR：在传送的时候不像PageRank那样概率传送到图中每个节点而是只取<strong>一个节点子集S</strong>。这个子集由之前的随机游走记录而得，每个节点的概率由访问次数决定。</p><div align="center">  <img src="/2021/05/05/cs224w/ppr.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>Random walk with restarts：将传送节点子集S缩小至单个节点，即总是传送回起始节点。</p><p>优势在于这类方法考虑了：1）一对节点间多种连接；2）多条路径；3）连接是否有向；4）路径中节点的度。</p><h4 id="3-矩阵分解与节点表征的关系"><a class="markdownIt-Anchor" href="#3-矩阵分解与节点表征的关系"></a> 3. 矩阵分解与节点表征的关系</h4><p>以“存在边连接”定义节点相似度的内积形式的解码器与邻接矩阵A的矩阵分解等价。</p><p>DeepWalk、node2vec等方法具有更为复杂的节点相似度定义（基于随机游走），它们等同于形式更为复杂的矩阵的矩阵分解。</p><p>下图为DeepWalk对应的矩阵形式：</p><div align="center">  <img src="/2021/05/05/cs224w/DeepWalk.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="4方法局限性"><a class="markdownIt-Anchor" href="#4方法局限性"></a> 4.方法局限性</h4><p>这类基于矩阵分解/随机游走的节点表征方法，如DeepWalk、node2vec等（PageRank也可以视为一维嵌入）有以下几点局限：</p><ul><li><p>无法得到新加入（未在训练集中出现过）的节点的表征向量；</p></li><li><p>无法捕获结构相似性（structurally similar），比如下图中节点1和节点11会有很不同的表征。如果使用匿名随机游走也许有提升；</p></li><li><p>无法利用节点、边或整个图的特征信息</p></li></ul><div align="center">  <img src="/2021/05/05/cs224w/limit.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>针对以上这些缺陷的解决方法是：Deep Representation Learning和Graph Neural Networks。</p><h2 id="课程作业借鉴"><a class="markdownIt-Anchor" href="#课程作业借鉴"></a> 课程作业借鉴</h2><h4 id="1可视化函数"><a class="markdownIt-Anchor" href="#1可视化函数"></a> 1.可视化函数</h4><pre><code class="hljs python"><span class="hljs-comment"># Helper function for visualization.</span>%matplotlib inline<span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> networkx <span class="hljs-keyword">as</span> nx<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-comment"># Visualization function for NX graph or PyTorch tensor</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">visualize</span><span class="hljs-params">(h, color, epoch=None, loss=None)</span>:</span>    plt.figure(figsize=(<span class="hljs-number">7</span>,<span class="hljs-number">7</span>))    plt.xticks([])    plt.yticks([])    <span class="hljs-keyword">if</span> torch.is_tensor(h):        h = h.detach().cpu().numpy()        plt.scatter(h[:, <span class="hljs-number">0</span>], h[:, <span class="hljs-number">1</span>], s=<span class="hljs-number">140</span>, c=color, cmap=<span class="hljs-string">"Set2"</span>)        <span class="hljs-keyword">if</span> epoch <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> loss <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:            plt.xlabel(<span class="hljs-string">f'Epoch: <span class="hljs-subst">&#123;epoch&#125;</span>, Loss: <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">.4</span>f&#125;</span>'</span>, fontsize=<span class="hljs-number">16</span>)    <span class="hljs-keyword">else</span>:        nx.draw_networkx(G, pos=nx.spring_layout(G, seed=<span class="hljs-number">42</span>), with_labels=<span class="hljs-literal">False</span>,                         node_color=color, cmap=<span class="hljs-string">"Set2"</span>)    plt.show()</code></pre>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络的局限</title>
    <link href="/2020/12/22/limit-graph/"/>
    <url>/2020/12/22/limit-graph/</url>
    
    <content type="html"><![CDATA[<p>后面计划进行有关图模型攻击方面的研究，学习斯坦福<a href="http://web.stanford.edu/class/cs224w/" target="_blank" rel="noopener">CS224W《图机器学习》</a>a&gt;和大佬Stephan Günnemann教授<a href="https://www.in.tum.de/daml/teaching/mlgs/ " target="_blank" rel="noopener">MLGS课程</a>中“Limitations of GNN”部分，记录如下。</p><p>关键点：</p><ul><li>图同构判断问题：单射，max/mean/sum pooling，WL Test</li><li>对抗攻击：Nettack，离散数据（无法直接梯度下降优化）、双层优化问题、如何对抗（certification）</li><li>Robutness and certification部分</li></ul><h2 id="mlgs"><a class="markdownIt-Anchor" href="#mlgs"></a> MLGS</h2><div align="center">  <img src="/2020/12/22/limit-graph/summary.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="一-表达能力"><a class="markdownIt-Anchor" href="#一-表达能力"></a> 一. 表达能力</h3><h4 id="1-图同构问题"><a class="markdownIt-Anchor" href="#1-图同构问题"></a> 1. 图同构问题</h4><p>如何判断两个图是否在结构上相同？此问题最优解最差时间复杂度呈指数形式。</p><p><strong>WL test</strong>（Weisfeiler-Lehman Test），只能得出“两个图同构或可能同构”的结论。</p><div align="center">  <img src="/2020/12/22/limit-graph/wl.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>在这个问题上GNN无法做到比WL test更好，尤其是它使用了非单射的聚合操作的时候更是无法区分图同构问题。</p><div align="center">  <img src="/2020/12/22/limit-graph/increase.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2-过平滑问题"><a class="markdownIt-Anchor" href="#2-过平滑问题"></a> 2. 过平滑问题</h4><p>随着层数增加GNN的预测结果过于平滑。无穷多层的GNN会导致所有的</p><p>节点得到同样的表征向量，这个向量表达了整个图的结构信息（和PageRank类似）而无法区分局部信息。</p><div align="center">  <img src="/2020/12/22/limit-graph/limit.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>关注一下<strong>PageRank</strong>。在PageRank里我们使用teleport vector进行信息局部化（关注邻居），同理可以应用到GCN场景中，相关工作为<strong>PPNP</strong>（Personalized Propagation of Neural Predictions，2018，建议阅读原文）。将转换与传播操作分开，并加入personalized teleportation，最终将迭代公式修改为：</p><div align="center">  <img src="/2020/12/22/limit-graph/shizi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>PPNP在防止过平滑、计算效率、扩展性等方面有如下优势：</p><div align="center">  <img src="/2020/12/22/limit-graph/ppnp.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-鲁棒性"><a class="markdownIt-Anchor" href="#二-鲁棒性"></a> 二. 鲁棒性</h3><p>有关图数据的对抗可以发生在<strong>节点属性</strong>和图<strong>结构信息</strong>两方面（后者在现实世界中更普遍），进行针对某些节点的<strong>有目标攻击</strong>或进行针对整个图的<strong>全局攻击</strong>。</p><p>图对抗攻击的<strong>挑战</strong>：</p><ul><li>针对离散变量的优化问题；通过非凸的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">L_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>范数计算扰动；</li><li>样本节点间相互关联，不可以单独计算；</li><li>如何定义“难以察觉”的扰动？</li><li>现实中多抽象为投毒攻击（影响训练数据集），抽象为一个双层优化问题。</li></ul><div align="center">  <img src="/2020/12/22/limit-graph/gongshi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div>最早的攻击为Nettack‘2018，目标是影响single node's prediction。关键操作在于首先将分类器线性化（为简化模型去掉了激活函数ReLU），之后通过贪心算法迭代找到最优扰动。<p>如何提升鲁棒性：</p><p>1）启发式防御方法：adjacency low-rank approximaition via truncated Singular Value Decomposition （Entezari 2020）; filtering of malicious edges via attribute similarity（Wu 2019）等，但这些方法在CNN领域已被证明无法应对最差情况的扰动。</p><p>2）鲁棒的训练方法，如 via Projected Gradient Descent（Xu et al，2019，但目前这种通过生成其它图样本的方法效果不是特别好）或者propose with a certification technique（low up bound，这个方面教授发表了很多论文）</p><ul><li>《Certifiable Robustness and Robust Training for Graph Convolutional Networks》</li><li>《[Certifiable robustness of graph convolutional networks under structure perturbations](javascript:void(0))》</li><li>《Certifiable Robustness to Graph Perturbations》</li></ul><p>3）随机平滑（randomized smoothing），如何在离散的图结构信息上加入高斯噪声？将邻接矩阵上的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>个边视为伯努利随机变量，但由于实际中的网络大多比较稀疏，很难找到一个合适的概率参数p。所以我们需要进行sparsity-aware random sampling。（这部分需要更详细得看一下）《Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more》‘ICML 2020</p><p>这方面的问题依然大有可为！（GNN robustness/certification is a highly active research area）</p><h3 id="三-扩展性"><a class="markdownIt-Anchor" href="#三-扩展性"></a> 三. 扩展性</h3><p>消息传递机制下需要同时处理整个网络，节点数据非独立同分布，动态增删节点/边会造成较大影响。</p><h2 id="cs-224w"><a class="markdownIt-Anchor" href="#cs-224w"></a> CS 224W</h2><h3 id="一-capture-graph-structure"><a class="markdownIt-Anchor" href="#一-capture-graph-structure"></a> 一. Capture graph structure</h3><p>Graph Isomorphism（图同构问题），邻居节点聚合函数（mean，max）并不单射。提出GIN（Graph Isomorphism Network），使用sum pooling。GIN可以更好地把握图结构信息，对于图分类问题表现更优秀，尤其是当网络中没有节点属性信息时。</p><p>GIN的思想与WL测试法近似。WL可以解决实际中的绝大多数图同构判断问题，但有一些例外，比如下面的例子：</p><div align="center">  <img src="/2020/12/22/limit-graph/except.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-vulnerability-of-gnns-to-noise-in-graph-data"><a class="markdownIt-Anchor" href="#二-vulnerability-of-gnns-to-noise-in-graph-data"></a> 二. Vulnerability of GNNs to noise in graph data</h3><p>以图上半监督节点分类问题为例，重点介绍了KDD18上Stephan Günnemann的工作，第一次提出该问题的数学模型并解答。解如下优化问题有两个难点：1）离散数据难以使用梯度下降；2）该问题为双层优化问题，如果使用迭代求解，每一步重新训练GNN非常耗时。作者为了保证高效，使用了很多启发式近似方法，比如贪心地一步步进行图修改，删除GCN中的ReLU激活函数进行简化等。（更多细节可以直接看论文，Adversarial Attacks on Neural Networks for Graph Data，PPT也做的很赞）。</p><div align="center">  <img src="/2020/12/22/limit-graph/attack.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><div align="center">  <img src="/2020/12/22/limit-graph/math.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><h3 id="三-challenges-and-future"><a class="markdownIt-Anchor" href="#三-challenges-and-future"></a> 三. Challenges and Future</h3><p>带标签数据集不容易获得（这是整个ML领域的问题），数据集不足又比较容易出现过拟合问题。为解决这个问题，提出Pre-training GNNs [Hu+ 2019]，先在某些相关数据集上训练之后，遇到真实任务再进行finetune。</p><p>如何防御上述类型对抗攻击？</p><p>攻击过程中如何在离散数据上找到最优解？</p><p>如何在准确性和鲁棒性之间找到最佳平衡？</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大规模图数据分析技术：挑战与机遇</title>
    <link href="/2020/11/16/talk-1/"/>
    <url>/2020/11/16/talk-1/</url>
    
    <content type="html"><![CDATA[<p>2020年11月16日，FIT楼多功能厅，听取樊文飞教授报告。以下为报告笔记，因为背景知识有限，听取现场报告，可能会有一些遗漏或理解错误之处。</p><p>当前图数据已成为大数据分析场景下的重要数据来源，老师在报告中就<strong>4V问题</strong>（Volume，Variety，Velocity，Veracity/value），分析图数据存储、分析中遇到的问题与部分解决思路。</p><h4 id="1volume体量"><a class="markdownIt-Anchor" href="#1volume体量"></a> 1.Volume（体量）</h4><p>大型图网络中包含百亿节点，百亿条边，如何存储处理？</p><p>DFS虽然是线性复杂度，但在大图上已经难以运行。</p><p>使用并行计算系统完成大图处理，在工业界存在很多问题：</p><p>1）已知的图算法能否并行化，学术界有很多这样的尝试比如google的pregel，CMU的Graphlab/PowerGraph，Facebook的Giraph，Berkeley的GraphX，IBM的Giraph++，但工业界未使用。樊老师团队有此类工作发表在SIGMOD，同时项目被阿里收购，今年开源为GraphScope。</p><div align="center">  <img src="/2020/11/16/talk-1/graphscope.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>2）并行计算中如何选择同步或异步，樊老师团队提出AAP工作，《Adaptive Asynchronous Parallelization of Graph Algorithms》</p><p>2）如何衡量某个并行算法是有效的？</p><p>3）传统计算复杂性理论在并行计算环境下有什么样的新表现？</p><h4 id="2velocity动态"><a class="markdownIt-Anchor" href="#2velocity动态"></a> 2.Velocity（动态）</h4><p>实际中的网络动态变化，但变化部分相对原网络占比较小，如何高效处理变化的信息？</p><p>计算output的变化。</p><p>1）很多被证明不是bounded的问题仍要解决，樊老师团队提出relative bounded，《Bounded incremental graph computations: Undoable and doable》。</p><p>2）如何提出增量式的算法，《Incrementalization of graph partitioning algorithms》VLDB 2020。</p><h4 id="3-variety异构"><a class="markdownIt-Anchor" href="#3-variety异构"></a> 3. Variety（异构）</h4><p>如何达成图数据库与原有关系型数据库的统一？比如阿里提到的数据中台概念。但这一方向距离落地还有很多工作要做。樊老师提出gSQL概念，以及联邦数据库（这里与联邦学习的概念不同，强调的是多类型数据的联邦）。</p><div align="center">  <img src="/2020/11/16/talk-1/gsql.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="4-veracity质量"><a class="markdownIt-Anchor" href="#4-veracity质量"></a> 4. Veracity（质量）</h4><p>存在过时数据、链接丢失与语义不一致等问题，如何解决？这个问题是4V中解决度最低的。</p><p>樊老师认为可以将logic规则与AI统一在一个框架下，同时还可以提高ML的可解释性。</p><div align="center">  <img src="/2020/11/16/talk-1/ml-1.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/11/16/talk-1/ml-2.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/11/16/talk-1/ml-3.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3><p>大数据场景下，图数据分析在4V中存在如下挑战：</p><ul><li>Volume，并行处理——reduction，complete problems等</li><li>Velocity，增量计算理论与实践</li><li>Variety，在SQL上做到图数据库与关系型数据库的统一</li><li>Veracity，统一规则（logic）与ML；图数据清洗；链路预测等</li></ul><h3 id="相关文献"><a class="markdownIt-Anchor" href="#相关文献"></a> 相关文献</h3><ol><li>Application driven graph partitions. SIGMOD 2020</li><li>Capturing associations in graphs VLDB 2020</li><li>Incrementalization of graph partitioning algorithms. VLDB 2020</li><li>Graph algorithms: Parallelization ans scalability. Science China Information Sciences, 2020</li><li>Adaptive asynchronous paralleization of graph algorithms, TODS2020</li><li>Deducing certain fixes to graphs, VLDB 2020</li><li>Parallelizing sequential graph computations TODS 2018</li><li>Incremental graph compulations: doable and undoable SIGMOD 2017</li></ol><h3 id="思考"><a class="markdownIt-Anchor" href="#思考"></a> 思考</h3><p>1）有关数据质量的部分，老师提到将规则与AI纳入统一框架，这个让我联想到网络安全领域目前基于规则和基于AI的方法，是否可以进行结合？比如Yara规则与CFG恶意软件检测等。这部分可以搜一下相关资料，樊老师并没有更详细地分享。</p><p>2）<strong>理论与系统</strong>是计算机专业领域的核心，要结合考虑理论研究的应用落实，同时学会如何在项目开发中发现待解决问题。</p><p>3）科研技术与产业落地之间还会有比如，政策、安全性、市场等多方面问题，需要综合考虑。比如XML并没有实现统一，比如在当前已有标准的情况下，为什么不能把关系型数据库全部转化为图数据存储。</p>]]></content>
    
    
    <categories>
      
      <category>交流报告</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
      <tag>大数据分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【小故事】无法消散</title>
    <link href="/2020/07/07/story-1/"/>
    <url>/2020/07/07/story-1/</url>
    
    <content type="html"><![CDATA[<p>下午四点半，9号楼的小洛死了。</p><p>一个女孩就这么从楼上摔下去，“肝脑涂地”，围观的人们在惋惜中带着阵阵恶心。</p><p>警队来了，120无聊地待在一边。</p><p>小洛家在十一楼，电梯坏了，郝队爬得气喘吁吁，“大夏天的，整这出？”</p><p>小洛和父母同住，郝队进门，家里一尘不染，物件规整，门口立着扫帚，卫生间里的拖布还没晾干。阳台的花鸟，墙上的字画，尤其沙发、床头随处可见的书籍，显示出这家人对文化生活的习惯，不是那种张扬在外的追捧。</p><p>“没有打斗的痕迹，看来是失足坠楼。”</p><p>小洛摔下去的地方正对着家里小阳台的窗户，窗页大敞着，郝队看了看，有齐腰高，如果不是故意爬上去，应该没可能发生意外。奇怪的是，周围窗台也一尘不染，连在附近活动过的痕迹都没有。如果是失足坠落，这窗户就显得过分冷静了。</p><p>“真会给我出难题。”</p><p>在屋内转了一圈，没获得什么实质性线索，郝队转过头来询问家人亲友。据了解，小洛是个成绩不错的学生，正在读博士，平时安安静静，不太可能涉及校园贷、勒索威胁等杂七杂八的事情。小洛有近期和同学出游的计划，前一天洛爸还听见她在电话上兴奋地讨论行程安排。听同学说，小洛最近有一篇论文在写，她还报名了半个月之后的一场线上比赛。哦，还有在小区附近的美容院里预约了两天后的祛痘清洁服务。</p><p>“你家孩子近期遇到什么事情了么？”，  “没有啊，疫情在家，每天平平淡淡的，哪有什么大事。” 洛妈已经哭晕了，都是洛爸在撑着回答。</p><p>“孩子人际交往怎么样？”</p><p>“性格有些内向，打小害羞，爱自己一个人玩，但长大就好很多，上学也结识了几个挺不错的朋友，这几天晚上还经常一起打游戏聊天呢。”</p><p>“那她平时生活状态怎么样？我看她是博士生，是不是课业压力挺大的？”</p><p>“刚读博的时候确实是，她老觉得毕业没希望，打电话回家也都挺沮丧的，后来发了两篇论文，就好很多了。我孩子不可能自杀，她最近也作息规律，偶尔锻炼，跟我俩聊天，都很好的，警官您可得仔细给查查啊，我们都配合，都配合。”</p><p>“不像啊”，郝队心想，“这感觉过得挺好。” 忽然，郝队看到垃圾桶里有个弯了的勺子，是平时做饭用的不锈钢厨具。要不是强外力，勺子不可能拧成这样。刚刚在书房里发现的塑料碎片，应该就是这勺子把手上掉下来的。郝队一阵激动，“这案子应该另有隐情。”</p><p>其实，这就是一场简单的自杀。</p><p>小洛是个理解力远超表达力几个维度的人，这样的人是孤独的，她在期盼与失落中交替，觉得没劲，就走了。</p><p>临走的几个小时前，又一次失落后，小洛很愤怒，刷碗时猛地把勺子砸向地板。看着弯折的勺子和四散的勺柄，她觉得挺好笑的。小洛有很多年，或者甚至说是从小，就不会生气，她好像总能站过去理解对面的逻辑，然后承认现实。但承认现实，安慰不了自己。因为无法被理解，所以时常失落，又因为能理解，所以她的失落没有焦点。</p><p>不如算了吧，通过模仿别人而产生的烟火气，总也不能落地，搞得大家都麻烦。后来她想到，家里人都爱干净，就一一收拾好，还彻底打扫了卫生。以往这种时候，洛妈回来都会表扬几句，小洛听着，觉得“你开心就好”。</p><p>窗台上，小洛一边擦去最后的痕迹，一边记起初中时，在思想政治书上背过，“热爱生活，珍惜生命，回报父母，贡献社会”。真没办法，对不起当时考出的98分。下坠时，她记起，之前和同学讨论起生命的意义、自杀等问题，同学说，“死不死得无所谓，走之前可以把角膜啥的这些器官，捐献给那些想活着的人”。</p><p>我走了，就任你们处置了，最好可以尽快消散掉。不过，好像没大可能，而且又要为当代青年抹黑了，真是不好意思。</p><p>“现在这些年轻人啊，生活条件那么好了，蜜罐里泡大，心理素质就是差。”</p>]]></content>
    
    
    <categories>
      
      <category>人文艺术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>瞎写</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>这个博客的搭建过程</title>
    <link href="/2020/06/01/build-blog/"/>
    <url>/2020/06/01/build-blog/</url>
    
    <content type="html"><![CDATA[<p>本文重点介绍基于Hexo+Github搭建个人网站流程，最初基本源自<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog" target="_blank" rel="noopener">这篇文章</a>（如何使用Github从零开始搭建一个博客），作者把步骤已经介绍得非常详细完善了。我只是补充踩到的一些坑….</p><p>后来，我背弃了极简审美，开始使用<strong>Fluid</strong>主题。</p><h3 id="一-基础搭建"><a class="markdownIt-Anchor" href="#一-基础搭建"></a> 一. 基础搭建</h3><p>搭建过程使用的两个最基本、最重要的东西是Hexo和Github，其中前者是一个轻量级博客框架，支持将Markdown编写的文章直接编译为静态网页文件并发布，省去了数据库问题。Github则用来解决域名问题，其Github Pages允许每个用户创建一个名为{username}.github.io的仓库，发布博客网页。当然也可以自己申请域名，使用CNAME跳转。</p><h4 id="1-github创建仓库"><a class="markdownIt-Anchor" href="#1-github创建仓库"></a> 1. Github创建仓库</h4><p>在Github上创建一个名为{username}.github.io的仓库，注意必须是github.io结尾。比如我的github账户为“DeepDeer”，创建仓库为“<a href="http://deepdeer.github.io" target="_blank" rel="noopener">deepdeer.github.io</a>”。另外，申请对应仓库时不要弄成private的，否则开放博客Github要收费哈。</p><h4 id="2-安装环境"><a class="markdownIt-Anchor" href="#2-安装环境"></a> 2. 安装环境</h4><p>首先在自己电脑上安装Node.js，确保环境变量配置好，可以使用npm命令；</p><p>其次使用npm命令安装Hexo，安装后确保可以使用<code>hexo</code>命令。</p><pre><code class="hljs bash">npm install -g hexo-cli</code></pre><h4 id="3-初始化项目"><a class="markdownIt-Anchor" href="#3-初始化项目"></a> 3. 初始化项目</h4><p>选定存储博客文件的位置，在此文件夹中使用如下命令创建项目及对应文件夹：</p><pre><code class="hljs bash">hexo init &#123;name&#125;</code></pre><p>命令下产生的文件夹包括themes、source等文件夹，调用如下命令，则在public文件夹中生成js、css、font等内容。</p><pre><code class="hljs verilog">hexo <span class="hljs-keyword">generate</span></code></pre><p>使用server命令在本地运行博客，可以看到类似结果：</p><pre><code class="hljs routeros">hexo<span class="hljs-built_in"> server </span> #或简写为 hexo s</code></pre><div align="center">  <img src="/2020/06/01/build-blog/hello.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/hexo.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><h4 id="4-部署至github"><a class="markdownIt-Anchor" href="#4-部署至github"></a> 4. 部署至Github</h4><p>安装一个支持Git的部署插件</p><pre><code class="hljs sql">npm <span class="hljs-keyword">install</span> hexo-deployer-git <span class="hljs-comment">--save</span></code></pre><p>修改Hexo的配置文件_config.yml，找到Deployment部分，修改为如下内容：</p><pre><code class="hljs bash"><span class="hljs-comment"># Deployment</span><span class="hljs-comment">## Docs: https://hexo.io/docs/deployment.html</span>deploy:  <span class="hljs-built_in">type</span>: git  repo: git@github.com:DeepDeer/deepdeer.github.io <span class="hljs-comment">#你自己的Github仓库地址</span>  branch: master</code></pre><p>使用deploy命令部署后，可通过域名deepdeer.github.io访问，Github上传代码如下：</p><pre><code class="hljs ebnf"><span class="hljs-attribute">hexo deploy</span></code></pre><div align="center">  <img src="/2020/06/01/build-blog/github.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p><a href="http://xn--deploy-hp7ik1vdf32vsxrqigxw5bw73eklg.sh" target="_blank" rel="noopener">可编写如下内容脚本deploy.sh</a>，此后每当有内容更新时，<code>.deploy.sh</code>运行脚本即可。</p><pre><code class="hljs bash">hexo cleanhexo generatehexo deploy</code></pre><h3 id="二-加入主题"><a class="markdownIt-Anchor" href="#二-加入主题"></a> 二.  加入主题</h3><p>目前我加入的是Fluid主题（因为看上了颜值），之前用过一段时间Next主题，也很推荐。</p><h4 id="1-next主题"><a class="markdownIt-Anchor" href="#1-next主题"></a> 1. Next主题</h4><p>有关Next主题的配置及各种插件，在<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog" target="_blank" rel="noopener">这篇文章</a>中介绍地非常详细，这里只补充有关1）添加Gitalk插件和2）修改字体部分。</p><h5 id="1-gitalk插件"><a class="markdownIt-Anchor" href="#1-gitalk插件"></a> 1&gt; Gitalk插件</h5><p>申请Gitalk就在Github个人账户的settings——&gt; Developer settings ——&gt; OAuth Apps，点击 New OAuth App，出现申请界面。其中应用名称随便写就行，Hompage URL和Authorization callback URL写博客链接。如果有自己的域名可以更改Authorization callback URL。点击注册，生成Client ID和Client Secret。</p><p>注意，如果自己配置了域名，这个callback URL要改成自定义域名</p><div align="center">  <img src="/2020/06/01/build-blog/oauth.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>在配置了_config.yml文件后，第一次进入界面会出现下图效果。如果点击Github登录后跳转到了404界面，那么, 就说明配错了。我当时是在写_config.yml忘了把client_id, client_secret字段带的{ }去掉。这给我一顿google啊…</p><div align="center">  <img src="/2020/06/01/build-blog/begin.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>最后效果就像这个博客里一样，相关评论会显示在对应仓库的issues里，记得在仓库的settings里把features—&gt;issues勾选上（貌似默认就是开启的）</p><div align="center">  <img src="/2020/06/01/build-blog/comment.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/issues.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><h5 id="2-修改字体"><a class="markdownIt-Anchor" href="#2-修改字体"></a> 2&gt; 修改字体</h5><p>Next主题默认的博文正文字体大小有点大了，可以在配置文件里改一下。相关配置在hexo\themes\next\source\css\variables路径下的base.styl文件里的Font Size部分。这里面每个变量控制某一部分的字体大小，我是挨个试出来的font-size-large是正文字体（简单粗暴，真开心…)</p><div align="center">  <img src="/2020/06/01/build-blog/font.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>另外，在Hexo配置文件和Next主题配置文件中，都有一些有关网站信息的配置选项，最终使用Next主题搭建出的网站效果如下：</p><div align="center">  <img src="/2020/06/01/build-blog/next.jpg" srcset="/img/loading.gif" width="70%" height="50%" alt="oauth"></div><h4 id="2fluid主题"><a class="markdownIt-Anchor" href="#2fluid主题"></a> 2.Fluid主题</h4><p>其实这个主题有非常好的<a href="https://fluid-dev.github.io/hexo-fluid-docs/guide/" target="_blank" rel="noopener">配置指南</a>，其配置文件_config.yml的注释也很清晰，可以从头摸索。有几点经验包括1）图片插入；2）评论插件。</p><h5 id="1-图片插入"><a class="markdownIt-Anchor" href="#1-图片插入"></a> 1&gt; 图片插入</h5><p>需要注意的一点是，在Fluid主题下有文章背景图等存在于框架中的图片，这些图片一律存放在<code>./themes/fluid/source/img</code>文件夹下。即使是某个文章的缩略图也是这样。</p><div align="center">  <img src="/2020/06/01/build-blog/pic.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>其他存在于文章中的图片，可用如下形式添加。</p><p>首先，把_config.yml文件里的post_asset_folder选项设置为true。</p><p>其次安装一个插件，据说原有插件有一些bug，下面是修改过的插件，亲测有效，感谢<a href="https://www.jianshu.com/p/3db6a61d3782" target="_blank" rel="noopener">这篇博客</a></p><pre><code class="hljs vim">npm install http<span class="hljs-variable">s:</span>//github.<span class="hljs-keyword">com</span>/<span class="hljs-number">7</span>ym0n/hexo-asset-image --<span class="hljs-keyword">sa</span></code></pre><p>有了这些配置后，再运行hexo new xxx，在/source/_posts/路径下，除了可以生成新文章xxx.md之外，还生成一个同名文件夹。插入图片时放到这个文件夹里即可，在markdown里用如下语句：</p><pre><code class="hljs routeros">&lt;img <span class="hljs-attribute">src</span>=<span class="hljs-string">"xxx/图片名称.png"</span> <span class="hljs-attribute">alt</span>=<span class="hljs-string">"图片标识"</span> <span class="hljs-attribute">style</span>=<span class="hljs-string">"zoom:30%;"</span> /&gt;</code></pre><p>但是，在Fluid主题下，这些图片并没有默认居中，可以采用如下HTML代码控制位置和大小：</p><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">align</span>=<span class="hljs-string">center</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">img</span> <span class="hljs-attr">src</span>=<span class="hljs-string">"build-blog/pic.jpg"</span> <span class="hljs-attr">width</span>=<span class="hljs-string">"30%"</span> <span class="hljs-attr">height</span>=<span class="hljs-string">"30%"</span> <span class="hljs-attr">alt</span>=<span class="hljs-string">"oauth"</span>  /&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span></code></pre><h5 id="2评论插件"><a class="markdownIt-Anchor" href="#2评论插件"></a> 2&gt;评论插件</h5><p>Fluid推荐的utteranc.es插件，经常会有加载比较慢的问题，乍一看以为不让评论…</p><p>这个配置过程也很简单。</p><p>首先在github创建一个公开的仓库，比如命名为’deepdeer_comments’。</p><p>点击<a href="https://github.com/apps/utterances" target="_blank" rel="noopener">这个链接</a>安装应用，选择“only select repositories”选项，找到刚刚建立好的仓库，点击install。</p><p>在配置中填写repo名，格式为“用户名/仓库名”，如“DeepDeer/deepdeer_comments”。Issue的命名方式建议选择第一个“Issue title contains page pathname”。</p><p>根据个人喜好选择主题之后，最后一栏会自动生成配置信息，复制这些信息。</p><p>在fluid主题的配置文件中，找到<code>comments</code>部分，将enable设置为true，并将type写成utterances。</p><p>在后面的comments具体配置部分，改成之前自动生成的配置。</p><div align="center">  <img src="/2020/06/01/build-blog/utter.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p>这个博客目前用的是Gitalk插件，配置与Next主题中提到的大致相同。Fluid代码中该插件配置有问题，评论无法分页显示，感谢<a href="https://juejin.im/post/5ed177e36fb9a047923a39fe" target="_blank" rel="noopener">这篇文章</a>。即更改fluid主题下的<code>layout/_partial/comments/gitalk.ejs</code>文件内容中的’id’一栏部分</p><pre><code class="hljs python"><span class="hljs-comment">#原有的</span>id: <span class="hljs-string">'&lt;%- md5(theme.gitalk.id) %&gt;'</span>,<span class="hljs-comment">#改正后</span>id: &lt;%- theme.gitalk.id %&gt;,</code></pre><h3 id="三-自定义域名"><a class="markdownIt-Anchor" href="#三-自定义域名"></a> 三. 自定义域名</h3><p>本博客使用了阿里云上购买的域名。</p><p>在<a href="https://wanwang.aliyun.com/domain/searchresult/?keyword=skylasun&suffix=.cn#/?keyword=skylasun&suffix=cn" target="_blank" rel="noopener">这里</a>点击“控制台”，登录后，找到边栏中的“域名”。选择“域名注册”</p><div align="center">  <img src="/2020/06/01/build-blog/domain.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/reg.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p>进入后，查询你喜欢的关键字相关的域名的注册情况，选择中意的域名就可以交钱了。最终付款之前还需要一些身份认证。</p><div align="center">  <img src="/2020/06/01/build-blog/buy.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>购买成功，认证通过后，在已有域名那里，点击“解析”，添加解析规则。这里添加的IP地址是之前deepdeer.github.io的解析情况，可以通过各类IP或域名查询网站找到，比如<a href="https://site.ip138.com/" target="_blank" rel="noopener">这里</a>。注意下面要加上一条CNMA规则。</p><div align="center">  <img src="/2020/06/01/build-blog/map.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/ip.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>另外，在Github仓库的Settings里，需要加上“Custom domain”，保存配置后，会自动生成名为CNAME的文件，内容如下。但需要注意的是，每次我们重新部署时，使用deploy clean再generate后，会清除掉这个CNAME文件。为解决这个问题，可以把CNAME文件放到博客的“source”文件夹中。</p><div align="center">  <img src="/2020/06/01/build-blog/cname.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><h3 id="四-其它小经验"><a class="markdownIt-Anchor" href="#四-其它小经验"></a> 四. 其它小经验</h3><h4 id="1markdown编辑器推荐"><a class="markdownIt-Anchor" href="#1markdown编辑器推荐"></a> 1.Markdown编辑器推荐</h4><p>这些博客都是用<a href="https://www.typora.io/" target="_blank" rel="noopener">Typora</a>写的。该软件界面简洁，即时效果，很好用，推荐~</p><p>Typora除了支持公式块之外，还支持行内公式，在偏好设置中勾选”内联公式“即可。</p><div align="center">  <img src="/2020/06/01/build-blog/gongshi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2markdown中内容折叠"><a class="markdownIt-Anchor" href="#2markdown中内容折叠"></a> 2.Markdown中内容折叠</h4><p>有时文章内容过多不便于显示，可以使用如下语法进行折叠</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">details</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">summary</span>&gt;</span>可显示的标题<span class="hljs-tag">&lt;/<span class="hljs-name">summary</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">pre</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">code</span>&gt;</span>   折叠内容  <span class="hljs-tag">&lt;/<span class="hljs-name">code</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">pre</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">details</span>&gt;</span></code></pre><p>比如</p><div align="center">  <img src="/2020/06/01/build-blog/zhedie.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>效果如下，点击后显示黑色部分</p><div align="center">  <img src="/2020/06/01/build-blog/xiaoguo.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>如果有其它的坑，欢迎大家评论补充，谢谢！</p>]]></content>
    
    
    <categories>
      
      <category>教程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>工程技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>THU研究生国际会议出行准备流程</title>
    <link href="/2020/05/30/baoxiao/"/>
    <url>/2020/05/30/baoxiao/</url>
    
    <content type="html"><![CDATA[<p>下文仅限清华大学网络科学与网络空间研究院研究生同学使用，包含护照、签证、报销等</p><blockquote><p>提示：出国手续涉及部门较多，请尽早准备提前办理。如遇假期会有所调整，要关注邮件通知~</p></blockquote><h3 id="首先"><a class="markdownIt-Anchor" href="#首先"></a> 首先</h3><p>​你要有个<strong>护照</strong>，如果没有，办理的时候把发票留好，可以报销。</p><h3 id="前期准备"><a class="markdownIt-Anchor" href="#前期准备"></a> 前期准备</h3><p>注册会议并拿到<strong>邀请函</strong></p><p>预定<strong>机票和酒店</strong>（办签证使用）。目前携程等网站貌似不再支持付款前先打印行程单。</p><ol><li>机票时间，一般可定在会议安排往前往后各一天。有特殊情况，赶在自己文章汇报前到达即可。</li><li>酒店一般订会议推荐的，预定前注意一下学校给的当地住宿报销额度。有些会议官网会贴出提前订酒店有优惠的通知，发邮件过去即可。</li></ol><h3 id="学校审批"><a class="markdownIt-Anchor" href="#学校审批"></a> 学校审批</h3><h4 id="1申请出国批件"><a class="markdownIt-Anchor" href="#1申请出国批件"></a> 1&gt;申请出国批件</h4><ol><li>首先在info上进行申请，找“出国出境申报”——&gt;“因公出国（境）申报系统（新）</li></ol><div align="center">  <img src="/2020/05/30/baoxiao/apply.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><ol start="2"><li>进入系统后，选择”新申请”，点击“我已阅读”，在因公出境申请表上填写信息，其中会<strong>比较犹豫的几个字段</strong>有：</li></ol><ul><li>出访基本信息：出入境时间大概在会议日程往前往后各一天，离境、入境时间是否需要过境等如实填写即可</li><li>出访类别：单位公派，会议</li><li>出访经费：费用来源一般选择“全部校内支付”，“纵向科研经费”，校内支付，人民币（大致写一个费用即可）</li><li>日程计划：简单填写就行，比如出发，抵达，开会，回程等等（可适当扩展）</li></ul><ol start="3"><li><p>提交之后会有一个预算表，大概可以看到给当地的住宿、日常消费额度等。这类信息也可以在边栏中“预算、外汇与报销”的“政策与标准”的表格中看到。</p><div align="center">  <img src="/2020/05/30/baoxiao/biaozhun.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div></li><li><p>“报批材料”里上传会议的邀请函和论文录用证明。</p></li><li><p>点击提交，打印申请表，这个表需要自己和导师签字。</p></li><li><p>提交完成后，返回主界面会显示出当前进度，完成后圆圈会变绿。大概两周左右“单位审核”会变绿。等到“学校审批”通过，显示批件下达之后，可以下载电子版。</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/jindu.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div><h4 id="2批件领取"><a class="markdownIt-Anchor" href="#2批件领取"></a> 2&gt;批件领取</h4><p>去国际处，在李兆基4楼（可以进楼的门有点多，但失之毫厘谬以千里，所以可以问下保安…）</p><p>去之前先准备一份**“派出证明”<strong>。还是在刚刚的出入境申请系统的边栏里面。点击进去下载对应模板，注意老师们已经用最直接醒目的方法标示出的</strong>注意事项**。</p><div align="center">  <img src="/2020/05/30/baoxiao/chat.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div><div align="center">  <img src="/2020/05/30/baoxiao/chats.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><div align="center">  <img src="/2020/05/30/baoxiao/attention.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>在国际处主要有以下<strong>几个事情</strong>：</p><ul><li>拿批件</li><li>拿外汇预算单</li><li>派出证明需要盖章</li><li>如果办签证时需要单位法人证明一类的材料，需要<strong>主动</strong>和老师提及</li></ul><h3 id="签证办理"><a class="markdownIt-Anchor" href="#签证办理"></a> 签证办理</h3><p>这个就要看去哪个国家了，我以希腊为例，需要申根签，可以先在官网上填写表格申请，然后按照里面写的去依次准备材料。去使馆办事处。一定要注意时间，选最最最是工作时间的时段过去。我第一去的时候好像是下午3点左右到的，说是刚刚停止办理…</p><p>其它细节事项：</p><ol><li><p>保险可以直接在淘宝上买，搜“申根保险”就可以，看清楚额度是否符合要求；</p></li><li><p>户口页，如果户口在学校的话，直接在info上申请，”集体户口卡借阅”，里面包括“借阅预约”和“首页打印”。预约之后直接去地图里圈出的小房子（保卫处）里拿就好了；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/hukou.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="3"><li><p>银行对账单可以直接在C楼打印；</p></li><li><p>在读证明，在info上预约然后直接与三教打印（貌似改到了六教？，反正C楼应该都是万能的）；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/zaidu.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="5"><li>办理签证最后需要交纳现金。留好发票，这个在报销范围内。</li></ol><h3 id="出行及报销"><a class="markdownIt-Anchor" href="#出行及报销"></a> 出行及报销</h3><ol><li><p>行程中尽量保存好<strong>所有票据</strong>，回来整理<strong>报销</strong>。（我都是先垫付再报销，据说还可以先去学校<strong>借款</strong>）</p><p>各类车票，登机牌，行程单，机票购买记录及发票（让网站寄过来），酒店账单/发票，会议注册费发票等</p></li><li><p>去首都机场的话，清华科技园那里有大巴，车费是30块？这种貌似属于城建交通，也可以报销，不行的话，也有日常杂费可以cover掉。</p></li><li><p>回来后的报销主要是填写一个报销表格，还是在刚刚的出入境申报系统的边栏上的“表格下载”里，选择**”报销表格下载“**，表格如下图，里面也标明了一些报销流程和注意事项；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/baoxiaochat.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><div align="center">  <img src="/2020/05/30/baoxiao/items.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="4"><li><p>其它细节事项</p><ul><li>大额机票需要发票验真，通过官网或发票自带的网站都可以，截图打印</li><li>打印护照的出入境记录页</li><li>提供交易记录截图（微信通知，短信账单，订单等均可）</li><li>在发票上签字需要用<strong>油笔</strong></li></ul><p>我们组的报销可以去对门实验室请教<strong>乔老师</strong>，老师会给予很多帮助，在此表示感谢~</p></li></ol><p>本文凭借对半年前的回忆整理，如有疏漏，欢迎大家评论指正！</p>]]></content>
    
    
    <categories>
      
      <category>办公事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
