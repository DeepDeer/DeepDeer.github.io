<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>龙虾教授《人生十二法则》与《人格及其转变》</title>
    <link href="/2021/01/03/eclass-jp/"/>
    <url>/2021/01/03/eclass-jp/</url>
    
    <content type="html"><![CDATA[<p>最近集中学习了加拿大信息学教授乔治皮特森（jordan peterson）的相关课程与书籍，主要为书籍《人生十二法则》和课程《人格及其转变》，笔记整理如下。教授个人主页为，<a href="https://www.jordanbpeterson.com/%E7%9B%B8%E5%85%B3%E8%AF%BE%E7%A8%8B%E8%A7%86%E9%A2%91%E5%8F%AF%E4%BB%A5%E5%9C%A8B%E7%AB%99%E6%89%BE%E5%88%B0%EF%BC%8C%E6%84%9F%E8%B0%A2Up%E4%B8%BB@%E5%A4%A7%E5%BF%83%E8%84%8F%E6%8E%92%E6%8E%92%EF%BC%88https://www.bilibili.com/video/BV1AW411M7vL%EF%BC%89%E3%80%82" target="_blank" rel="noopener">https://www.jordanbpeterson.com/相关课程视频可以在B站找到，感谢Up主@大心脏排排（https://www.bilibili.com/video/BV1AW411M7vL）。</a></p><p>JP教授认为苦难就是人生的一部分，我们应当接受这一点，而后承担责任，从自身做起，尝试改善现状，真诚且真实地活着。这套理论乍一听起来和中国传统的“修身、齐家、治国、平天下”相似，甚至与一些“叫醒你的不是闹钟而是梦想”等鸡汤文字差不多。但教授分别从生物基础、进化、认知、宗教、哲学等不同层次、多种角度加以论述，知其然更知其所以然，使人信服，总之是狠狠激励了我一把。尤其，我在研究生阶段真切经历了混沌状态、虚无主义、极权主义等阶段，明白状态不佳却有深陷其中，偶尔悟出一些零零散散的道理，却又无力疏通。在忽明忽暗中学习到教授的课程，犹如被“点化”一般，哈哈哈。</p><p>另外，教授每节课程信息量极大，语速快且思维流畅，这是对相关知识了解透彻，真正相信并认可自己的研究内容才能够做到的，希望我以后可以成为大学老师，并且在承担教学任务的时候，也能拿出这样的态度与能力。</p><h2 id="人生十二法则"><a class="markdownIt-Anchor" href="#人生十二法则"></a> 《人生十二法则》</h2><p>这是教授各种主张的简化版本，B站有其演讲视频（<a href="https://www.bilibili.com/video/BV1Ki4y1t7YQ?from=search&amp;seid=5019313464000019345%EF%BC%89" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Ki4y1t7YQ?from=search&amp;seid=5019313464000019345）</a></p><h3 id="法则一笔直站立昂首挺胸"><a class="markdownIt-Anchor" href="#法则一笔直站立昂首挺胸"></a> 法则一：笔直站立，昂首挺胸</h3><p>这也是JP被称为龙虾教授的原因。</p><p>几年前就在TED上看过相关研究的视频，即我们的动作姿态如何影响了我们的心理态度，虽然后面貌似有爆出此科研项目作假的新闻（不知此条爆料是真是假）但就我个人经历来讲，这条是有效果的。健身一段时间，尤其是背部肌肉得到充分锻炼，肩膀和胸膛更加开阔后，整个人精气神好了很多，乐观且昂扬。</p><h3 id="法则二待己如助人"><a class="markdownIt-Anchor" href="#法则二待己如助人"></a> 法则二：待己如助人</h3><p>像照顾生病的宠物一样关心自己</p><h3 id="法则三与真心希望你好的人做朋友"><a class="markdownIt-Anchor" href="#法则三与真心希望你好的人做朋友"></a> 法则三：与真心希望你好的人做朋友</h3><h3 id="法则四和昨天的自己比别和今天的别人比"><a class="markdownIt-Anchor" href="#法则四和昨天的自己比别和今天的别人比"></a> 法则四：和昨天的自己比，别和今天的别人比</h3><h3 id="法则五别让孩子做出令你讨厌的事"><a class="markdownIt-Anchor" href="#法则五别让孩子做出令你讨厌的事"></a> 法则五：别让孩子做出令你讨厌的事</h3><h3 id="法则六批判世界前先清理你的房间"><a class="markdownIt-Anchor" href="#法则六批判世界前先清理你的房间"></a> 法则六：批判世界前先清理你的房间</h3><p>没有肩负起存在的重任会导致神经质的愧疚和恐惧。</p><h3 id="法则七追求意义拒绝苟且"><a class="markdownIt-Anchor" href="#法则七追求意义拒绝苟且"></a> 法则七：追求意义，拒绝苟且</h3><h3 id="法则八说真话或者至少别撒谎"><a class="markdownIt-Anchor" href="#法则八说真话或者至少别撒谎"></a> 法则八：说真话，或者至少别撒谎</h3><h3 id="法则九认真倾听"><a class="markdownIt-Anchor" href="#法则九认真倾听"></a> 法则九：认真倾听</h3><h3 id="法则十直面问题言辞精确"><a class="markdownIt-Anchor" href="#法则十直面问题言辞精确"></a> 法则十：直面问题，言辞精确</h3><h3 id="法则十一不要打扰玩滑板的孩子们"><a class="markdownIt-Anchor" href="#法则十一不要打扰玩滑板的孩子们"></a> 法则十一：不要打扰玩滑板的孩子们</h3><h3 id="法则十二关注存在的善"><a class="markdownIt-Anchor" href="#法则十二关注存在的善"></a> 法则十二：关注存在的善</h3><h2 id="人格及其转变"><a class="markdownIt-Anchor" href="#人格及其转变"></a> 《人格及其转变》</h2><p>将自己想象成人类灵魂工程师，同时塑造这自己和他人的人格，所以我们一同努力想要达成的是什么？？？</p><p>课程包括关于元叙述、大五人格、存在主义、现象学、虚无主义与极权主义、荣格、弗洛伊德、皮亚杰等心理学理论与人物的介绍。</p><h3 id="一-元叙述"><a class="markdownIt-Anchor" href="#一-元叙述"></a> 一. 元叙述</h3><p>古今中外广泛流传的故事中，大多可以抽取出一些共同点，一些内核，而这些部分组成了**“元故事”<strong>（the meta-story）。比如</strong>约拿和鲸鱼**的故事中，约拿因逃避责任而受到了神的惩罚，先是在航海中狂风大作，而后又被吞到了鲸鱼肚子里。约拿知道自己应道前去承担责任，正如每一个“拖延症”的我们。</p><div align="center">  <img src="/2021/01/03/eclass-jp/john.jpeg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>又比如<strong>彼得潘的故事</strong>告诉我们，如果你不在成熟的过程中（自然岁月中）使用潜能的话，潜能自己也会消耗，不要害怕丢失可能性而无所作为。</p><p><strong>英雄出走</strong>是非常经典的元故事之一，由此派生出很多故事，如勇士与恶龙、地下城、《哈利波特》、《霍比特人》、《狮子王》、漫威系列等等，这些故事中常常是拥有至善人格的人被施以残酷的惩罚。由乐园到失乐园，再到复乐园，这个过程大概率并不顺利，我们需要有意义的东西支撑下去，履行自己的使命。英雄传说的主旨就是，<strong>走出去探索未知的领域，直面未知带来的恐惧，获得宝物凯旋而归</strong>。</p><p>如果真的掉到“地下城”中，应当怎么办呢？首先要想一想，也许自己是一切不幸的源头，要审视自己的假设和行为方式，看清楚它们是如何阻拦着我，而我又该如何做出改变；进一步，你决定做该做的事，重新塑造自己，从而以一种更正确的方式生活，完成转化（transformation），<strong>凤凰重生</strong>。重要的是你要<strong>直面</strong>那些事实与恐惧，摊开分析，弄懂它们，然后彻底放下。如同《哈利波特与密室》的结尾，哈利必须要直面巨蛇，即使有可能被咬伤会死掉，直面也是最好的选择。<strong>那些可能让你动弹不得的事情，会让你不足的那部分死去，然后等待新生</strong>。当然这个过程会十分痛苦，因为原始的你的一部分需要被剔除。</p><div align="center">  <img src="/2021/01/03/eclass-jp/model.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>科学用来描述“世界是什么“，而神话、戏剧、梦境、潜意识以及其它美学的、艺术的、幻想方面的文学作品是在告诉我们事情应该是怎样的，告诉我们如何行动，传授为人处世之道。经过多少个世纪，提炼出至善、至恶，让我们理解，他们就像圣经故事中常出现的两兄弟，也在我们体内时常斗争。</p><p>此外，古往今来，人类各种活动中体验到的共同的情绪、形成的规则，也可以抽取出共通的部分，即**“元情绪”<strong>（唱歌、跳舞的时候你体会到了什么？）、</strong>“元规则”**（此处可以重读下《人类简史》）。另外还有已知与未知、理智与感性、秩序与自由、文化与自然、父亲与母亲、男性与女性，所有这些元素都是具有两面性等，而人们常常会忘掉其中一面。</p><p>人类还有很多<strong>共通点</strong>，比如自我意识与自我局限、致幻剂的体验、古老的关于世界的描述、天生的恐惧和脆弱（致力于一些有意义的事可以抵挡脆弱）、追求至善、追求复仇与毁灭、处于社群当中、以及如何照顾好自己。最后一点展开六个维度来讲，即每个人都需要有日常惯例（routine，比如固定的作息）、一份事业（生产你认为有意义的东西）、家庭、朋友、亲密关系、制定职业计划（学会如何谈判、表达需求、学会讲真话、学会倾听）等。另外，<strong>运动是最好的阻止智力下滑的方法</strong>。</p><p>人类社会发展中存在一些<strong>元现实（meta-reality）</strong>，比如等级优势、部落/社会之外的危险、他人制造的危险、自己给自己造成的威胁等。</p><h3 id="二-英雄与萨满启蒙"><a class="markdownIt-Anchor" href="#二-英雄与萨满启蒙"></a> 二. 英雄与萨满启蒙</h3><p>这部分讲述有关已知、未知以及二者如何结合。</p><p>各种文化、秩序、社会、父亲等其实都体现着已知，已知的不足之处在于暴君极权，不论是希特勒（狡辩）、斯大林（告密）还是一些神话人物，都有这方面的缺陷。</p><p>而我们常以黑暗之地、禁果、潘多拉魔盒、潜伏的怪兽、山谷、深海、月亮等表示未知，通常可以联想到女性、潜意识、带着酒神力量的本我、万物的来源于归宿等一些超越性的意向。西方文化中，蛇带给我们的感受，象征了我们对于未知威胁的态度。虽然带有恐惧，但我们天生喜欢探索未知。而探索的根源在于每次你学到人生的一课，学到的瞬间，你的世界都地动山摇。</p><p>科学抹去了所有的主观性，但那不是我们真实生活的世界，那只是“现实”。我们的世界中充满了动机和感情，充斥着恐惧、痛苦、挫败与喜悦，以及其它人，包含着一切已知和未知。面对已知与未知，我们应当将二者结合。想象一下<strong>道教的阴阳图</strong>，我们要尽力活在秩序和混沌的交界处，这里是元位置。作为一个人类，你本身也会希望在可预料当中有一些无法预料。而阴阳图也预示着，已知的部分可能在一瞬间变为未知。一生中可能会经历无数次这样的挫折，我们注定会经历混动（choas），甚至可能在某次混沌中一蹶不振，但我们要了解并相信，<strong>混沌中蕴含着向秩序的转变</strong>。</p><p>我们要主动探索未知，强化秩序，让自我人生模型不断重构。<strong>我们知道外面蕴含着巨大的危险</strong>，要<strong>主动挑战威胁</strong>，这样身体就会被激活，警惕萌生的威胁，及早行动，否则我们会进入猎物模式。当我们处于最佳边界，大脑会制造出一种全情投入、富含意义的感受，你足够稳定也足够有兴趣，达到了一种完美的平衡。我们一定不要逃避问题或者逃避个人责任，我们必须愿意牺牲掉安逸，去寻找问题的源头，解决它们并获取胜利果实。这是<strong>人类历史上经典的面对不确定性的应对方法</strong>。</p><p>有些人可以容忍大范围的混沌（<strong>自由派</strong>），但有些人更喜欢现存结构的稳定性（<strong>保守派</strong>），环境瞬息万变，没有永远的对错，这就需要双方的沟通交流。</p><p>在混沌与秩序之间，是意识、幻想、梦境等，创造力从这里出发。文化、艺术、幻想、音乐、戏剧、故事等，它们起到缓冲作用，<strong>我们被文化和幻想包围着也保卫着</strong>。我们应当尊重秩序，对明君心存感激，但同时也要<strong>足够聪明</strong>，知道他的另一面是邪恶暴君，这才是对世界的完整认知。</p><blockquote><p>迦利神像是毁灭与恐惧的具体符号，我们要做出牺牲，以今天的愉悦为筹码换取明日的优势，给”社会”想要的东西，与未来做交易，也对我们的自然母亲，做出正确的牺牲。</p><p>许多19、20世纪被普遍认为伟大的人，如尼采、达尔文、陀思妥耶夫斯基、托尔斯泰、弗洛伊德、荣格等，都经历过“创造类疾病”，即长期的、深刻的、心理上的不安与不确定。</p></blockquote><h3 id="三-皮亚杰构成主义"><a class="markdownIt-Anchor" href="#三-皮亚杰构成主义"></a> 三. 皮亚杰构成主义</h3><p>皮亚杰，是一位知识巨匠，杰出发展心理学家，其理论研究人们如何表现世界并学习，想要填补科学与价值之间的裂缝。</p><p>皮亚杰认为，<strong>世界不是所有等待发现的客观事实的合集</strong>，世界是更为复杂的。不随时间改变的才可以称为事实，比如人们产出事实的方式，而不是事实本身，再比如知识（世界观）获取与转换的方式，而不是知识本身。因此我们需要将知识视为工具，而不是客观独立的现实，一个糟糕的工具依然可以使用，而一个错误的事实则将完全扼杀你。</p><p>皮亚杰理论与萨满有共通之处，首先存在有序状态，之后意外发生，陷入混沌，原有体系瓦解，最后重组新生，达到一个更加完整的状态。而所谓<strong>终极现实，就是经历这些阶段的过程</strong>。这也是英雄传说的另一种表现形式。在学习的过程中，我们在死亡和生存之间保持着精妙的平衡，我们学习的过程（比如冲浪）真的改变生物构造，我们需要杀死部分已知（甚至有真实的某些生物结构）才能学到位置，虽然过程中会带来痛苦。</p><div align="center">  <img src="/2021/01/03/eclass-jp/model_pi.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>依据皮亚杰的发展心理学理论，人类分为<strong>程序记忆和表现记忆</strong>，婴儿时期还没有表现记忆。社会结构隐形地嵌于程序记忆的系统结构中，你所出生于的这个社会结构会被编入你的行为，而你并不知道规则。<strong>人类间的社会互动来源于一个有限制的空间</strong>，我们永远都在玩游戏，但分为三个阶段：小孩子的游戏嵌在大人游戏中（人可以在无法解释的时候做出行动，比如孩子无法描述游戏规则但可以玩游戏）；人们会随着发展，更加有意识地玩游戏，并开始在行为上表现游戏，开始学习显示游戏规则；之后在道德发展的最高阶段，<strong>人们意识到自己不仅仅是游戏的玩家，还是规则的制定者</strong>，也可以发明游戏。</p><p>皮亚杰高级抽象认知模型，如下图所示，皮亚杰认为要从底层的行为感知序列开始，从实际的微观行动逐步发展出高层抽象。所以也告诉我们，要<strong>从小事做起，从多方面做起</strong>。另外不要一经失败，就直接推倒最高层的抽象，要遵循**“最小定罪原则”**。比如，我这次面试表现不够高，直接推到了，我不是个有价值的人，这就很容易抑郁。</p><div align="center">  <img src="/2021/01/03/eclass-jp/multi.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>皮亚杰模型与弗洛伊德模型有很大差别，教授更倾向于前者，他认为在<strong>健康状态下是皮亚杰模型</strong>。病态发展中，你并没有把攻击冲动和性冲动整合到你的人格中去，而是让超我直接<strong>压抑了它们</strong>，没能称为活跃的一部分；你不去展示它，所以成为了所谓的“好人”。你对自己非常专制，成为一个超严厉超我的受害者；你把父母、祖父母及<strong>内化的“父母”集合成了一个超级严厉的法官，一直在注视你，评判你</strong>。</p><p>另外，教授认为弗洛伊德理论的最大缺点是认为人类有连续录影带式的记忆，某些由于太过痛苦或其他原因而被压抑，导致遗忘。jp认为人的记忆并不是这样有条理的，过去的信息是杂乱的，待清除的，所以<strong>人们没有注意到，而不是有意识（自我意识或潜意识）地去压抑</strong>。如同，弗洛伊德认为梦展现了被压抑的东西，梦尽力地像掩盖所要表达的内容。<strong>荣格则认为梦在尽可能清晰地表达</strong>，它不属于语义记忆系统，而更像是探索未知的触角，所以使用象征符号，而并不是想要向做梦者隐藏不愉悦的内容，而是用它唯一可以使用的语言体系。</p><blockquote><p>孩子在商场与家长分开会恐慌，因为原本家长将外界的复杂性与孩子隔绝，家长离开，混沌和不确定性向孩子涌来。</p><p>2-4岁时，应当让孩子学会如何和其它人（尤其是其它孩子）玩耍，加入到社会游戏中。</p><p>如何毁掉一个孩子？在他做好事或尝试做好事时，惩罚或者忽略他。</p></blockquote><h3 id="四-狮子王意象"><a class="markdownIt-Anchor" href="#四-狮子王意象"></a> 四. 《狮子王》意象</h3><p>本部分借由经典动画电影《狮子王》中的情节与画面，阐释了侵犯欲、自性、阿尼玛投射等问题。</p><p>一个理想的人，是爬到了某个优势等级顶端的人（<strong>龙虾理论</strong>）。</p><p>真正意义上的好人，单纯遵守纪律是不够的，需要<strong>理解恶并经受住恶</strong>。这里的恶，包括世间的恶，更指的是我自己的阴暗面，只有认识到自己是个魔鬼之后，才能对自己有足够的尊重，对自己的行为有所控制。我们可以通过读历史来理解自己的阴暗面，比如读奥斯维辛集中营等，想象自己是里面的卡波或党卫队，而不是前来解放的“英雄”。你必须对人性有了解，并不是作恶者在一边而受害者在另一边，<strong>人性之邪恶，人性之崇高都是你的中心元素</strong>。</p><p>根据弗洛伊德理论，我们内心的<strong>性欲与侵犯欲</strong>与口渴和饥饿等动机不同，这两种驱动力通常会被人类社会排除在外。早期的辛巴像一只瞪大了眼睛无辜的鹿，接收一切信息，但没有反应和产出，很容易牺牲自己而称为环境的出气筒。<strong>整合好自身的攻击性</strong>就像成年后的辛巴，面庞更加坚毅。<strong>一个人背后的愤恨变相地展示了他的不成熟</strong>。你没有能力伤害别人不代表你就是道德的，你需要<strong>长出獠牙</strong>，牺牲自己迎合别人，永远不制造冲突并不会让你成为一个更好的人。<strong>宜人性过高而同时还有愤恨的阴暗面</strong>，会让你无法拒绝加入某些情况下群众病态的行动当中，恶意的企图会把你的邪恶勾引出来。荣格认为，你应该将恐怖的一面整合进自身，而不是一味摒弃它、鄙视它、妄图彻底清除它，因为你无法做到。圣人不是纯白的，在这里我又想到了<strong>太极图</strong>。</p><p><strong>怨恨是一种恶毒的情绪</strong>。如果你怨恨某事，第一，这意味着你应该<strong>有所成长，承担责任</strong>，不再四处抱怨，哭哭啼啼。或者，这意味着正有人压迫你、欺辱你，你<strong>有东西要说出来或做出来</strong>，但你没有（因为那可能在短期内让你惹上危险）。日复一日，虽然短期内可以保护你，但这会把你挤压变形，随着怨恨积累，疯狂滋长，成为复仇的欲望。你会成为压迫者，你在背后说坏话，你会很马虎或者很勉强地去做被指派的任务。《地下室手记》里描述如果你在地下室里，三四年都想着这个世界多么邪恶，别人如何针对你、拒绝你，那么<strong>你会觉得活着本身就是有毒的</strong>，你会想要走出去做尽极恶之事。所以，在发展初期（比如事不过三，你要记得人们会狡辩，你要坚持住自己的观点），<strong>你最好把自己从压迫中释放出来，把真实想法讲出来</strong>（虽然它们不一定正确），你要与敌人或者不同观点的人真诚交流，探索未知。</p><p>我们可以通过<strong>查验是否还存在比较久的（18个月以上）活跃的负面记忆</strong>，来判断自己的感知价值结构是否存在漏洞。具体形式为<strong>自我写作</strong>，仔细考虑过去发生过在你身上那些不好的事，你需要弄清楚到底发生了什么，如何避免未来再次发生，梳理并表述自己的负面记录。（这个在《哈佛幸福课》里也提到过）。</p><p>如果你真正想在某处感到舒适，想主导并且融入这个地方，你需要<strong>花心思让每个角落都被光照耀到</strong>，包括对自己、对亲密关系，以合适的步伐去探索那些黑暗与未知。</p><p>《狮子王》中狒狒一角代表着人的<strong>自性</strong>，即如果你以获取最大化信息的方式与世界交互，未来可能成为的你。在这个过程中，我们<strong>要允许自己是傻瓜</strong>。当你尝试新鲜事物的时候，你确实回事一个傻瓜，允许自己犯错才能进步，完美主义在这时要不得。影片中辛巴出逃后达到了一片荒漠，如同《出埃及记》，当你离开一个国度（不管是怎样的残暴），你都会陷入混乱中。<strong>你抛弃了原有的价值体系，但并不会马上迎来提升</strong>（比如戒酒、戒毒时的戒断反应）。刚开始意识到自己无用的时候，会<strong>伴随着辩解，生气而毫无力量</strong>，爱发牢骚、满怀愤恨、可悲而欠揍的面容；唤醒自我意识，经过非常痛苦的自我反思，意识到自我价值结构的不足，我们会希望找到“父亲”（权威），但是父亲已经死了（<strong>这里没有权威</strong>）。你需要受到睿智的召唤，重新发现孩童时期和太阳相连的部分，并且<strong>相信</strong>。<strong>一旦你意识到自己错了，并且打开了潜在改变的大门，你的一部分自我会做出回应</strong>，因为你的深层自我还保存着潜在的可能性。有些心理治疗可以被<strong>高境界的道德努力</strong>取代，如果你真的想让情况好转起来，你考虑当前的问题，想弄明白下一步究竟该怎么办，那么你会找到内心中有某种东西在指引着你，那就是<strong>自性，是更高阶的自我，是转变中的不变</strong>。</p><div align="center">  <img src="/2021/01/03/eclass-jp/sinba.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>在一些故事里，国王会在国外培养，比如哈利波特、亚瑟王、狮子王等。这可能是由于，第一，周围文化滞后腐朽；第二，你并没有发挥出你的潜力践行价值，在现有的评价体系里你并不成功；第三，睿智的部分没有对邪恶的部分有足够的戒心，你要警惕自身有烦扰到自己或他人的倾向，不要让它反过来控制你，特别是当你逃避它的时候。<strong>人生的洪水</strong>（一些灾难）有上天随机的因素，也有一些你过去的短视或选择视而不见的结果。</p><h3 id="五-现象学"><a class="markdownIt-Anchor" href="#五-现象学"></a> 五. 现象学</h3><p>科学将主观剥离世界，将任何主观视为偏见或错误，但问题在于，人类，每个人都是一种主观。在这样的倡导下，人成为了冷漠的客观事实中一种孤立的存在，容易导致<strong>存在的虚无</strong>。于是会有尼采提出的**“上帝已死”**。主要问题并不是“世界是由什么组成的？”，而是“我们应该怎样存在于这个世界”。人类暴露于一种无意义且痛苦的存在，现代生活中不可避免地承担着很多焦虑，这可能是人们对科技的提升与意识觉醒付出的代价。人文类学科像魔法，让人意识到，我不止是父母的孩子，还是自然的孩子，整个人类文化的孩子。</p><p>基督教以追求真理为最高教义，但同时这一点最终会伤害其根本。<strong>物质世界观中隐藏着虚无主义</strong>，这会让你一无所有，无法应对生活中的挑战或苦难。如果你的价值系统瓦解了，那么你就会没有目标，没有积极情绪，人们也许会飞奔至极权主义的怀抱，牺牲理智与智力换取秩序与确定性，变成一堆抽象理论中的提线木偶，走向毁灭（纳粹及后期的苏联）。</p><p>马丁海德格尔（哲学家）对现象学影响深远。<strong>现象学认为人们生活在一个自我定义的感知框架之中</strong>。现象学认为客体是非常复杂的，比如波粒二象性。另外，即便以科学方法定义某个客体，也并非真正在定义，你只能说，这是个多维度物体，如果我以这样的方式接触它，也就是采取这样的过程或方法，它就会显现出那样的特质，但还有很多种其它的可能。<strong>每个人都有自己的价值体系</strong>，内在价值取向，隐形道德观，这是不可避免的。因为世界过于复杂而<strong>人类生物机能有限</strong>（比如大猩猩视频），我们只能聚焦一部分世界，而选择聚焦哪一部分。与此同时，我们的注意力又在很大程度上是自主的，并不完全有我们掌控。根据弗洛伊德理论，人类深层潜意识和其它哺乳动物、爬行动物等有很多共通之处，我们无法完全掌握这些“后台程序”，<strong>你的人格越没有整合好，就越容易失去对这类意识的掌控</strong>。目标启示着你的世界，组织着你的情感，并让你准备好做行动。目标包含了很多，比如内驱力、目的、动机等等，它是你人格的一部分。同时，我们还要时刻了解到，自己正处于收窄的视野范围。<strong>既然我们用简化视角看待世界，什么才是最好的处世之道呢？</strong></p><ul><li>如果你想适应好生活，你需要找到适应你性情的位置。做到这一点之后，你还是要努力提升与你性格相反的技能，理解与你性格不符的观点，扩展你的性情能力，相当于为各种环境准备更加齐全的工具箱。</li><li>研究跟你对立的观点是大有裨益的，比如你不知道的事，你无法采纳的视角，你没有掌握的技能等；</li><li>亲密关系中，不要找太过不同（尤其是开放性与尽责性）或太过相同的人。</li><li>人蕴含着很多不同的倾向，某些在休眠状态。你需要让自己处于合适的环境，进行一系列适当的练习，打开其它开关，<strong>需要努力、决心和自律</strong>。</li></ul><p>有关于“美”的探讨。宾斯旺格认为，美丽是主观的，是由于感知‘滤镜’产生的，我们最先感知到的，不是味道、声调或触感印象，也不是物体或客体，而是意义。鲍斯则认为，美丽固有于客体本身，显现了自身，向外发光。我们追求那些向外发光的东西，如同哈利波特中<strong>金色飞贼</strong>象征着的那样。有关于<strong>美来自何处</strong>，教授支持上述两个观点的结合。我们无法完全掌控自己的好奇心，但好奇心也并不是完全随机的。<strong>没有主体能脱离结构去感知，同时，被感知的那个客体也带着它自己的潜能向外发光</strong>。当我们探索者某个新的东西，在一次探索中发生了什么呢？首先，会有一个新的你，因为探索的过程会改变你（物质的、精神的）；此外，你同样<strong>在探索中生成了一个世界</strong>。**我们面前不是一个固定的客观世界，而是一个充满潜能的世界。**你能和这种潜能的任何方面进行互动，在互动中，你将一些以前不存在的东西拉进了现实。这些潜能并不是无限的，因为你本身就是有限的，但不管你有什么目标打算，它都足够了，因为它永远比你所需的潜能更多。梦境处于思考的前沿，走在我们前面。<strong>艺术超越了语言能表达的东西，否则它就不是艺术，而是宣传</strong>。它们发源于未知世界，并提供给我们一些信息。艺术家、诗人、哲学家先后了解到未知。</p><div align="center">  <img src="/2021/01/03/eclass-jp/gold.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>荣格后半生转向现象学，提出了<strong>人发展的三个必要层面</strong>（可视为皮亚杰道德模型的扩展）。<strong>第一，阴阳结合，即思想与情绪结合。<strong>比如，可以通过未来自我写作训练，让焦虑称为你的助手，想象如果你不去处理某个你目前尽量逃避的问题，会发生什么；比如坚定性训练，想象你具体想要什么，不要因为当下得不到就不去想象，想象你不去这个做的代价，整合攻击性。第二，再与身体结合，即</strong>行动起来，知行合一</strong>，保证认知与行动的连贯性。<strong>第三，消除自己与世界的区分</strong>。比如从打扫自己的房间开始，<strong>世界会随着你的目标而改变</strong>。在两个人沟通时，做到真正的倾听。<strong>真正的倾听是我们给别人非常宝贵的礼物</strong>。我希望了解你的观点，而不是输出我的观点然后只希望你赞同，我在这里是为了一起达到更好的你的一部分。</p><h3 id="六-存在主义"><a class="markdownIt-Anchor" href="#六-存在主义"></a> 六. 存在主义</h3><p>存在主义代表人物有尼采、陀思妥耶夫斯基、克尔凯郭尔等，存在主义包括几点基本理念：</p><p><strong>第一，行动比语言更有力</strong>。如果想了解别人或自己的信念，最好去看看他做了什么而不是说了什么。行胜于言，身心协调，知行合一。《地下室手记》里描述了典型的反面教材。</p><p><strong>第二，麻烦和苦难是人类经历的固有元素。<strong>人难以摆脱苦难，即使这个人本身没什么问题。那些痛苦、困境并不都是童年或过往经历的错。精神病态是人的脆弱性表现，<strong>焦虑、抑郁不需要理由</strong>，我们需要关注的是如何达到安全协调。合理的痛苦是正常的，人的生活本来就有问题，我们要关注的是如何面对和解决。类似圣经故事中所说，人们从美好的天堂掉落，总是处于残缺的状态，认为自己哪里出了问题，需要被纠正。你需要</strong>整合好自己的思想、精神</strong>、将每一种构成要素转换成功能性的存在，但这个功能性必须要与外界图景具有一致性，即<strong>融入社会</strong>。</p><p>大部分伟大宗教体系也拥抱这一观点，<strong>苦难的来源</strong>有如下三个，第一是<strong>社会结构的武断审判</strong>（无论你处于哪个社会中都会有不同程度的专断）但你需要和他人生活在一起，你要找到你适合的地方；第二是自然界的人性，<strong>人生而有限</strong>；第三是你自己也对某些不必要的痛苦负有责任，你本可以做某些事，让你的生活以及别人的生活得到改善；<strong>你有未承担起来的责任，而你的良知明明知道</strong>，人心真的有良知，听一听来自良知的劝告（conscience）。</p><p><strong>第三，存在主义包含了一些浪漫主义</strong>。存在主义悲观又乐观，它承认人是脆弱的，有限的，而<strong>一旦你直面恐惧，又会激发出我们无法估量的力量</strong>。如果你恰当地把自己推进世界，你会开启新的能力，并在探索过程中获得信息。<strong>对于你自己，你还有很多不知道的事</strong>，去新的环境，会改变你的微观生物结构。所以。不要针对死亡焦虑构建虚假的抵抗，而是<strong>积极学会如何应对这个世界</strong>，自愿地以更多方式挑战自己，可以促进这种转变。另外，存在主义<strong>反对理性与智力的至高地位</strong>，认为理性并不是指引人们的根本原则，而是需要与其它主观因素相辅相成。<strong>科学是一种需要被合理利用的工具，而不是描述存在的方式</strong>。另外，存在主义关注个体，认为<strong>个体才是心理分析的恰当层面</strong>。要将每个患者视为独特的个体，有独特的问题，而不仅是使用精神分析那种成体系的框架。</p><p><strong>人并不是理性的</strong>（这是对于乌托邦主义的批判）。人所做的事情，是为了时时刻刻证明他是人，而不是钢琴键，不是可以计算、推理的事物，就算这可能会损害他自己，就算要以自相残杀为代价。尼采认为，<strong>也许感到不满足其实也是一种满足，也许你必须受限</strong>，这是你想要的，也许这些才会给你生活的意义。</p><p>以恐狂症的逐步治疗为例，心理治疗是建立一段真诚的关系，同时自<strong>愿接触你不愿面对的东西是由疗愈性的</strong>，但由于很多普通场合并不是安全的地方，所以需要心理治疗。早期阶段中，我们会看到患者的<strong>退行</strong>，回到小孩子阶段，认为自己没有能力。一些心理分析师不赞成这种类似行为主义的治疗手段，认为对于电梯的恐惧不针对电梯，而是象征着其它东西，如果治好了电梯，这恐惧还会从其它地方冒出来。JP教授认为不然，治好电梯，患者会自行挑战出租车。这是在教患者学会勇敢，而<strong>勇敢会扩大化</strong>。</p><blockquote><p>联想到了自己刚开始科研的时候，由于对自己不断的批判，导致非常依赖DDL，因为到了那个时候你必须要专注，没心思再批判什么，必须专心完成。</p><p>发论文也是如此，直面恐惧，真的做到后，就会觉得没那么困难。</p><p>我在本科的时候就超高标准要求自己，把自己要求得宕机了…至今我也无法完全达到本科时的要求！</p><p>另外，表现得懦弱无用会成为一种武器。虚无主义者很痛苦，但他们的优势在于不用承担责任，是你自己你放手让你的价值体系崩溃了，于是你无需承担责任，代价是时常痛苦，但你可以一直哭哭啼啼，人们会为你感到难过。</p><p>可以写一个对自己未来三到五年的计划与反计划。</p></blockquote><h3 id="七-古拉格群岛"><a class="markdownIt-Anchor" href="#七-古拉格群岛"></a> 七. 《古拉格群岛》</h3><p>亚历山大索尔仁尼琴，可以认为是存在主义作家，类似的还有写《活出生命的意义》的维克克多弗兰克，以及哈维尔等。除了《古拉格群岛》，索尔仁尼琴还有《伊凡德尼索维奇的一天》等代表作。《古拉格群岛》是诺奖作品，1973年出版，之后Samizdat地下传阅，1989年再次公开出版。古拉格即苏联的纠正性劳动营主管部门，关押的是基于阶级和种族的罪名的与“特权”有关的人士。他们让一些罪犯（强奸犯、抢劫犯、小偷等）管理集中营，认为这些人会犯罪是由于沙皇俄国体制的压迫。这本书同《1984》、《动物农场》等一起例证劳动人民的乌托邦可以实现的这种想法的幻灭（茨威格《昨日的世界》中也有类似描述）。书中包括压迫性苏联体制的产生，斯大林统治下全面展开，communist system等部分。</p><p><strong>柬埔寨</strong>，某索邦大学PhD，认为城里人是基础生产上的寄生虫，所以他们几乎清空了城市，让人们去乡下生产，大约600万人死在了那里，而当年法国知识分子很支持这个想法。</p><p><strong>苏联解体</strong>后人们把这个归咎于斯大林的个人崇拜扭曲了最初准确的主义，认为如果列宁活得就一些，乌托邦就可以实现了。索尔仁尼琴从根本上反驳的了个观点，梳理了主义与列宁定制的某些法律之间的问题，比如清除异己、个人崇拜、专制权力、无处不在的监控，KGB等。在那里即使是坚定的党员也无法幸免，人类的心无法承受被心爱的斧头所伤，却还要证明那把斧头是智慧的。</p><p><strong>帕累托分布</strong>，可以影响金钱分布、公司关系等情况，支配了几乎所有创造性生产的领域。这是根本性原则，而目前没人知道该怎么有效且持续地把资源从几乎掌握一切的人手里撒到下层几乎什么都没有的人那里。<strong>即使是抛硬币式的随机财富分配</strong>也会最终集中到少数人手里，时间足够长，一个人手里。</p><p><strong>社会的病态和个体的病态间根本联系</strong>在于，<strong>个体倾向于欺骗自己</strong>，从而无法以真实真诚的方式行事，最终个体变为虚无主义者，或由于品格被逐渐削弱，不真诚称为了生活的一部分，转向意识形态和极权主义的解决方案，放弃恰当生活，放弃个体责任。我们很容易分辨一个被意识形态控制住的人，因为一旦你掌握了他们意识形态底层结构的五六个公理，你甚至可以预测他会说什么。这<strong>减少了他们思考的负担</strong>，也让他们相信自己完全掌握了世界上所有的知识，而且相信自己不需要思考就可以分辨出谁在善的一边。（休蒙格斯采访视频）</p><p>古老的故事中也有关于极权主义的体现，比如圣经故事中的《巴别塔》，极权主义大厦或乌托邦，越建越好，要容纳更多的因素，更多不同的人，最终会成为一盘散沙。《失乐园》中尝试为上帝辩护，包括其中所有的痛苦、恶毒和腐败。理性思维产生的政治的、意识形态的理性建构与引导着人类组织的超然神话间有一种紧张的关系。<strong>上帝的最高天使撒旦就是这种经理性思维的拟人化象征</strong>，这类思维倾向于产生极权系统，并爱上极权系统，系统之外的东西都不允许存在，最终将自己投入地狱。</p><p>这些故事指导我们<strong>正确度过人生的方式</strong>是，<strong>真诚地存在，拒绝参与说谎和欺骗，让你的语言/行动尽可能真实，为你的生活（也许还有其它人的生活）负起责任</strong>。这样做事是有意义的、负责人的、高尚的，而且也有助于减轻痛苦，这种痛苦会带来虚无主义，或让人逃入极权主义的怀抱。你需要一些东西来抵抗你自己的脆弱性，你可以采纳别人给你制定好的对现实的综合描述，这种描述<strong>把世界简洁地分为天真的无辜受害者和犯了罪的苦难制造者，而且他们都和你无关</strong>，但这不是评估世界的合理方法。<strong>即使你精神/心理没有出现问题，事情也向糟糕的方向发展了</strong>，但我们仍然有前进的方向，<strong>选择活得高尚一些，让你可以忍受你自己，甚至可能尊敬你自己，因为你能直面那可怕的脆弱和痛苦</strong>，具体来做，我们需要<strong>避免欺骗、承担责任、试图改善</strong>。</p><blockquote><p>什么是真实？你看到什么，听到什么，做什么，和谁在一起，如果有一种从心灵深处满溢出来的，不懊悔也不羞耻的，平和与喜悦，那就是真实。</p><p>如果提前了解了你们要面对的人生，不知道你们是否还会有勇气前来。</p></blockquote><p><strong>成为你自己。</strong></p><p>你很清楚你没有完全实现自己的潜能，<strong>你造成了部分的苦难</strong>，也许你可以换一种看待世界的方式，换一种行动的方式，不要再浪费眼前的机会！  人心真的有良知，虽然我们并不知道那是什么，你<strong>听从良心的劝告</strong>，<strong>五年或十年后</strong>，你会感到完全不同。以你真实自我建立起来的亲密关系会更强健、更愉快，完整的你，通过和伴侣协商，活出真实的生活，这是养育孩子的基础。但实际上人们不仅不做他们应该做的、让情况变好的事，他们还积极地把事情搞糟，因为他们（我目前是他们的一员）怀恨在心、怨气冲天、狂傲自满、尔虞我诈甚至杀人如麻，所有这些病态都纠缠在一起。你振作起来，多大程度上活出真实的自我，不仅关乎你一个人的命运，而是<strong>关乎所有与你产生联系的人的命运</strong>。你的所作所为非常重要，大多数事情都是有意义的，这同时意味着你需要承当由此而来的责任。不要低估自己的影响力。<strong>如果你过着病态的生活，你病态化了整个社会</strong>，如果有足够多的人这样做，社会会变成什么样子？也许在《古拉格群岛》中你会看到，这就是20世纪发生的事情，你是否想去那种地方，是否想让你在意的人去那种地方。</p><blockquote><p>我恳求你们不要错走路，不要惶惑，不要忘记你们的天职，千万不要理会那恶俗的力量的引诱，诞妄的巨体的叫唤，拥积的时尚与无意识，无目的的营利的诱惑。</p><p>这个时代缺的不是完美的人，缺的是从自己心底里给出的，真心、正义、无畏和同情。</p><p>在这个世界上，做一个好人要比做一个没原则的人付出更多的代价。可是做什么会让你真正开心，你要问清楚自己。</p><p>看到的和听到的，经常会令你们沮丧，世俗这样强大，强大到生不出改变它们的念头。可是无论外界的社会如何跌宕起伏，都对自己真诚，坚守原则。内心没有了杂念和疑问，才能勇往直前。</p></blockquote><p>人们需要在生命中的某阶段时间<strong>全身心参与某项游戏</strong>，从某时刻起，你得在某一方面有所成就，<strong>即使这意味着你牺牲掉了其他所有的可能，但你必须做出选择</strong>（某种职业、某种价值观），否则会徒增年岁而依旧混乱。</p><h3 id="八-大五人格"><a class="markdownIt-Anchor" href="#八-大五人格"></a> 八. 大五人格</h3><p>人格心理测量，当被试足够多的时候，观察各种问题的答案是如何协变的。人格特征会影响你的价值框架，你的决定，你的目标。在任何语言的演变过程中，人格与人格之间的差异如此巨大，它们被编入了词汇当中。大五人格理论的局限性在于，它假设没有语言不能描述的人格元素。</p><p>大五人格特质是简化世界的方法，将人们按照外向性、宜人性、尽责性、开放性、情绪不稳定型五个相互正交的维度分类。大五人格可以分为两类，一类是与多巴胺分泌功能有关的可塑性，包括外向型和开放性；一类是与血清素分泌功能有关的稳定性，包括尽责性（勤奋度、条理性）、情绪稳定性（情绪波动、孤僻型）和宜人性（礼貌与同性心）。</p><p>优秀的职业成就，需要创造力与尽责性的有机结合。创造力是一种高风险高回报的要素，如果你要追求创意事业，你需要找一份稳定的能够带给你收入的工作，然后在业余时间做创意。</p><blockquote><p>【自我分析】</p><p>本科及之前我的尽责性非常好，所以成绩都很好。但是研究生阶段，虽然正常情况下，此阶段取得的成绩依然和尽责性成正比，但环境很恶劣，我没有及时转弯调整，即创造力不足，不够灵活。这几年下来，我觉得创造力有了十足的提升，但是要记住，研究生以及后面的很多阶段，只要从事的是在某个规则结构下的工作（除了艺术和创业），尽责性都是非常重要，非常宝贵的！！</p></blockquote><h4 id="1宜人性"><a class="markdownIt-Anchor" href="#1宜人性"></a> 1.宜人性</h4><p>表现为同情心和礼貌，有时候很难把它和情绪不稳定性以及外向性区分开。女性总体上比男性有更高的宜人性，高半个标准差，比如表现在监狱中男性比女性多。</p><p>宜人性高的人擅于团队合作，很会肯定他人的功劳，但<strong>劣势在于不善于表达自己的利益诉求</strong>。比如女性总体比男性薪水低。因为长期薪水取决于你的技术、能力、职位、人际关系等，也取决于你是否真的要求加薪。女性消极情绪比较重，倾向于低估自己在商业领域的价值；因为你更注重自己的过失而不是贡献，如果你去和一个人谈判，但是已经犹豫不决自我怀疑，又在谈判中表现宜人，那么你就不会常赢了（比如面试）。你身居高位，人们可能就不喜欢你了，宜人的人特别需要别人的喜欢，他们要维持亲密积极的关系，倾向于避免冲突。<strong>世界对女性来说更危险（比如肢体冲突、性方面，以及为了适应母亲+婴儿的联合体）</strong>，也可以解释为什么女性对恐惧和惩罚更敏感。两性在心理问题上也有所不同，男性在酗酒、药物滥用、反社会人格、以及一系列学习障碍和注意力障碍，女性主要是抑郁和焦虑。</p><p>该如何要求加薪呢？</p><ol><li>这是我做的成果</li><li>这是为什么这些成果有用</li><li>这是你为什么应该给我加薪</li><li>如果不加薪，这是我其他的机会</li><li>给他一个加薪的理由，你必须有力地、非宜人地（不要太低，像个混蛋）提出你的诉求</li></ol><p>既宜人又尽责的无名劳动力大多为中年女性。如果大型公司想要雇佣可以高产能地剥削的人，要瞄准高尽责性、高宜人性的中年女性，因为他们当牛做马却毫不归功于自己，也不会抱怨。这令人厌恶但在现实社会中很普遍。</p><p>如果宜人性很高，他们常常连自己想要什么都不知道，因为太习惯为了别人活着。我们<strong>应当进行攻击性管理，说出自己的真实想法</strong>，你以为你的想法尖锐又讨厌（它们也许确实如此），但这些是实话，我们需要把这些问题拿到桌面上来讨论，即便你非常厌恶冲突。另外，<strong>身体流畅性常见于取悦自己的人</strong>。</p><h4 id="2-开放性与智力"><a class="markdownIt-Anchor" href="#2-开放性与智力"></a> 2. 开放性与智力</h4><p><strong>开放性</strong>是对感知、幻想、审美和情绪的认知参与；可以预测文艺领域的创造性成就。相关测量问卷的中位数是0，因为需要满足来两方面要求：1）你产生的点子多大程度与其他人不同；2）新颖且有用。<strong>（科研就是如此！！！）</strong></p><p>对于小说的偏爱是经验开放性特质的一部分，小说的作用是能让你体验林林总总的模拟世界，阅读小说可以提升人际交往能力。</p><p><strong>智力</strong>是指通过与创造力相关的推理能力反映个体对抽象信息和语义信息的认知参与；可以预测科学领域的创造性成就。智力与学习新事物的能力非常相关，<strong>但不预测学会之后能做多好</strong>。智力可以通过量表测量，甚至<strong>是心理测量最成功的领域</strong>，如果不认可的话，基本其它心理学定义也无需认可了。</p><p>当我们面对事物本身，我们用感知将其简化成为一个符号表象，之后安上一个词语，从而将世界的复杂性转化为可掌控的事物。自闭症儿童无法抽象出不同情形下的常态并冠以抽象特征，尤其是对于其他人，因为这过于复杂。</p><p>有关智力这部分，有几个需要接受的现实。</p><ul><li>智力只有一个因子，是一个重要特征。随着智商增高，其不同方面差异性会增大，即愚蠢的人都是相似的，但高智商的人各有各的智慧。</li><li>大约15%的人智商在85，社会上基本没有合适的工作（美军不征收低于83的人入伍）。</li><li>世界正在快速复杂化，不聪明的人越来越难以找到工作，工人阶层、低端白领。但无所事事而尽责性高的人会非常痛苦，他们可能会患上慢性疼痛或抑郁，进而有鸦片类药物上瘾的问题。比如目前自动驾驶对司机这一职业的威胁。</li><li>男女在进化过程中在认知能力方面进行着竞赛，女性智力高会更倾向于单身。</li><li>计算机设备是智力和尽责性的倍增器。智商高且尽责的人会最终处于帕累托分布的顶端。速度快、努力、精力旺盛、身体健康。</li></ul><h4 id="3-尽责性"><a class="markdownIt-Anchor" href="#3-尽责性"></a> 3. 尽责性</h4><p>尽责性高的人条理、尽职、勤勉，会为了未来而牺牲现在。但这要求环境必须要足够稳定，恶性通货膨胀损害最多的就是高尽责性的人。而且这样的人非常苛责，在大环境情况不顺利的时候，苛责自己。</p><p>测量结果表明，尽责性与智商毫不相干。尽责性可以准确预测军事表现，因为执行力强，忠于职守。但是我们并不知道为什么会有勤奋的人。没有相关心理或理论、认知模型也没有医药研究。尽责性与情绪不稳定性呈负相关，与内疚经历负相关，与内疚倾向正相关。</p><p>条理性与厌恶敏感相关，苦味饮料也可能引发更重的道德批判。条理性高的人偏好秩序、传统、不喜欢平均注意，有保守主义倾向。他们对违反道德准则的行为强烈谴责。</p><p>希特勒具有极高的开放性，极高的条理性，极其崇尚意志力。纳粹更像是文明的恶疾，利用人们的条理性，起初是“除四害”，然后安乐死精神病人，之后是政治活动（将一些人定义为寄生虫，刺激人们的厌恶系统）。在纳粹的反犹宣传电影中，我们会看到<strong>原型是如何与底层生物基础紧密相连，以及这种紧密关系如何被利用来服务于政治宣传目的</strong>，这具有非常强大的情绪煽动力。他们采用不同的小调乐曲、交易场面、肮脏混乱的环境等煽动大家对犹太人的厌恶。影片中还宣传参与基本生产的劳动力才是劳动力，参与商业活动的劳动力不算劳动力。针对这类宣传的抵抗方法是，<strong>不把极度复杂的问题灾难性地简化成教条，你一定要在一定程度上了解这些政治宣传机制，以及为何它们如此有效</strong>。</p><p>在更高层面讲，<strong>行为免疫系统</strong>也体现了尽责性。寄生虫应激假说在社会层面的反应，避免高水平感染，但可能引发种族主义、苛责的性道德等。极权政府出现在致病源体高发的地区，使个体更加支持专制政府。饥荒、战争等也都有影响。</p><h3 id="九-人生表现预测"><a class="markdownIt-Anchor" href="#九-人生表现预测"></a> 九. 人生表现预测</h3><h4 id="1教授辛酸史"><a class="markdownIt-Anchor" href="#1教授辛酸史"></a> 1.教授辛酸史</h4><p>教授讲述了自己艰辛创业的血泪史。很多公司会买MBTI测试，但它基本在表现预测上没有任何效用。教授有一个更为准确且能为公司带来更大收益的性格量表，但就是很难卖出去。因为，首先多数人没有统计学思维，其次每个公司对测试题有有限的预算，而且有部门间的竞争。人们希望要一个测试准确、不带歧视色彩、让每个人都高兴、非常廉价、费时极短的测试。教授最终总结道，创业艰辛，即便你很棒的点子，也不能直接创业。你要有目标客户群，并且不断了解他们购买的意愿。</p><p>有关销售技巧，<strong>“人们不想成功，而是更不想失败”</strong>。如同斑马和鳕鱼一样，人类的基本动机就是“希望自己不惹人注意，不被打扰”。<strong>人类的伪装色是针对人群的</strong>，希望自己待在人群的中央，和鱼群一样人们总是试图向群落中央靠拢，随着群落移动，这样就有他人形成的保护圈。<strong>负面情绪系统是绝佳的动机源</strong>，人们不是想变开心，而是想避免痛苦。情绪不稳定的特点就是，不要威胁我，不要伤害我。</p><h4 id="2-人存在天壤之别"><a class="markdownIt-Anchor" href="#2-人存在天壤之别"></a> 2. 人存在天壤之别</h4><p>通过大五人格我们了解到，在<strong>能力方面</strong>，智商、尽责性是正态分布，而基本高层的所有职业都需要极高的智力和极高的尽责性。</p><p>另外，基本各个领域都是遵循帕累托分布的，极小部分的人产出了这个领域几乎全部的成果。即便是随机交易（抛硬币）也会造成这种结果，0与1之间差距巨大，1表示你已经进入游戏场，<strong>要尽量避免自己“归零”，让自己处于正反馈回路</strong>。从这个角度来讲，一个人进入恶性循环<strong>可能并不是心理问题</strong>，而是世界的规律，仿佛有一个外力拉着你一路向下。你只是“遇上麻烦了”需要解决它，你只是某些心理上的不足（导致你很难从困境中出来，甚至把处境弄得更糟），而不是有心理问题。</p><p>人并不是一个个独立的个体，你和外界的动态网络紧密相连，这些网络非常重要！人们非常需要“社会认同”，在说服别人的时候，一定要尽力降低他决策的风险，而这也和正反馈回路相关。</p><p>再向外延伸，可以转化为平方根准则/普莱斯法则，即某个领域中，总人数的平方根人数产出一半的成果。从公司管理者角度，要非常注意这个法则，因为如果业绩不好而前面高产出的人又离职了，就非常危险了。</p><p>在<strong>政治倾向方面</strong>，<strong>最根本的政治议题是，事物间的界限应该是打开的还是封闭的</strong>。条理性是政治保守性倾向的一个重要指标，没有标准答案，所以我们用不同性情的人去解决，这就是政治讨论。</p><p>此外<strong>人格特质会驱使感知结构</strong>，比如外向者追求社会成功，高宜人者追求合作而非竞争，情绪不稳定者导向保障与安全，尽责着导向秩序、规则、使命、执行，高开放性导向抽象。</p><p>所以人与人之间本身就存在着天壤之别，在做实验的时候应当把智力因素、大五人格因素都抛弃掉，但大多数心理学家并不这样做。</p><h4 id="3-取得成功需要什么"><a class="markdownIt-Anchor" href="#3-取得成功需要什么"></a> 3. 取得成功需要什么</h4><p>第一，需要<strong>智商</strong>，20岁之后流体智商会快速下降，保持身体健康，时常锻炼非常重要，因为大脑非常耗氧，它需要保持清醒。</p><p>第二，需要<strong>高尽责性</strong>，虽然这很大程度上取决于基因，但我们可以通过从细节习惯着手进行改善。</p><p>第三，<strong>人际关系</strong>，我们需要有意识地发展和经营你的人际关系网，尽力让自己周边是一些在不同领域很厉害的人，并与他们保持联系，互惠的关系。</p><p>第四，<strong>技能</strong>，每次你有机会学习新的技能，这个机会能教你一些知识以便于你更能抓住下次机会，你就要抓住这个机会。每次有机会来临，你要抓住他，利用它提升自己的本领，这是真正的财富，正如<strong>荣格的经典理念中沙特尔大教堂</strong>所象征的。教堂穹顶代表天空，穹顶正下方有个十字（苦难发生的地方），是世界中心，也是你，因为你就是有意识承受着苦难的地方。地面刻有一个大迷宫，图案是曼陀罗，中央是花型图案，想要到达中央需要走遍整个迷宫。</p><p>第五，<strong>树立你认为有价值的目标</strong>。首先，<strong>在未来三到五年，你想要怎样的友谊？怎样的亲密关系？事业如何？怎样度过业余时间？如何保持自己的身心健康？<strong>重点是你一定要有一个你认为有价值的目标，否则你没有足够的积极情绪。弄清楚自己</strong>想成为什么样的人</strong>，然后以此为目标，用你所学到的知识，将自己锻造成那样的人。注意这里<strong>是“想要成为”而不是“应该成为</strong>”，你需要对生活有一个愿景，这个愿景让你的努力变得更有意义，这样你就不会抱怨生活，否则对人对己都是危险的。之后，你需要<strong>将目标分解为可以实行的微行程</strong>，其奖赏价值来自它跟大目标的因果联系。<strong>你要具体化一个你的长远目标，同时具体化一个你非常不想要的场景</strong>。人们常常不愿意具体化目标，因为他们不愿意具体化自己失败的情景。让自己处于含混不清的状态很容易，那就是无所事事，不会看到失败，但那其实是时时刻刻的失败，到最后无以复加的时候，你再也无法视而不见。最后，你要<strong>设计日程表</strong>。但不要把你的日程表搞成监狱。做好日程计划是为了让你过上想要的一天，<strong>你需要和自己（和大象）协商，而不是苛责</strong>，要充满希望，提出自己的愿景和要求。即便没有完成全部计划，也比零好，好过直接放弃。而且，你一定要认识到自己<strong>时间的价值！！！<strong>否则你会无所事事，无需对任何事情负责，然后</strong>备受生存意义的拷问</strong>。</p><blockquote><p>【如何和老板沟通】</p><p>1）我发现了这个问题；2）这是我的解决办法（重点）；3）我来跟你讨论一下这个问题和解决方法。</p><p>【关于友谊】</p><p>真正的朋友，你可以把有关自己的坏消息告诉他们，他们会倾听，你可以分享好消息，他们会为你庆贺。让你身边环绕那些为了你最好的部分自身着想的人，助你成长的人。另外，对于那些不听劝告一直抱怨的朋友，你要直说，“你继续过你的悲惨生活吧，我去过我的，期待你在未来的某个时刻会醒过来”，不要容忍，不要默许这种行为。</p><h3 id="四-心理问题"><a class="markdownIt-Anchor" href="#四-心理问题"></a> 四. 心理问题</h3></blockquote><p>如何解决心理问题，比如你如今的生活不如意，那么你期待一年以后的生活（或者至少某些方面）是什么样的？确定下来，作为目标，<strong>（诚实地）筹划一下</strong>，认真考虑会遇到哪些阻碍，然后制定策略，试着朝理想状态买迈步。</p><p>很多心理学家也许过分重视人的内在，而忽略了那个外在的我、外在的世界。有些人并没有心理问题，而是生活真的出现了问题。</p><p>临床心理学是有价值导向的，并不是纯粹的科学学科</p><p><strong>对于真实的定义（即使是科学）也只是一种人类用来探索世界的工具</strong>，我们会看到世界工具性的一面。</p><p>文化是一种恐惧管理机制，赋予意义，避免陷入<strong>混沌</strong>，而且不对死亡过度焦虑。这种混沌时混沌的世界和混沌的自我，除了不安、焦虑，还有对于世界、对于自己甚至对于过去的理解都在瓦解。</p><p>让某人去做某事的最佳方法，是禁止他做这件事而又不告诉他原因。</p><p>最后加上最近刚看的电影《Soul》剧照，Terry与Jerry，也是秩序与自由，阴与阳的代表吧，二者相辅相成，和谐共存。</p><div align="center">  <img src="/2021/01/03/eclass-jp/soul.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>《无问东西》中还有这样一句台词，讲到心里：</p><blockquote><p>那一刻，我从思索生命意义的羞耻感中释放出来，希望你们今后的岁月里，不要放弃对生命的思索，对自己的真实。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>心理学</tag>
      
      <tag>哲学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIT《计算机科学“补漏”课》</title>
    <link href="/2020/12/27/mit-null/"/>
    <url>/2020/12/27/mit-null/</url>
    
    <content type="html"><![CDATA[<h2 id="概述"><a class="markdownIt-Anchor" href="#概述"></a> 概述</h2><p>课程官网中文版（<a href="https://missing-semester-cn.github.io/%EF%BC%89%EF%BC%8CB%E7%AB%99%E8%A7%86%E9%A2%91%EF%BC%88https://www.bilibili.com/video/BV1aE41157q6%EF%BC%89%E3%80%82%E5%AE%98%E7%BD%91%E4%B8%AD%E7%9A%84%E6%9D%90%E6%96%99%E5%B7%B2%E7%BB%8F%E9%9D%9E%E5%B8%B8%E8%AF%A6%E7%BB%86%E4%BA%86%EF%BC%8C%E8%BF%99%E9%87%8C%E5%8F%AA%E8%AE%B0%E5%BD%95%E4%B8%8B%E6%94%B6%E8%8E%B7%E6%AF%94%E8%BE%83%E5%A4%A7%E7%9A%84%E9%83%A8%E5%88%86%EF%BC%88%E5%AF%B9%E4%BA%8E%E6%88%91%E8%BF%99%E4%B8%AA%E6%8A%80%E6%9C%AF%E6%B8%A3%E6%B8%A3%E5%8F%AF%E8%83%BD%E5%90%84%E5%A4%84%E9%83%BD%E6%94%B6%E8%8E%B7%E9%9D%9E%E5%B8%B8%E5%A4%A7%EF%BC%89%E3%80%82" target="_blank" rel="noopener">https://missing-semester-cn.github.io/），B站视频（https://www.bilibili.com/video/BV1aE41157q6）。官网中的材料已经非常详细了，这里只记录下收获比较大的部分（对于我这个技术渣渣可能各处都收获非常大）。</a></p><p>课程包括11部分，每部分一小时左右，各部分基本独立。本课程的主旨在于让计算机学科学生精通工具，具有很强的实践性和实用性。</p><p><strong>最好是把各个课后作业弄完！！！</strong></p><h2 id="命令行与shell"><a class="markdownIt-Anchor" href="#命令行与shell"></a> 命令行与Shell</h2><h3 id="一-基础"><a class="markdownIt-Anchor" href="#一-基础"></a> 一. 基础</h3><p>Bourne Again SHell，简称**“bash”**，课程重点关注这类shell。Shell是一个编程环境，具备变量、条件、循环和函数等，在shell中执行命令时其实是在执行一段shell可以解释的代码。如果命令中包含非关键字指令，它会去查询环境变量$PATH。环境变量中记录了shell会去查询可执行程序的所有路径。</p><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-variable">$PATH</span>     <span class="hljs-built_in">which</span> <span class="hljs-built_in">echo</span>     <span class="hljs-comment">#显示echo指令的路径</span>/bin/<span class="hljs-built_in">echo</span> <span class="hljs-variable">$PATH</span>  <span class="hljs-comment">#绕过环境变量，直接指定路径执行程序</span></code></pre><p>在bash中输入的单个内容中<strong>包含空格</strong>时，可以使用引号或者转义字符。</p><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> Hello\ World</code></pre><p>命令中的<strong>选项</strong>，如果不需要输入叫做‘flag’，需要输入参数的叫‘option’。</p><p>-h，–help与man（manual），获得关于某指令的帮助手册。另外可以通过<a href="https://tldr.sh/" target="_blank" rel="noopener">tldr</a>查看手册的简化版，即一些常见命令。</p><p>Ctrl + L快捷键，快速清空界面。</p><p>rm -r（递归删除），rmdir也可以但是只能删除内容为空的文件夹。</p><p>xdg-open，将文件以适合的程序打开。</p><p><strong>文件系统</strong>中，d表示文件夹，后面的九个参数分别表示文件所有者、用户组和其它人的权限。为了进入某个文件夹，用户需要具备该文件夹以<strong>及其父文件夹</strong>的“搜索”权限（也即需要有可执行权限）。</p><pre><code class="hljs bash">ls -l /homedrwxr-xr-x 1 missing users 4096 Jun 15 2019 missing</code></pre><p><strong>输入输出流</strong>最简单的重定向是<code>&lt; file</code>和 <code>&gt;file</code>，使用&gt;&gt;可以追加内容； | （管道）允许我们将不同程序的输出输入连接起来。但是有些命令需要从参数接受输入，我们可以用<code>xargs</code>命令将标准输入中的内容作为参数，比如<code>ls|xargs rm</code>会删除当前目录中的所有文件。</p><h3 id="二-强大的root用户"><a class="markdownIt-Anchor" href="#二-强大的root用户"></a> 二. 强大的root用户</h3><p>通常我们不以root身份登录，而是使用sudo命令，确认是否执行某些操作，避免对系统造成伤害。</p><p>向sysfs文件写入内容是必须以根用户身份才能做的。系统被挂载在/sys下，sysfs文件暴露了一些内核参数。（Windows和macOS没有这个文件）</p><pre><code class="hljs bash">sudo <span class="hljs-built_in">echo</span> 3 &gt; brightness <span class="hljs-comment">#出错</span><span class="hljs-built_in">echo</span> 3 | sudo tee brightness <span class="hljs-comment">#可以写入</span><span class="hljs-comment"># tee指令将输入写入文件，同时也会输出至终端</span></code></pre><p>命令中的 | &gt; &lt; 等是通过shell执行的而不是被各个程序单独执行，第一个命令中shell在设置sudo echo前尝试打开brightness文件并写入，但因为此时不是根用户而被拒绝。</p><h3 id="三-shell脚本"><a class="markdownIt-Anchor" href="#三-shell脚本"></a> 三. Shell脚本</h3><p>shell脚本中使用空格会起到分割参数的作用，有时候可能会造成混淆。（尤其是在变量赋值的时候）</p><p>Bash中的字符串通过<strong>单引号</strong>定义为原义字符串，<strong>双引号</strong>的字符串会将变量值进行替换。</p><pre><code class="hljs bash">foo=bar<span class="hljs-built_in">echo</span> <span class="hljs-string">"<span class="hljs-variable">$foo</span>"</span>  <span class="hljs-comment">#打印bar</span>echp <span class="hljs-string">'$foo'</span>  <span class="hljs-comment">#打印$foo</span></code></pre><p>source指令即在当前bash环境下读取并执行filename中的命令；和“.”命令等同。source（或点）命令通常用于重新执行刚修改的初始化文档，如 .bash_profile 和 .profile 等等。</p><p>Bash使用了很多<strong>特殊变量</strong>表示参数、错误代码和相关变量，<a href="https://tldp.org/LDP/abs/html/special-chars.html" target="_blank" rel="noopener">完整列表</a>。</p><div align="center">  <img src="/2020/12/27/mit-null/shell.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p><strong>返回码或退出状态</strong>是脚本/命令之间交流执行状态的方式，返回0表示正常执行，其它所有非0的返回值都表示有错误发生。false的返回码永远是1，true的永远是0。退出码可以<strong>搭配与（&amp;&amp;）、或（||）操作符使用</strong>，进行条件判断。</p><p>另外可以通过<strong>命令替换（command substitution）实现</strong>，即以变量的形式获取一个命令的输出。通过<code>$(CMD)</code>执行某CMD之后其结果会替换掉这个变量。另外还可以通过<strong>进程替换（process substitution）实现</strong>，比如<code>diff &lt; (ls foo) &lt;(ls bar)</code>，&lt;（CMD）会执行CMD并将结果输出到一个临时文件，而且把&lt;(CMD)替换成临时文件名，比如上面的命令就会显示文件夹foo和bar中文件的区别。</p><p>在Bash中进行<strong>比较</strong>时，<strong>尽量使用双方括号</strong>而不是单方括号，可以降低犯错几率，虽然这样没法兼容sh。</p><p>Shell的<strong>通配（globbing）</strong>：1）通配符？和*用来匹配一个或任意个字符；2）花括号{}，自动展开一系列命令汇总的公共子串。</p><pre><code class="hljs bash">cp /path/to/project/&#123;foo,bar,baz&#125;.sh /newpath<span class="hljs-comment"># 会展开为</span>cp /path/to/project/foo.sh /path/to/project/bar.sh /path/to/project/baz.sh /newpath<span class="hljs-comment"># 也可以结合通配使用</span>mv *&#123;.py,.sh&#125; folder<span class="hljs-comment"># 会移动所有 *.py 和 *.sh 文件</span></code></pre><p>可以使用<strong>shellcheck工具</strong>帮助定位sh/bash脚本中的错误。</p><p><strong>脚本与shell函数</strong>的不同点：</p><ul><li>脚本可以使用任意语言，因此包含shebang是非常重要的，在shebang行中使用env命令（而不是直接用绝对路径）可提高脚本可移植性，比如<code>#!/usr/bin/env python</code>；</li><li>脚本在每次执行时被加载，函数在定义时被加载；</li><li>函数会在当前shell环境中执行，脚本会有单独的进程执行，因此函数可以对环境变量（比如当前工作目录等）进行更改，脚本需要使用expert将环境变量导出；</li><li>函数可以提高代码模块性、复用性并创建清晰的结构，sehll脚本中往往会包含自己的函数定义。</li></ul><h3 id="四-常用命令"><a class="markdownIt-Anchor" href="#四-常用命令"></a> 四. 常用命令</h3><p><strong>find</strong>，查找文件或目录，可以递归地搜索符合条件的文件，其-exec选项非常厉害。另外，<code>fd</code>是find命令的一个很好的替代品。<code>locate</code>可以实现更快速的搜索，由updatedb负责更新的数据库，通过cron每日更新。find和类似的工具可以通过别的属性（文件大小、修改时间、权限等）来查找文件，但locate只能通过文件名。</p><pre><code class="hljs bash"><span class="hljs-comment"># 名为src的文件夹，--inmae，不区分大小写</span>find . --name src --<span class="hljs-built_in">type</span> d<span class="hljs-comment"># 路径包含test的python文件</span>find . --path <span class="hljs-string">'**/test/**/*.py'</span> --<span class="hljs-built_in">type</span> f<span class="hljs-comment"># 前一天修改的所有文件</span>find . --mtime -1<span class="hljs-comment"># 所有大小在500k至10M的tar.gz文件</span>find . --size +500k --size -10M --name <span class="hljs-string">'*.tar.gz'</span><span class="hljs-comment"># 删除所有.tmp文件</span>find . --name <span class="hljs-string">'*.tmp'</span> --<span class="hljs-built_in">exec</span> rm &#123;&#125; \;<span class="hljs-comment"># 将所有png转为jpg</span>find . --name <span class="hljs-string">'*.png'</span> --<span class="hljs-built_in">exec</span> convert &#123;&#125; &#123;&#125;.jpg \;</code></pre><p><strong>grep</strong>，查找文件内容，包含很多选项，比如-C：获取查找结果的上下文；-v，对结果进行反选；-R，递归地进入子目录并搜索所有文本文件等。该命令也有很多替代品，如<strong>ack，ag和rg</strong>。</p><pre><code class="hljs bash"><span class="hljs-comment"># 查找所有使用了requests的文件</span>rg -t py <span class="hljs-string">'import requests'</span><span class="hljs-comment"># 所有没写shebang的文件（包含隐藏文件）</span>rg -u --files-without-match <span class="hljs-string">"^#!"</span><span class="hljs-comment"># 所有foo字符串，并且打印其后5行</span>rg foo -A 5<span class="hljs-comment"># 打印匹配的统计信息</span>rg --stats PATTERN</code></pre><p><strong>history</strong>，查找输入过的命令，一般搭配grep使用。使用<code>Ctrl+R</code>可以对命令历史记录进行回溯搜索，多按几次会不断显示匹配的搜索结果，可以配合fzf使用。fzf是一个通用对模糊查找工具。另外输入命令的时候，<strong>如果在开头加了空格，它就不会被记录在shell</strong>。也可以通过编辑.bash_history或.zhistory手动移除。</p><p><strong>fasd</strong>可以查找最常用和最近使用的文件和目录，它会同时针对频率和时效进行排序。</p><p>tree，rboot可以查看文件目录结构，nnn和ranger可以提供更加完整的对文件的管理。</p><h2 id="文本编辑vim"><a class="markdownIt-Anchor" href="#文本编辑vim"></a> 文本编辑（Vim）</h2><p>现代文本编辑器都是一些复杂且强大的工具，永远有新东西可学，学的越多，效率越高。目前来讲，VS code是最流行的代码编辑器，Vim是最流行的命令行编辑器。</p><p>Vim是一个多模态编辑器，一个能跟上你思维速度的编辑器。Vim最重要的设计思想是，它的界面本身是一个程序语言，键入操作时可以组合的命令。</p><h3 id="一-vim基本操作"><a class="markdownIt-Anchor" href="#一-vim基本操作"></a> 一. Vim基本操作</h3><p>包含正常、插入（i）、替换（R）、可视化（v/V/Ctrl-V，选中文本块，包含正常、行和块三种模式）、命令（:）多种模式。不同模式下有不同的键盘键入意义。</p><p>Vim会维护一系列打开的文件，即“缓存”。一个Vim会话包含多个标签页，每个标签页包含一系列窗口，每个窗口显示一个缓存。而缓存和窗口并不是一一对应关系，窗口只是视角。</p><p>命令模式中<code>:e {文件名}</code>打开要编辑的文件，<code>:ls</code>，显示打开的缓存，：w {filename} 另存为其它文件，：saveas 相同。</p><div align="center">  <img src="/2020/12/27/mit-null/vim.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>编辑方法常见操作如下。可以在前面加上数字进行**“计数”**，如'3w'表示向前移动三个词。另外，可以加上一些**修饰语**，比如“i”表示内部或在内，“a”表示周围。<p>i在当前光标处插入，I在行首插入，A在行末插入，a在光标后插入编辑，o在当前行后插入新行，O在当前行前插入新行，cw替换光标从所在位置到单次结尾的字符。</p><div align="center">  <img src="/2020/12/27/mit-null/edit.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>平时常用操作总结：<ul><li>h（向左）、j（向下）、k（向上）、l（向右）、w（下一个单词）、b（上一个单词）。</li><li>使用Shift+zz也可以直接退出;</li><li>&quot;.&quot;重复之前操作；Ctrl + o 可以回到光标之前的位置；</li><li>删除操作：dw, daw (delete a word), dnw, 10x等加入计数的删除操作；（d操作其实不是删除，而是剪切，<strong>ddp可以直接实现上下行交换</strong>）</li><li>：set nu，显示行号；gg（移动到第一行），G（移动到最后一行），ngg/G（移动到第n行）</li><li>~ 对于当前字符的大小写转换；</li><li>y复制，yy复制整行，y$复制到行尾，y^复制到行首，p在光标后面粘贴，P在光标前面粘贴</li><li>r替换，R连续替换（直到按esc），cc（整行替换并进入插入），C（替换至行尾并进入插入）；😒 也是替换命令，比如<code>%s/foo/bar/g</code>是在整个文件中将foo全局替换成bar；</li><li>U撤销当前行的所有修改，Ctrl+r，redo即撤销undo的操作。</li><li>缩进，<code>:set shiftwidth=4</code>, &gt;&gt;； :ce（居中）, :ri（靠右），:le（靠左）</li><li>搜索：/向后查找，？向前查找；n下一个内容，N上一个查找内容；noh取消查找；普通模式下输入<code>\*</code>，会查找光标所在位置的单词，<code>\#</code> 作用相同但是向前查找，<code>g\*</code>表示模糊查找；f&lt;字母&gt;，向后搜索到&lt;字母&gt;并跳转到第一个匹配位置，F为向前搜索；</li><li>可以打开多个文件，使用:n和:N切换；:e#回到之前一个文件；:e! &lt;新文件&gt;，打开新文件且放弃当前文件；:f new.txt，给当前文件重命名；:f 显示正在编辑的文件名；:b 文件名/编号，直接进入某个打开了的文件；:bd 文件名/编号，删除某个打开了的文件</li><li>文件意外关闭，可以进入vim使用<code>: recover 1.txt</code> 恢复；或者直接使用<code>vim -r 1.txt</code> 直接选择使用swp交换文件打开。</li><li>可视模式，进入之后选择光标走过的地方，再按取消；v进入字符选择，Shift+v进入行选择，Ctrl+V进入区域选择。</li><li>多个视窗，<code>:new</code>打开新的视窗，:sp 1.txt，打开水平窗口编辑1.txt，:vsp表示打开垂直窗口；ctrl+w s，将当前窗口分割成两个水平窗口，ctrl+w v，将当前窗口垂直分割；<code>:q</code>结束当前视窗；ctrl+w o，打开新的视窗并隐藏之前所有的；Ctrl+w {h,j,k,l}在视窗间移动；Ctrl+w {H,J,K,L}将当前视窗移动位置；Ctrl+w {-, +} 调整当前视窗高度。</li><li>使用 vim -x file 命令进行文件加密</li><li>加入！可以执行某些外部命令，比如 !ls，!rm file 等</li><li>帮助，普通模式下按F1，命令模式下输入<code>:h xxx</code>，或者输入:ver显示版本及参数</li></ul><h3 id="二-自定义与扩展"><a class="markdownIt-Anchor" href="#二-自定义与扩展"></a> 二. 自定义与扩展</h3><p>保存永久配置需要修改vim的配置文件<code>~/.vimrc</code>，可以下载一些别人配置好了的进行替换（最好是理解了里面的内容）。</p><p>在命令行中输入:set 显示所有修改过的配置，:set all，显示所有的设定值，set的其它常用参数：</p><ul><li><code>:set &lt;option&gt;?</code>，当前option的设定值</li><li><code>:set nooption</code>，取消当前设定值</li><li><code>:set autoindent(ai)</code>，自动缩进</li><li><code>:set cindent(cin)</code>，C语言风格缩进</li><li><code>:set autowrite(aw)</code>，自动存档</li><li><code>:set backup（bk）</code>，自动备份</li><li><code>:set background=dark</code>，背景风格</li></ul><p>Vim中有很多扩展插件，可以创建一个<code>~/.vim/pack/vendor/start/</code>的文件夹，然后把插件都放在这里。<a href="https://vimawesome.com/" target="_blank" rel="noopener">Vim Awesome</a>里有很多好用的插件，比如支持模糊文件查找的ctrip.vim，支持代码搜索的ack.vim，文件浏览器nerdtree，魔术操作vim-easymotion等。</p><p><strong>（宏的部分是什么）</strong></p><p>Vim是一个熟能生巧的强大编辑器，用 Vim 做你 <em>所有</em> 文件编辑。 每当不够高效的时候, 或者你感觉 “一定有一个更好的方式”， 尝试求助搜索引擎， 很有可能有一个更好的方式。</p><h2 id="数据整理"><a class="markdownIt-Anchor" href="#数据整理"></a> 数据整理</h2><p>grep可以对管道内容进行过滤；</p><h3 id="一-sed编辑器"><a class="markdownIt-Anchor" href="#一-sed编辑器"></a> 一. sed编辑器</h3><p>sed是一个基于文本编辑器ed构建的“流编辑器”，在其中我们使用一些较短的命令修改文件。常用<code>s</code>即替换命令，配合正则表达式一起，<code>s/REGEX/SUBSITITUTION</code>。</p><p>正则表达式的特殊字符含义如下，* 和 + 默认是贪婪模式，尽可能多的匹配文本，某些情况下可以加上?变成非贪婪模式，但sed并不支持，我们可以切换到perl模式。可以使用<a href="https://regex101.com/" target="_blank" rel="noopener">在线正则表达式调试工具</a>，以及<a href="https://regexone.com/" target="_blank" rel="noopener">在线交互正则表达式教程</a>。使用捕获组（capture groups）的概念保留匹配内容。</p><ul><li><code>.</code>，除空格之外的任意单个字符；</li><li><code>*</code>，匹配前面字符零次或多次；</li><li><code>+</code>，匹配前面字符一次或多次；</li><li><code>[abc]</code>匹配其中任意一个；</li><li><code>（RX1|RX2）</code>，任何能够匹配RX1或RX2的结果；</li><li><code>^</code>, <code>$</code>，行首和行尾</li></ul><p>sed本身是一个比较全能的工具，但是在具体功能上往往能找到更好的工具作为替代。下面有一个使用sed统计数据的实例：</p><pre><code class="hljs bash">ssh myserver journalctl     <span class="hljs-comment">#处理 ssh服务器日志</span> | grep sshd | grep <span class="hljs-string">"Disconnected from"</span>  <span class="hljs-comment"># 获取日志中的用户名</span> | sed -E <span class="hljs-string">'s/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'</span> | sort | uniq -c           <span class="hljs-comment"># sort按字典序排序，uniq -c 将连续出现的行折叠并以出现次数为前缀</span> | sort -nk1,1 | tail -n10  <span class="hljs-comment"># sort -n 按数字顺序排序，-k1，1即基于空格分隔的第一列进行排序，且仅排到第一部分；</span> | awk <span class="hljs-string">'&#123;print $2&#125;'</span> | paste -sd, <span class="hljs-comment"># 对于每行文本打印其第二部分内容，paste合并行（-s）</span></code></pre><p>另外我们还可以对二进制文件做同样的处理，比如用ffmpeg从相机中拿一张图片，转换成灰度图后通过SSH将压缩的文件发送到远端服务器，并进行解压、存档并显示。</p><pre><code class="hljs bash">ffmpeg -loglevel panic -i /dev/video0 -frames 1 -f image2 - | convert - -colorspace gray - | gzip | ssh mymachine <span class="hljs-string">'gzip -d | tee copy.jpg | env DISPLAY=:0 feh -'</span></code></pre><h3 id="二-awk编辑器"><a class="markdownIt-Anchor" href="#二-awk编辑器"></a> 二. awk编辑器</h3><p>awk接受某模式串及代码块，指定当模式匹配时应该做的操作。</p><pre><code class="hljs bash"><span class="hljs-comment"># 匹配文本中所有以c开头，以e结尾且仅尝试过一次登录的用户，输出统计个数</span>awk <span class="hljs-string">'$1 == 1 &amp;&amp; $2 ~ /^c[^ ]*e$/ &#123; print $2 &#125;'</span> | wc -l</code></pre><h3 id="三-分析数据"><a class="markdownIt-Anchor" href="#三-分析数据"></a> 三. 分析数据</h3><p>使用paste，R语言都是可以的。</p><p>另外我们可以使用数据整理来确定参数，配合xargs实现：</p><pre><code class="hljs bash">rustup toolchain list | grep nightly | grep -vE <span class="hljs-string">"nightly-x86"</span> | sed <span class="hljs-string">'s/-x86.*//'</span> | xargs rustup toolchain uninstall</code></pre><h2 id="命令行环境"><a class="markdownIt-Anchor" href="#命令行环境"></a> 命令行环境</h2><h2 id><a class="markdownIt-Anchor" href="#"></a> </h2>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络的局限</title>
    <link href="/2020/12/22/limit-graph/"/>
    <url>/2020/12/22/limit-graph/</url>
    
    <content type="html"><![CDATA[<p>后面计划进行有关图模型攻击方面的研究，学习斯坦福<a href="http://web.stanford.edu/class/cs224w/" target="_blank" rel="noopener">CS224W《图机器学习》</a>a&gt;和大佬Stephan Günnemann教授<a href="https://www.in.tum.de/daml/teaching/mlgs/ " target="_blank" rel="noopener">MLGS课程</a>中“Limitations of GNN”部分，记录如下。</p><p>关键点：</p><ul><li>图同构判断问题：单射，max/mean/sum pooling，WL Test</li><li>对抗攻击：Nettack，离散数据（无法直接梯度下降优化）、双层优化问题、如何对抗（certification）</li><li>Robutness and certification部分</li></ul><h2 id="mlgs"><a class="markdownIt-Anchor" href="#mlgs"></a> MLGS</h2><div align="center">  <img src="/2020/12/22/limit-graph/summary.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="一-表达能力"><a class="markdownIt-Anchor" href="#一-表达能力"></a> 一. 表达能力</h3><h4 id="1-图同构问题"><a class="markdownIt-Anchor" href="#1-图同构问题"></a> 1. 图同构问题</h4><p>如何判断两个图是否在结构上相同？此问题最优解最差时间复杂度呈指数形式。</p><p><strong>WL test</strong>（Weisfeiler-Lehman Test），只能得出“两个图同构或可能同构”的结论。</p><div align="center">  <img src="/2020/12/22/limit-graph/wl.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>在这个问题上GNN无法做到比WL test更好，尤其是它使用了非单射的聚合操作的时候更是无法区分图同构问题。</p><div align="center">  <img src="/2020/12/22/limit-graph/increase.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2-过平滑问题"><a class="markdownIt-Anchor" href="#2-过平滑问题"></a> 2. 过平滑问题</h4><p>随着层数增加GNN的预测结果过于平滑。无穷多层的GNN会导致所有的</p><p>节点得到同样的表征向量，这个向量表达了整个图的结构信息（和PageRank类似）而无法区分局部信息。</p><div align="center">  <img src="/2020/12/22/limit-graph/limit.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>关注一下<strong>PageRank</strong>。在PageRank里我们使用teleport vector进行信息局部化（关注邻居），同理可以应用到GCN场景中，相关工作为<strong>PPNP</strong>（Personalized Propagation of Neural Predictions，2018，建议阅读原文）。将转换与传播操作分开，并加入personalized teleportation，最终将迭代公式修改为：</p><div align="center">  <img src="/2020/12/22/limit-graph/shizi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>PPNP在防止过平滑、计算效率、扩展性等方面有如下优势：</p><div align="center">  <img src="/2020/12/22/limit-graph/ppnp.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-鲁棒性"><a class="markdownIt-Anchor" href="#二-鲁棒性"></a> 二. 鲁棒性</h3><p>有关图数据的对抗可以发生在<strong>节点属性</strong>和图<strong>结构信息</strong>两方面（后者在现实世界中更普遍），进行针对某些节点的<strong>有目标攻击</strong>或进行针对整个图的<strong>全局攻击</strong>。</p><p>图对抗攻击的<strong>挑战</strong>：</p><ul><li>针对离散变量的优化问题；通过非凸的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">L_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>范数计算扰动；</li><li>样本节点间相互关联，不可以单独计算；</li><li>如何定义“难以察觉”的扰动？</li><li>现实中多抽象为投毒攻击（影响训练数据集），抽象为一个双层优化问题。</li></ul><div align="center">  <img src="/2020/12/22/limit-graph/gongshi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div>最早的攻击为Nettack‘2018，目标是影响single node's prediction。关键操作在于首先将分类器线性化（为简化模型去掉了激活函数ReLU），之后通过贪心算法迭代找到最优扰动。<p>如何提升鲁棒性：</p><p>1）启发式防御方法：adjacency low-rank approximaition via truncated Singular Value Decomposition （Entezari 2020）; filtering of malicious edges via attribute similarity（Wu 2019）等，但这些方法在CNN领域已被证明无法应对最差情况的扰动。</p><p>2）鲁棒的训练方法，如 via Projected Gradient Descent（Xu et al，2019，但目前这种通过生成其它图样本的方法效果不是特别好）或者propose with a certification technique（low up bound，这个方面教授发表了很多论文）</p><ul><li>《Certifiable Robustness and Robust Training for Graph Convolutional Networks》</li><li>《[Certifiable robustness of graph convolutional networks under structure perturbations](javascript:void(0))》</li><li>《Certifiable Robustness to Graph Perturbations》</li></ul><p>3）随机平滑（randomized smoothing），如何在离散的图结构信息上加入高斯噪声？将邻接矩阵上的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>个边视为伯努利随机变量，但由于实际中的网络大多比较稀疏，很难找到一个合适的概率参数p。所以我们需要进行sparsity-aware random sampling。（这部分需要更详细得看一下）《Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more》‘ICML 2020</p><p>这方面的问题依然大有可为！（GNN robustness/certification is a highly active research area）</p><h3 id="三-扩展性"><a class="markdownIt-Anchor" href="#三-扩展性"></a> 三. 扩展性</h3><p>消息传递机制下需要同时处理整个网络，节点数据非独立同分布，动态增删节点/边会造成较大影响。</p><h2 id="cs-24w"><a class="markdownIt-Anchor" href="#cs-24w"></a> CS 24W</h2><h3 id="一-capture-graph-structure"><a class="markdownIt-Anchor" href="#一-capture-graph-structure"></a> 一. Capture graph structure</h3><p>Graph Isomorphism（图同构问题），邻居节点聚合函数（mean，max）并不单射。提出GIN（Graph Isomorphism Network），使用sum pooling。GIN可以更好地把握图结构信息，对于图分类问题表现更优秀，尤其是当网络中没有节点属性信息时。</p><p>GIN的思想与WL测试法近似。WL可以解决实际中的绝大多数图同构判断问题，但有一些例外，比如下面的例子：</p><div align="center">  <img src="/2020/12/22/limit-graph/except.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-vulnerability-of-gnns-to-noise-in-graph-data"><a class="markdownIt-Anchor" href="#二-vulnerability-of-gnns-to-noise-in-graph-data"></a> 二. Vulnerability of GNNs to noise in graph data</h3><p>以图上半监督节点分类问题为例，重点介绍了KDD18上Stephan Günnemann的工作，第一次提出该问题的数学模型并解答。解如下优化问题有两个难点：1）离散数据难以使用梯度下降；2）该问题为双层优化问题，如果使用迭代求解，每一步重新训练GNN非常耗时。作者为了保证高效，使用了很多启发式近似方法，比如贪心地一步步进行图修改，删除GCN中的ReLU激活函数进行简化等。（更多细节可以直接看论文，Adversarial Attacks on Neural Networks for Graph Data，PPT也做的很赞）。</p><div align="center">  <img src="/2020/12/22/limit-graph/attack.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><div align="center">  <img src="/2020/12/22/limit-graph/math.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><h3 id="三-challenges-and-future"><a class="markdownIt-Anchor" href="#三-challenges-and-future"></a> 三. Challenges and Future</h3><p>带标签数据集不容易获得（这是整个ML领域的问题），数据集不足又比较容易出现过拟合问题。为解决这个问题，提出Pre-training GNNs [Hu+ 2019]，先在某些相关数据集上训练之后，遇到真实任务再进行finetune。</p><p>如何防御上述类型对抗攻击？</p><p>攻击过程中如何在离散数据上找到最优解？</p><p>如何在准确性和鲁棒性之间找到最佳平衡？</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅2020——半监督、无监督</title>
    <link href="/2020/12/22/lhy-semi/"/>
    <url>/2020/12/22/lhy-semi/</url>
    
    <content type="html"><![CDATA[<h2 id="半监督"><a class="markdownIt-Anchor" href="#半监督"></a> 半监督</h2><p>semi-supervised，有另外一组无标签数据集且数量远大于带标签集合。分为Transductive learning（使用了testing set的属性/信息）和Inductive learning（训练时不考虑testing set）。</p><h3 id="一-半监督生成模型"><a class="markdownIt-Anchor" href="#一-半监督生成模型"></a> 一. 半监督生成模型</h3><p>在生成模型上的做法：1）初始化/随机化/估测参数值（均值、方差等）；2）计算几率；3）更新模型参数。</p><p>理论上这个方法会收敛，但是初始值会影响结果。<strong>EM算法</strong></p><div align="center">  <img src="/2020/12/22/lhy-semi/generate.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="二-low-density-seperation"><a class="markdownIt-Anchor" href="#二-low-density-seperation"></a> 二. low density seperation</h3><p>假设两个分类样本之间存在一条“鸿沟”，在两类的交界处样本密度很小。最常见的是<strong>self-training</strong>。首先基于带标签集合训练一个模型，将其应用于无标签集合得到它们的**“伪标签”**，根据某些规则/也可以加入权重，从无标签集合中拿一部分视为带标签集合样本，重新训练样本。<strong>这一招在regression上是不能用的</strong>。</p><p>唯一的差别在于self-training中我们使用了hard label，上面介绍的生成模型中使用了soft label。在DNN的self training是一定要使用hard label的，否则其实加入无标签样本并没有什么影响。</p><p><strong>Entropy-based regularization</strong>，模型输出的结果信息熵要比较大才好。设计损失函数的时候，在带标签数据集上希望代价函数小，在无标签数据集上希望输出结果信息熵小。</p><p><strong>semi-supervised SVM</strong>（ICML, 1999），穷举所有无标签样本的label可能性，对每一种都算一个SVM模型，找出哪一种可能性使margin最大且分类错误最小。</p><h3 id="三-smoothness-assumption"><a class="markdownIt-Anchor" href="#三-smoothness-assumption"></a> 三. Smoothness Assumption</h3><p>X的分布不平均，如果样本<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">x_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">x_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>通过某<strong>高密度区域</strong>而比较相似的话，则两样本标签大概率相同。</p><div align="center">  <img src="/2020/12/22/lhy-semi/similar.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>最简单的实现方法就是，聚类后标记，但其实这个聚类的过程并不容易。</p><p>另一种方法是引入<strong>图模型</strong>，使用graph上的边表示样本间的联系（相似）。**如何构建一个graph？<strong>比如网页超链接、论文引用等。通常会定义如何计算两个样本间的相似度；之后可以使用各种方式添加边，比如K近邻邻居、e-Neighborhood（加阈值）、也可以给边加入与样本相似度成正比的权重、Gaussian Radial Basis方法</strong>（RBF，指数处理）**也比较常用。**如何定量描述图上标签的smoothness？**常用方法有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>=</mo><mn>1</mn><mi mathvariant="normal">/</mi><mn>2</mn><msub><mo>∑</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><msub><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><msup><mi>y</mi><mi>i</mi></msup><mo>−</mo><msup><mi>y</mi><mi>j</mi></msup><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S=1/2\sum_{i,j}w_{i,j}(y^i-y^j)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.260482em;vertical-align:-0.43581800000000004em;"></span><span class="mord">1</span><span class="mord">/</span><span class="mord">2</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.0746639999999998em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>，可以简化为如下矩阵的形式（又出现了图拉普拉斯矩阵概念）。最后在设计损失函数的时候，带权重地加上这一项，也可以叫做smoothness正则项。</p><div align="center">  <img src="/2020/12/22/lhy-semi/lap.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="无监督"><a class="markdownIt-Anchor" href="#无监督"></a> 无监督</h2><h3 id="一-word-embedding"><a class="markdownIt-Anchor" href="#一-word-embedding"></a> 一. Word Embedding</h3><p>如何在词的表征向量中考虑到上下文信息，分为<strong>count based</strong>和<strong>predition based</strong>方法。前者假设如果两个词经常同时出现，则其表征向量距离会更近，比如Glove Vector；后者是通过当前词预测下一个词，一般会扩展到N个词汇。过程中坚持让不同词对应的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>,<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">w_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>相同，之后可以通过将one-hot embedding vector和W相乘得到对应词向量。</p><p>prediction-based常用模型包括CBOW（Continuous bag of word model）和Skip-gram。</p><div align="center">  <img src="/2020/12/22/lhy-semi/model.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>扩展领域工作包括：Document Embedding（beyond bag of word）、multi-lingual Embedding（把不同语言的词向量拉在一起）、multi-domain Embedding（图片与文字，可以处理未见过的文字）</p><h3 id="二-linear-method-for-dimension-reduction-pca"><a class="markdownIt-Anchor" href="#二-linear-method-for-dimension-reduction-pca"></a> 二. Linear Method for Dimension Reduction (PCA)</h3><p>clustering，化繁为简。常用方法有K-Means（直接指定簇数目）、HAC（建树、通过设阈值决定簇数目）。</p><div align="center">  <img src="/2020/12/22/lhy-semi/kmeans.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/12/22/lhy-semi/hac.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>Distributed Representation，考虑到聚类过于硬核（样本必须在某一类中）。</p><p>常用于降维的方法是特征选择、主成分分析（PCA）等。</p><p><strong>PCA</strong>中希望可以找到投影结果分布更加分散（large variance）的方向<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span>，而且各个方向间正交，一共找几个方向自己决定。PCA可以用<strong>拉格朗日乘子解</strong>，也可以用DNN解。最终找到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span>是协方差矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span></span></span></span>中与最大特征值对应的特征向量。找<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>w</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">w^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>要加上与<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>w</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">w^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>正交的限制条件，最终得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>w</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">w^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>是协方差矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span></span></span></span>与第二大特征值对应的特征向量。</p><div align="center">  <img src="/2020/12/22/lhy-semi/pca1.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>另一个角度理解PCA，可以通过SVD进行矩阵分解。</p><div align="center">  <img src="/2020/12/22/lhy-semi/svd.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>PCA可以视为只有一层隐藏层的神经网络（线性激活函数），于是增加层数可以发展到<strong>AutoEncoder</strong>。如果只是一层线性的话，还是直接用SVD解比较好，使用梯度下降并不一定得到最优解。</p><p>PCA有一些缺点，为解决这些问题出现<strong>LDA（考虑标签信息）</strong>；而且PCA是线性的，比如无法拉直S形的曲面；NMF（Non-negative matrix factorization）要求每一个维度值都是正值，比如在MNIST上可以看出基本都是笔画；<strong>矩阵分解</strong>（Matrix Factorization），常用于推荐系统中。</p><h3 id="三-neighbor-embedding-tsne"><a class="markdownIt-Anchor" href="#三-neighbor-embedding-tsne"></a> 三. Neighbor Embedding (TSNE)</h3><p>Manifold Learning,</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅2020——深度学习【其它】</title>
    <link href="/2020/12/21/lhy-advanced/"/>
    <url>/2020/12/21/lhy-advanced/</url>
    
    <content type="html"><![CDATA[<p>包括可解释性、对抗学习、网络压缩等深度学习相关其它方面。</p><h2 id="可解释ai"><a class="markdownIt-Anchor" href="#可解释ai"></a> 可解释AI</h2><p>目标在于看出机器学到了什么，其实人类也是黑盒，也许最终要ML会针对不同的人给出不同的解释（这个想法和在绿盟实习时好像？？）。关于可解释性、对抗方面的研究也会为进一步提升模型的性能提供方向。</p><p>线性模型容易解释，但是能力太差。DNN虽然是黑盒，但能力非常强大，所以不要“削足适履”而是要加强DNN的可解释性。有没有一种既可以解释而且能力又强的呢？比如决策树、随机森林。但在某些复杂问题下，这些模型也非常难以解释。</p><p>重点是要把这个领域放到网络安全方向！！！</p><h3 id="一-saliency-map"><a class="markdownIt-Anchor" href="#一-saliency-map"></a> 一. Saliency Map</h3><p>可解释性AI背后的观念其实很简单，即改动某些components，如果对结果造成了大的改动，那么这些components就是非常重要。实现方式可以通过比如灰色块遮挡、加入噪声扰动计算结果扰动的偏微分（画出<strong>Saliency Map</strong>，相关工作如Grad-CAM、SmoothGrad、Layer-wise Relevance Propagation、Guided Backpropagation）等。</p><p>但这种基于梯度的方法可能会受限于gradient satruation问题，比如在某个范围内大象鼻子的长度对识别是否为大象的偏微分为0，但并不代表影响很小。解决这个问题的相关工作有Integrated gradient和DeepLIFT等。</p><p>由local attribution转化为<strong>Global attribution</strong>的思想，关注输入和权重共同结果的作用。相关工作有Layer-wise relecance propagation（LRP），有点像自己写background。DeepLIFT提出了<strong>灵活的baseline</strong>（不是从0开始），另一种用灵活baseline的是Integrated gradient。</p><p>目前有很多不同的<strong>归因方法（attribution method）</strong>。<strong>Sensitivity-n</strong>用于评估各种归因方法到底有多好。</p><h3 id="二-攻击推断"><a class="markdownIt-Anchor" href="#二-攻击推断"></a> 二. 攻击推断</h3><p>可以对机器的推断结果进行攻击，比如加入一些噪声，导致产生错误的推断。相关论文为，<a href="https://arxiv.org/abs/1710.10547" target="_blank" rel="noopener">https://arxiv.org/abs/1710.10547</a></p><h3 id="三-生成器正则项"><a class="markdownIt-Anchor" href="#三-生成器正则项"></a> 三. 生成器正则项</h3><p>在设计损失函数的时候加入一些正则项表某些限制，让得到的结果不是那么随机。</p><div align="center">  <img src="/2020/12/21/lhy-advanced/regu.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>GAN、VAE等结构是通过Generator（某个神经网络）加入这些限制，比如要求输入是人可以看懂的图片。</p><div align="center">  <img src="/2020/12/21/lhy-advanced/gan.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="四-解释其它模型"><a class="markdownIt-Anchor" href="#四-解释其它模型"></a> 四. 解释其它模型</h3><p>用一个比较具有可解释性的模型来模仿另一个模型。比如使用线性模型模拟某个黑盒模型的局部（线性模型的能力无法完全模拟），<strong>LIME技术</strong>。LIME关键步骤：1）选择需要解释的数值点；2）采样其附近的点（如何定义附近，会带来不同的结果）3）训练一个线性模型或其它可解释性模型；4）解释这个线性模型。这个技术有很多参数可以调。</p><p>使用决策树模仿，在训练原模型的时候，损失函数里就加入有关决策树复杂度的惩罚项。有工作训练了一个DNN用于计算决策树的复杂度，从而解决这个惩罚项不可微的问题。</p><h3 id="五-选学部分"><a class="markdownIt-Anchor" href="#五-选学部分"></a> 五. 选学部分</h3><p>使用Probing方法完成对Bert模型的解释。</p><p>这个部分没有听懂。。。。。</p><h2 id="ml模型攻击与防御"><a class="markdownIt-Anchor" href="#ml模型攻击与防御"></a> ML模型攻击与防御</h2><h3 id="一-基础"><a class="markdownIt-Anchor" href="#一-基础"></a> 一. 基础</h3><p>参数固定，找到使输出与理想结果差别最大的输入。计算constraint一般使用L2范数或L-infinity（取最大差异值）。</p><div align="center">  <img src="/2020/12/21/lhy-advanced/attack.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>解决上述带限制的优化问题，需要一种改进的梯度下降方法。在限制范围内找与计算出的理想x最接近的一个。</p><div align="center">  <img src="/2020/12/21/lhy-advanced/constraint.jpg" srcset="/img/loading.gif" width="20%" height="20%" alt="oauth"></div><p>相关方法有很多很多，它们的不同之处主要是：1）不同的约束条件设置方法；2）不同的优化方法。</p><ul><li>FGSM (<a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">https://arxiv.org/abs/1412.6572</a>) ，只在意方向，指定了一个挺大的学习率，只进行一步操作；</li><li>Basic iterative method (<a href="https://arxiv.org/abs/1607.02533" target="_blank" rel="noopener">https://arxiv.org/abs/1607.02533</a>)</li><li>L-BFGS (<a href="https://arxiv.org/abs/1312.6199" target="_blank" rel="noopener">https://arxiv.org/abs/1312.6199</a>)</li><li>Deepfool (<a href="https://arxiv.org/abs/1511.04599" target="_blank" rel="noopener">https://arxiv.org/abs/1511.04599</a>)</li><li>JSMA (<a href="https://arxiv.org/abs/1511.07528" target="_blank" rel="noopener">https://arxiv.org/abs/1511.07528</a>)</li><li>C&amp;W (<a href="https://arxiv.org/abs/1608.04644" target="_blank" rel="noopener">https://arxiv.org/abs/1608.04644</a>)</li><li>Elastic net attack (<a href="https://arxiv.org/abs/1709.04114" target="_blank" rel="noopener">https://arxiv.org/abs/1709.04114</a>)</li><li>Spatially Transformed (<a href="https://arxiv.org/abs/1801.02612" target="_blank" rel="noopener">https://arxiv.org/abs/1801.02612</a>)</li><li>One Pixel Attack (<a href="https://arxiv.org/abs/1710.08864" target="_blank" rel="noopener">https://arxiv.org/abs/1710.08864</a>)</li></ul><h4 id="1-图像攻击"><a class="markdownIt-Anchor" href="#1-图像攻击"></a> 1. 图像攻击</h4><p><strong>One Pixel Attack</strong>，只修改样本图片上的某一个像素点。</p><div align="center">  <img src="/2020/12/21/lhy-advanced/one.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>我们也不一定需要那个最好的像素点的值，只要攻击成功就好了。可以使用<strong>差分进化方法</strong>找到一个足够好的解就可以。差分演化方法的关键在于如何生成新的“孩子”，以及如何筛选。差分演化法（<a href="https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/%EF%BC%89%E3%80%82%E5%BA%94%E7%94%A8%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E8%BE%93%E5%85%A5%E6%98%AF%EF%BC%88x" target="_blank" rel="noopener">https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/）。应用的时候，输入是（x</a>, y, R, G,B）</p><h4 id="2-语音攻击"><a class="markdownIt-Anchor" href="#2-语音攻击"></a> 2. 语音攻击</h4><p>攻击ASR（auto speech recognition，语音转文字），方法可以和FGSM类似。</p><p>攻击ASV（说话人识别），汉堡王广告实例。</p><p>Hidden Voice Attack，人听不出来，但是ML模型可以，实现攻击。</p><div align="center">  <img src="/2020/12/21/lhy-advanced/signal.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>Perturbation分为四种：Time Domain Inversion(TDI)、Random Phase Generation(RPG)、High Frequency Addition(HFA)和Time Scaling(TS)。</p><div align="center">  <img src="/2020/12/21/lhy-advanced/tdi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-黑盒白盒"><a class="markdownIt-Anchor" href="#二-黑盒白盒"></a> 二. 黑盒白盒</h3><p>保护好<strong>网络参数</strong>，那么如何攻击嘞？这就是黑盒攻击。一般是要训练一个代理网络proxy network。所以就是还要保护好<strong>训练资料</strong>。但是如果是线上提供服务的，可以通过测试API积累出训练资料。</p><p>Universal Adversarial Attack（<a href="https://arxiv.org/abs/1610.08401%EF%BC%89%E6%89%BE%E5%88%B0%E5%9C%A8%E6%89%80%E6%9C%89%E8%AE%AD%E7%BB%83%E8%B5%84%E6%96%99%E9%83%BD%E6%88%90%E5%8A%9F%E7%9A%84%E6%94%BB%E5%87%BB%E5%99%AA%E5%A3%B0%E3%80%82%E3%80%8AUniversal" target="_blank" rel="noopener">https://arxiv.org/abs/1610.08401）找到在所有训练资料都成功的攻击噪声。《Universal</a> adversarial perturbations》</p><p>Adversarial Reprogramming，完全改掉了原有网络模型的功能。《Adversarial Reprogramming of Neural Networks》</p><h3 id="三-防御手段"><a class="markdownIt-Anchor" href="#三-防御手段"></a> 三. 防御手段</h3><p>被动防御手段，比如加入一个filter、feature squeeze（如果squeeze前后判断结果差很多，则判断可能已经被攻击过了）、加入随机缩放或padding操作（但是不要太过分，不然都训练不出来了）。</p><p>主动防御方式，在训练过程中就找出漏洞并补起来。对抗学习（主动找出每个样本的对抗样本），增加样本集。但这类方法受限于防御者可以用到的攻击方法。</p><p>防御目前仍然是一个尚待解决的问题。</p><h2 id="网络压缩"><a class="markdownIt-Anchor" href="#网络压缩"></a> 网络压缩</h2><h2 id="异常检测"><a class="markdownIt-Anchor" href="#异常检测"></a> 异常检测</h2><h2 id="元学习"><a class="markdownIt-Anchor" href="#元学习"></a> 元学习</h2><h2 id="终生学习"><a class="markdownIt-Anchor" href="#终生学习"></a> 终生学习</h2>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch笔记</title>
    <link href="/2020/12/16/pytorch-book/"/>
    <url>/2020/12/16/pytorch-book/</url>
    
    <content type="html"><![CDATA[<p>网上有很多可供学习的资料，尤其官网资料已经足够好了，<a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">60分钟入门（原版）</a>，<a href="http://pytorch123.com/" target="_blank" rel="noopener">中文版</a>，以及<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/" target="_blank" rel="noopener">Dive into deeplearning（Pytorch版）</a>。</p><h2 id="基础"><a class="markdownIt-Anchor" href="#基础"></a> 基础</h2><h3 id="一-自动微分"><a class="markdownIt-Anchor" href="#一-自动微分"></a> 一. 自动微分</h3><p>张量tensor，类似于Numpy的ndarrays，但它提供GPU计算和自动求梯度等功能。Tensor支持<strong>索引操作</strong>，但索引出来的结果与原数据共享内存，除了索引之外pytorch还提供了很多高级的选择函数。view() 改变张量形状，但<strong>与原有tensor共享数据</strong>，内存不变，只改变了观察角度。如果想要一个真正的副本，推荐使<strong>用clone创建</strong>，这样创建的副本还会被记录在计算图中。</p><ul><li><strong>autograd包</strong>是PyTorch中所有神经网络的核心，为Tensors上所有操作提供自动微分，是一个由运行定义的框架。关键操作如下：<br>.requires_grad设置为True，会开始跟踪针对tensor的所有操作；</li><li>完成计算后，可以调用.backward（）自动计算所有梯度，结果将累积到.grad属性中；</li><li>停止追踪可以调用.detach（），与计算记录分离，或者可以用with torch.no_grad（）把代码包装起来；</li></ul><p>另外PyTorch中支持一些线性函数，比如trace（对角线元素之和）、diag（对角线元素）、triu/tril（矩阵的上三角/下三角）、mm/bmm（矩阵乘法，batch的矩阵乘法）、转置、求逆、奇异值分解等等，<strong>注意看官网，避免重复造轮子。</strong></p><h3 id="二-神经网络"><a class="markdownIt-Anchor" href="#二-神经网络"></a> 二. 神经网络</h3><p>一个nn.Module包括层和一个方法forward(input)，它会返回输出output。</p><p>可以通过调用net.parameters（）返回模型的可训练参数。</p><p>优化器可以直接调用torch.optim包，关键在于.zero_grad（）和.step（）两步。</p><h3 id="三-图像分类器"><a class="markdownIt-Anchor" href="#三-图像分类器"></a> 三. 图像分类器</h3><p>处理图像、文本、语音或视频数据时，可以使用标准python包将数据加载成numpy格式，然后转换成torch.*Tensor。图像可以用Pillow，OpenCV，语音可以用scipy、librosa，文本可以直接用python或cpython，NLTK，SpaCy等。特别对于视觉，已经有torchvision的包，包含公共数据及加载及转换模块。</p><p>计算每一类的测试准确率；将神经网络转移到GPU上，device = torch.device(“cuda:0”)   <a href="http://model.to" target="_blank" rel="noopener">model.to</a>(device)。</p><h3 id="四-数据并行处理"><a class="markdownIt-Anchor" href="#四-数据并行处理"></a> 四. 数据并行处理</h3><p>Pytorch上默认使用一个GPU，使用DataParallel可以让模型并行运行，<strong>model = nn.DataParallel(model)</strong>。数据并行自动拆分并发送到多个GPU上，每个模型完成任务后，DataParallel收集并合并这些结果，之后返回。</p><pre><code class="hljs python">model = Model(input_size, output_size)<span class="hljs-keyword">if</span> torch.cuda.device_count() &gt; <span class="hljs-number">1</span>:  print(<span class="hljs-string">"Let's use"</span>, torch.cuda.device_count(), <span class="hljs-string">"GPUs!"</span>)  <span class="hljs-comment"># dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span>  model = nn.DataParallel(model)model.to(device)<span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> rand_loader:    input = data.to(device)    output = model(input)    print(<span class="hljs-string">"Outside: input size"</span>, input.size(),          <span class="hljs-string">"output_size"</span>, output.size())</code></pre>]]></content>
    
    
    <categories>
      
      <category>工具教程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅2020——深度学习【基础】</title>
    <link href="/2020/12/16/DL-lhy/"/>
    <url>/2020/12/16/DL-lhy/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a class="markdownIt-Anchor" href="#背景"></a> 背景</h2><h3 id="一-历史发展"><a class="markdownIt-Anchor" href="#一-历史发展"></a> 一. 历史发展</h3><p>深度学习所基于的神经网络模型和用数据编程的核心思想实际上已经被研究了数百年。</p><ul><li><p>1958年感知机模型被提出；1969年提出感知机模型无法解决非线性问题。</p></li><li><p>1980s提出多层感知机尝试解决非线性问题，其实与今天的DNN没有特别明显的差别；</p></li><li><p>1986年提出反向传播技术，但在超过三层的模型中就无法训练出好的结果；1989年有人提出只需要一层就可以模拟任何函数，加入足够神经元即可，不需要Deep；（那个年代NN非常之不吃香，人们开始尝试…改名）</p></li><li><p>1995年到2005年，大部分机器学习研究者不再关注神经网络，首先由于算力不足，其次因为当时的数据集体量较小，从经验上来说，使用如核方法、决策树和概率图模型等统计工具效果更好，而且不需要那么长的训练时间。</p></li><li><p>2006年，RBM initialization（受限玻尔兹曼机），非常复杂，人们在一段时间里认为这是一个突破，但目前很少在有使用，因为带来的帮助有限。但它让大家再次对NN领域有了兴趣。</p></li><li><p>2009年使用GPU加速；2011年开始在语音识别领域流行；2012年赢得了图像领域的比赛。</p></li></ul><p>当前深度学习已渗入机器人学、物流管理、计算生物学、粒子物理学和天文学等领域。</p><p>总结近十年来深度学习长足发展的部分原因：</p><ul><li>优秀的容量控制方法，比如dropout、随机权重等；</li><li>注意力机制，解决了如何在不增加参数的情况下扩展系统的记忆容量和复杂度；</li><li>记忆网络和神经编码器-解释器这样的设计允许针对推理过程的迭代建模；</li><li>对抗生成网络，将原来的采样部分替换成了任意含有可微分参数的算法，而且优势在于可以用任意算法来生成输出；</li><li>分布式并行训练算法，极大提升模型训练速度，同时为强化学习的发展做出了贡献；</li><li>有很多易用的深度学习框架；</li></ul><h3 id="二-基础"><a class="markdownIt-Anchor" href="#二-基础"></a> 二. 基础</h3><p>机器学习是人工智能的分支，在其众多研究方向中，<strong>表征学习</strong>关注如何自动找出表示数据的合适方式，深度学习就是具有<strong>多级表示的表征学习方法</strong>，可以逐级表示越来越抽象的概念或模式，表现在外在特点就是端到端的训练。相对其它经典的机器学习方法，深度学习的不同在于：对非最优解的包容、对非凸非线性优化的使用以及用于尝试没有被证明过的方法。</p><p>给定了<strong>网络架构</strong>（连接方法，神经元数目，层数），其实就是定义了一个函数集合，我们希望通过不同的w和b找到其中最优的那个函数。</p><p>不同网络架构的区别在于神经元的不同连接方式，常用的有全连接网络。</p><p>深度神经网络它就是很深。。。。特别深的网络需要比较特殊的网络结构，全连接网络太深的话很难训练起来。</p><div align="center">  <img src="/2020/12/16/DL-lhy/deep.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>DNN其实就是一连串矩阵预算（输入向量乘权重矩阵再加上偏量），可以使用GPU进行矩阵运算的加速。DNN的隐藏层可以全部视为特征工程部分，最后输出层视为一个多分类器（加一个softmax）。</p><div align="center">  <img src="/2020/12/16/DL-lhy/matrix.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="1-是否要选择使用dnn"><a class="markdownIt-Anchor" href="#1-是否要选择使用dnn"></a> 1. 是否要选择使用DNN？</h4><p>原本不是DNN的模型，我们需要<strong>找到恰当的特征工程方法</strong>，使用DNN可以直接丢过去，但<strong>问题转化为如何设计合适的DNN网络结构</strong>。我们可以考虑这两个任务哪个更容易解决，来决定是否选择使用深度学习方法。比如在CV或语音识别方面，DNN的效果非常强，但在NLP领域优势没有那么大。</p><h4 id="2-是否可以自动决定网络架构"><a class="markdownIt-Anchor" href="#2-是否可以自动决定网络架构"></a> 2. 是否可以自动决定网络架构？</h4><p>有相关研究（Evolutionary Artificial Neural Networks），但是目前没有被广泛应用起来。</p><p>大家通常选择使用CNN，RNN等架构。</p><h4 id="3-为什么要深度学习"><a class="markdownIt-Anchor" href="#3-为什么要深度学习"></a> 3. 为什么要深度学习？</h4><p>只要有足够多的神经元，一层网络就可以近似所有的函数（宽度学习 &gt;__&lt;）</p><p>“Deeper is Better”？会存在梯度消失的问题，比如损失函数中使用Sigmoid，经过链式法则，靠近输出层的地方有比较大的梯度值更新很快，靠近输入层则相反，导致几乎在已经收敛完成时，靠近输入层还是随机参数状态，陷入局部最小值。</p><div align="center">  <img src="/2020/12/16/DL-lhy/vanish.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div>同等量级参数的情况下，矮胖model和瘦长model哪个好？实验结果表明是后者好。<p>DL更有效率，只需要比较少的神经元，意味着比较少的参数，所以用比较少的数据量就可以运算。模组化、逻辑电路、剪窗花的例子。我们没有足够的训练数据所以比较需要Deep learning（这和传统想法不同啊）。</p><p>在语音识别领域，DL逐渐取代以往《信号与系统》等领域的某些步骤（DCT, log, Filter bank等），google甚至尝试从第二步（DFT）开始就直接使用端到端的深度学习网络，最终效果是与传统方法打平。</p><div align="center">  <img src="/2020/12/16/DL-lhy/speech.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="三-反向传播"><a class="markdownIt-Anchor" href="#三-反向传播"></a> 三. 反向传播</h3><p>DNN里依然是使用<strong>梯度下降</strong>法找最优参数，很多框架都实现了<strong>反向传播</strong>技术，即一种在神经网络中更<strong>有效率</strong>计算偏微分的方法，可以直接使用。大多数深度学习“专家”并不会计算微分。</p><p>最大的问题是DNN中有过多参数，如何有效计算？反向传播就是一种比较有效率的方法，其关键点是<strong>链式法则</strong>。</p><p>两个关键步骤就是：Forward Pass 之后进行 Backword Pass。</p><div align="center">  <img src="/2020/12/16/DL-lhy/back.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h2 id="训练tips"><a class="markdownIt-Anchor" href="#训练tips"></a> 训练Tips</h2><h3 id="一-问题区分"><a class="markdownIt-Anchor" href="#一-问题区分"></a> 一. 问题区分</h3><p>首先检查<strong>在训练集</strong>上有没有得到好的结果（有没有训练起来），之后再看测试集（不要直接上）。</p><p>另外，不要看到所有不好的效果，都说是因为overfitting，你需要先检查训练集上的效果，如果测试集比训练集差才是<strong>过拟合</strong>。也不是叫欠拟合，因为有的时候模型的能力其实是够的，但是因为局部最优解等问题而没有训练好。李宏毅老师认为严格的“欠拟合”概念是模型参数不够多而导致它本身没有能力解决这个问题。</p><div align="center">  <img src="/2020/12/16/DL-lhy/recept.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>针对在<strong>训练集效果不好</strong>和在<strong>测试集效果不好</strong>这两个问题，有不同的处理方法，这个必须要清楚。</p><div align="center">  <img src="/2020/12/16/DL-lhy/recipt.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="1-新的激活函数"><a class="markdownIt-Anchor" href="#1-新的激活函数"></a> 1. 新的激活函数</h4><p>梯度消失、Sigmoid</p><p>ReLU、Leaky ReLu、Paramatic ReLu、Maxout</p><p>早年的解决方法是依次（或分批次）训练不同层。目前常用的方法是更换激活函数，由Sigmoid换成了<strong>ReLU</strong>。ReLU的优势在于：1）计算迅速；2）有生物学基础；3）等同于无穷多带有不同bias的sigmoid函数叠加的结果；4）可以弱化梯度消失问题。</p><p>使用的时候非常粗暴地解决了ReLU在0点无法微分的事情，直接忽略…也有一些变种，比如Leaky ReLU（小于零部分加入0.01的系数），Parametric ReLU（小于零部分加入可学习的系数）、<strong>Maxout</strong>（自动学习激活函数，可以学出ReLU，也可以学出其它形状的激活函数）等。</p><p>Maxout把多个神经元的参数视为一组，然后进行一个类似maxpooling的操作。max操作无法微分，那么maxout如何训练呢？其实对于被max操作选中的参数，模型和线性模型是一样的，其它参数直接拿掉就可以（类似一个开关），每一次有不同的输入时，z的大小会变化，被训练到的参数是不一样的，最终其实几乎全部都可以被训练到。</p><div align="center">  <img src="/2020/12/16/DL-lhy/train.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2-可调整的学习率"><a class="markdownIt-Anchor" href="#2-可调整的学习率"></a> 2. 可调整的学习率</h4><p>DNN的损失函数更加复杂，需要更灵活的学习率，只是用Adagrad可能无法满足。人们提出RMSProp（在Hinton的线上课程上提出来，没有paper）。RMSProp + Momentum形成Adam。</p><h4 id="3-早停"><a class="markdownIt-Anchor" href="#3-早停"></a> 3. 早停</h4><p>在DeepLearning中，正则化和早停的效果差不多，正则化的作用没有像在SVM等方法中这么厉害。</p><h4 id="4-正则化"><a class="markdownIt-Anchor" href="#4-正则化"></a> 4. 正则化</h4><p>正则化一般不考虑bias项，因为它和使函数更平滑没有关系。</p><p>L1和L2略有不同，L1每次减掉一个（固定的）值，而L2是通过乘法来做的。L1训练出来的结果比较稀疏，L2的话权重平均都比较小。</p><h4 id="5-dropout"><a class="markdownIt-Anchor" href="#5-dropout"></a> 5. Dropout</h4><p>每次都训练一个比较瘦长的网络架构，每次训练的网络是不一样的。在testing的阶段需要在参数上乘以dropout rate。</p><p>可以通过ensemble的角度理解dropout的效果。</p><p>感觉DL领域中的甚多地方都是偏工程或理解直觉的，无法通过严谨的理论推理。</p><h2 id="cnn"><a class="markdownIt-Anchor" href="#cnn"></a> ## CNN</h2><p>CNN的关键点是简化网络架构，如果用全连接网路的话，参数过多。可以做到这样的简化是因为：1）神经元为了发现图片中的某种模式并不需要整张图，只需要局部信息即可；2）同种模式可能出现在图片中的不同区域；3）subsampling（下采样）一些像素可能对整个图片没有太大影响。</p><div align="center">  <img src="/2020/12/16/DL-lhy/cnn.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>CNN可以视为将全连接层拿掉一些权重，即没有考虑全部输入信息。</p><p>图片分析中，每一个filter是同时考虑所有channel的，但也有相应的参数。CNN实现的过程中需要将vector搞成高维tensor，另外要考虑filter的大小及个数。</p><p>如何了解CNN模型学到了什么？将模型参数固定，通过梯度上升方法寻找使模型激活结果最大的输入（有一些可解释性AI的意思），如下图所示。选取<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub><mi>j</mi></mrow><annotation encoding="application/x-tex">a_ij</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span>不同的位置可以解释不同层的作用。比如，通过放在输出层有研究工作发现，CNN真正学到的东西和人类的想象不同，《Deep Neural Networks are Easily Fooled》。另外我们也可以通过调整最终的Loss函数，对输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>加入一些限制，使最终找到的CNN理想输入更符合人类的想象。</p><div align="center">  <img src="/2020/12/16/DL-lhy/what_learned.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>找理想输入/可解释性的部分，感觉可以有挺多应用，比如是否可以在恶意软件检测的部分自动抽取Yara规则。</p><p>**什么时候可以用CNN呢？**场景需要具有如下特性：1）局部信息（远小于全局信息）可以反映出某些模式（pattern），比如围棋中的落子模式；2）同样的模式会出现在不同区域，但代表了同样的意义；3）下采样并不会改变整个样本（比如图像识别），这是max pooling的感性基础，但这一条在alpho go那里为什么成立呢？看文章描述alpho go中应该是没有用到pooling。<strong>在应用时一定要考虑场景特性</strong></p><h2 id="gnn视为cnn的扩展"><a class="markdownIt-Anchor" href="#gnn视为cnn的扩展"></a> GNN（视为CNN的扩展）</h2><p>总的路线图如下，分为spatial-based和spectral-based两种，课程中首先讲了前者。GNN中常用的benchmark dataset是CORA, TU-MUTAG。常用任务为：图分类（真的是图像分类，在MNIST和CIFAR10上superpixel）；回归（ZINC）；节点分类（graph pattern recognition、semi-supervised graph clustering）；边分类（旅行商问题，TSP）；</p><div align="center">  <img src="/2020/12/16/DL-lhy/GNN.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="一-spatial-based-gnn"><a class="markdownIt-Anchor" href="#一-spatial-based-gnn"></a> 一. Spatial-based GNN</h3><p>基础操作有Aggregate（等价Convolution）和Readout（代表整个图）。</p><h3 id="二-spectral-based-gnn"><a class="markdownIt-Anchor" href="#二-spectral-based-gnn"></a> 二. Spectral-based GNN</h3><p>从信号与系统过来，经过傅里叶变换之后，卷积操作变成乘法操作即可。</p><p>合成分析，信号可以视为N维空间的向量，我们常用cos(x)或sin(x)作为bases。</p><div align="center">  <img src="/2020/12/16/DL-lhy/f.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>Graph Laplacian， L =  D - A，即度矩阵减去邻接矩阵，最后L是一个半正定矩阵，通过谱分解后，计算出特征值和特征向量。</p><div align="center">  <img src="/2020/12/16/DL-lhy/spectral.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>频率越大，相邻两点间的信号量变化越大。在图上的操作，某种程度上代表了某个节点与其相邻节点信号能量差，由此来量化频率的大小。所以特征值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span>表示频率变化。</p><p>如何在图上将信号在数值与频率意义上的相互转换。</p><div align="center">  <img src="/2020/12/16/DL-lhy/s1.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><div align="center">  <img src="/2020/12/16/DL-lhy/s2.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>在频率意义上如何进行过滤？有问题：1）滤波函数参数与输入节点数呈正比；2）学习滤波器的时候可能了一些不希望学习到的东西，并不是localized的，多次方会考虑多步邻居。</p><p>ChebNet，主打速度快而且可以局部化。选择拉普拉斯多项式函数从而限定在考虑K近邻局部信息，而且限制了参数数量与K成正比。但依然存在计算复杂度太高的问题<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。将一组多项式函数转换成为另一组多项式函数，会使计算更加简单。类比下面的高中试题例子。最后的计算时递归的形式，计算量变为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>K</mi><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(KE)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mclose">)</span></span></span></span>。</p><div align="center">  <img src="/2020/12/16/DL-lhy/chebnet.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>最后变成，需要学习一组/几组参数，处理经过k次递归计算的输入信号。</p><p><strong>GCN</strong></p><div align="center">  <img src="/2020/12/16/DL-lhy/gcn.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>MLP的意思是直接用特征来做，一般被认为是一个benchmark。GCN的效果并没有很强，一般使用GAT或GraphSAGE。</p><p>使用dropedge技巧稍稍拯救一下太深层的GCN。</p><h3 id="三-graph生成"><a class="markdownIt-Anchor" href="#三-graph生成"></a> 三. Graph生成</h3><p>通过VAE 和 GAN做。还有Auto-regressive-based model，每一步生成一个节点/边。</p><p><strong>最终总结</strong></p><div align="center">  <img src="/2020/12/16/DL-lhy/summary.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h2 id="rnn"><a class="markdownIt-Anchor" href="#rnn"></a> RNN</h2><h3 id="一-lstm基础"><a class="markdownIt-Anchor" href="#一-lstm基础"></a> 一. LSTM基础</h3><p>希望神经网络具有记忆，除了考虑输入之外还要考虑存在<strong>memory</strong>里的值，所以就考虑到了输入序列的次序。</p><p>Elman Network （memory里存的是hidden layer中的值）和 Jordan Network（memory里存的是output layer的值）。</p><p>Bidirectional RNN的好处就是上下文兼顾，而不是只处理上文。</p><p>LSTM（Long Short-term Memory）有三个gate，包括input gate（控制否是写入memory），output gate（控制是否可以读出memory）以及forget gate（控制memory要忘掉记得过去的东西）。可以认为LSTM有<strong>四个输入和一个输出</strong>。一般门里面选择sigmoid的函数，表示门被打开的程度。在神经元个数相同的情况下，LSTM是一般神经网络参数的4倍。</p><div align="center">  <img src="/2020/12/16/DL-lhy/lstm.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><div align="center">  <img src="/2020/12/16/DL-lhy/lstm_1.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>LSTM的最终形态，将前一个时间的hidden layer，memory都考虑到输入中来。然后叠加个五六层。</p><div align="center">  <img src="/2020/12/16/DL-lhy/lst_b.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>目前说在做RNN的人基本都是在用LSTM。Keras中实现了三种有，<strong>LSTM，GRU和SimpleRNN</strong>（最开始上课时引入的那个模型）。</p><p>RNN由于同样的参数在不同时间点反复使用（不是激活函数的锅），会有<strong>梯度消失问题</strong>，所以其的训练比较困难。RNN的error surface是非常陡峭的，参数比较容易飞出去，程序就NAN了。使用了非常工程化的一招clipping，即设计了一个阈值。解决的技巧就是<strong>LSTM</strong>，可以解决梯度消失问题（但不解决gradient explode问题）。**为什么LSTM可以做到呢？**因为memory和input是加在一起的，如果当前输入会影响到memory的话，那么这个影响会永远存在（传言，训练LSTM是需要确保forget gate在多数情况下都开启）。</p><p><strong>GRU</strong>只有两个门（input gate 和 forget gate合为一体），需要的参数量更少。LSTM如果过拟合比较严重，可以尝试一下GRU。</p><p>两种注意力机制网络结构，后者为Neural Turing Machine（包含了Writing Head Controller部分）。</p><div align="center">  <img src="/2020/12/16/DL-lhy/attention.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-structural-learning"><a class="markdownIt-Anchor" href="#二-structural-learning"></a> 二. Structural Learning</h3><h4 id="1-统一框架与三大问题"><a class="markdownIt-Anchor" href="#1-统一框架与三大问题"></a> 1. 统一框架与三大问题</h4><p>输入输出不是一个向量，而是一种结构化数据（sequence、list、tree、bounding box等）的时候该如何处理。应用场景包括比如：语音识别、机器翻译、语义解析、目标检测（图像到边框）、文本总结等。统一框架如下，即寻找一个可以衡量输入输出匹配性的函数F，测试阶段穷举所有可能的输出，选取在当前函数F下分值最高的y，作为最终结果。其实也可以将函数F理解为x和y一同出现的几率P。</p><div align="center">  <img src="/2020/12/16/DL-lhy/struct.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>我们需要解决如下三个问题：1）几率函数形式；2）如何找到最恰当的输出；3）如何找到合适的几率函数。可以将简单的DNN视为structural learning的子类，将DNN中的函数f和损失函数一同视为structural learning中的几率函数F。</p><div align="center">  <img src="/2020/12/16/DL-lhy/three.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-三大问题的解决"><a class="markdownIt-Anchor" href="#2-三大问题的解决"></a> 2. 三大问题的解决</h4><p>问题1：抽取features，目标检测中可以使用CNN抽取特征。</p><p>问题2：假装没有问题，已经解决；</p><p>问题3：算法步骤如下，重点在于证明这个方法最后是可以收敛/停止的。</p><div align="center">  <img src="/2020/12/16/DL-lhy/algorithm.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="3-structured-svm"><a class="markdownIt-Anchor" href="#3-structured-svm"></a> 3. Structured SVM</h4><h4 id="4-sequence-labeling"><a class="markdownIt-Anchor" href="#4-sequence-labeling"></a> 4. Sequence Labeling</h4><p>李老师认为GAN也是属于structural learning。</p><h3 id="三-deep-learning-vs-structural-learning"><a class="markdownIt-Anchor" href="#三-deep-learning-vs-structural-learning"></a> 三. Deep Learning vs Structural Learning</h3><div align="center">  <img src="/2020/12/16/DL-lhy/compare.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>现在流行把它们组装起来，未来的趋势是将Deep和Structured组装在一起。</p><div align="center">  <img src="/2020/12/16/DL-lhy/together.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅2020——机器学习</title>
    <link href="/2020/12/14/ML-lhy/"/>
    <url>/2020/12/14/ML-lhy/</url>
    
    <content type="html"><![CDATA[<p>课程视频可以在B站（<a href="https://www.bilibili.com/video/BV1JE411g7XF%EF%BC%89%E6%88%96Youtube%E6%89%BE%E5%88%B0%EF%BC%8C%E8%AF%BE%E7%A8%8B%E5%AE%98%E6%96%B9%E7%BD%91%E7%AB%99%E4%B8%BA%EF%BC%8Chttp://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html%E3%80%82" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1JE411g7XF）或Youtube找到，课程官方网站为，http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html。</a></p><div align="center">  <img src="/2020/12/14/ML-lhy/learn_map.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div>关键点：<p>老师的讲解思路以回归问题入手，使用MSE作为损失函数；之后再扩展/改进到分类问题</p><p>助教对于5大优化器方法（及各类研究型改进方法）的原理、使用场景的介绍。</p><h2 id="基础概念"><a class="markdownIt-Anchor" href="#基础概念"></a> 基础概念</h2><h3 id="一-梯度下降"><a class="markdownIt-Anchor" href="#一-梯度下降"></a> 一. 梯度下降</h3><p>线性回归问题没有局部最小解（损失函数是convex的）。</p><p>求解最优问题时“梯度越大的点距离最优点越远”这个通常只在<strong>仅有一个参数</strong>的时候成立，在<strong>跨参数的时候</strong>就无法进行这样的比对。最好的step应当把当前变量的二次微分也考虑进来。另外，在多个参数的时候也许会不降反增（我的世界视频）。</p><p>梯度下降法的局限不仅限于局部最小值，还有saddle point，或者导数很小的plateau。</p><h4 id="1-学习率"><a class="markdownIt-Anchor" href="#1-学习率"></a> 1. 学习率</h4><p>学习率，通常来讲可以随着参数的调整情况而减小学习率，但准确来讲是根据不同参数及不同更新情况调整学习率，比如<strong>Adagrad</strong>。当然这个方法到后面会计算比较慢。还有一系列表现更好的优化方法，比如常用Adam。</p><p><strong>Adagrad</strong>主要考虑梯度值的**“反差”**（How surprise it is），即当前梯度值与历史值的相对大小。从另一个角度来讲，Adagrad分母部分代表了二次微分的估算（这里是对于相对大小的近似，没有直接计算二次微分是出于复杂度的考虑）。</p><div align="center">  <img src="/2020/12/14/ML-lhy/adagrad.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>**Stochastic Gradient Descent**（SGD），每次只拿一个样本出来计算损失函数值，多次计算，速度快，不稳定。<h4 id="2-feature-scaling"><a class="markdownIt-Anchor" href="#2-feature-scaling"></a> 2. Feature Scaling</h4><p>尽量保证不同特征的取值范围相近，也是为了保证相应的权重对于损失函数的微分影响相近，最终图像更近似于圆形，更容易进行参数更新。Scaling的方法有非常多种，选一个喜欢的，常用的是减去平均值后除以标准差。</p><div align="center">  <img src="/2020/12/14/ML-lhy/scaling.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="3数学基础"><a class="markdownIt-Anchor" href="#3数学基础"></a> 3.数学基础</h4><p>泰勒级数展开，重点在于当x无限接近<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>时，只保留低阶项完成近似。所以学习率需要足够小以保证泰勒级数展开成立。另外，我们一般不会加入二次微分或更高阶的微分来增大学习率的恰当取值范围。</p><div align="center">  <img src="/2020/12/14/ML-lhy/mathbase.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="4五大常用优化方法"><a class="markdownIt-Anchor" href="#4五大常用优化方法"></a> 4.五大常用优化方法</h4><p>关注off-line类型的方法。以下五个是同时（几乎所有时候）工程上用到的优化方法，后三个是adaptive类型，比如在比较平缓的地方可以自动大步走过去，所以收敛速度比较快，但通常收敛结果要弱一些。其它研究型优化方法有非常多，但尝试起来可能比较费力。</p><ul><li>SGD</li><li>SGDM（with Momentum），计算当前参数更新值的时候考虑到过去的更新值。YOLO、ResNet模型训练时有使用。另外1983年提出了NAG方法针对动量进行了修改，look into the future，有一些数学计算保证无需维护两份参数，相当于把计算过程中的动量向前走了一个time step。这个思想用到Adam上就是Nadam方法。</li></ul><div align="center">  <img src="/2020/12/14/ML-lhy/SGDM.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><ul><li>Adagrad，考虑当前梯度相对于历史梯度值的大小。</li><li>RMSProp，是对Adagrad的改进，确保分母部分不会持续增大而导致出现计算结果过小的问题（没走几步就卡住了）。</li><li>Adam，是将SGDM与RMSProp结合；BERT、ADAM、Tacotron、Big-GAN、MEMO等模型都是使用ADAM训练出来的。</li></ul><p>为什么实际应用比较强的模型大多是使用SGDM、Adam训练出来的呢？助教认为这是因为这两个模型先抢到了两个最极端的位置。Adam比较快而SGDM比较稳。</p><div align="center">  <img src="/2020/12/14/ML-lhy/vs.png" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>有人提出SWATS，即开始的时候追求是速度使用Adam，后面使用SGDM。关键问题在于什么时候进行切换，作者并没有证明为什么要在那个点进行切换。</p><p>有一些研究旨在提升Adam，如何让Adam收敛得更稳。比如ICLR‘18论文AMSGrad，通过max操作记住过去最大时候的梯度值，降低影响较小的小梯度值带来的影响，其实有点像走回头路。ICLR’19发表AdaBound提出了两个人工定制的边界，限制动态调整的范围，但助教认为非常粗暴，有点像工程的解决方法。</p><p>另一个方向是从SGDM这里提升，是否可以帮助SGDM找到最佳学习率，提升其收敛速度。比如WACV 17’提出Cyclical LR周期性改变学习率大小（锯齿状信号）。SGDR也类似，只是变了不同的波形。One-cycle LR则是只做一个周期，周期中首先提升而变小，最后再平缓变小。</p><p><strong>Adam是否也需要warm-up？</strong>，答案是肯定的，这样会使它前面几次迭代结果不会太乱。工程上的wram-up经常是自己定义一个曲线。也有一些研究工作提出其它warm-up方法。ICLR’20 提出RAdam方法，开始的时候使用SGDM，后面用了改动后的Adam。</p><div align="center">  <img src="/2020/12/14/ML-lhy/radam.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>Geoffrey Hinton团队提出<strong>Lookahead方法</strong>，可以包装在现有各类优化算法外面，关键点是“K setp forward，1 step back”。这样做使梯度下降更稳定，better generalization，因为尽量保持在比较平坦的区域而不会进入太危险的区域。</p><p>计算SGDM和Adam时需不需要考虑L2正则项的参数呢？有研究结果表明最好是在计算动量或历史加和的时候不要加入，而在计算更新值的时候加入，提出的算法名为<strong>AdamW和SGDW</strong>，与大部分优化算法不同，AdamW是真正被工业界使用过的。</p><div align="center">  <img src="/2020/12/14/ML-lhy/all.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>通常的适用情况如下，并没有一个通用万能的优化器。更换更恰当的优化器的作用只能是在模型训练起来以后，提升一些性能；如果模型训练不起来，那可能是数据或架构本身有问题，通过更换优化器并不能解决。</p><div align="center">  <img src="/2020/12/14/ML-lhy/env.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="5-优化技巧"><a class="markdownIt-Anchor" href="#5-优化技巧"></a> 5. 优化技巧</h4><p>Shuffling、Dropout、Gradient noise（加入高斯噪声，随着时间标准差减小）</p><p>Warm up（一开始学习率定比较小，慢慢稳定之后调大）、Curriculum Learning、Fine-tuning；这几个技巧的本质是先用简单的数据或任务训练模型，之后再慢慢增加难度。</p><p>Normalization，Regularization，避免模型学到过于极端的参数。</p><h3 id="二-正则化"><a class="markdownIt-Anchor" href="#二-正则化"></a> 二. 正则化</h3><p>比较简单的model受不同数据分布的影响相对较小。如果模型的error主要来自于variance，则情况为overfitting（过拟合），需要增加训练集或加入正则项；如果主要来自bias，则情况为underfitting（欠拟合），需要重新设计模型，加入更多特征信息等。更直观的过拟合/欠拟合评价标准是观察模型在训练集和测试集上的误差表现。</p><div align="center">  <img src="/2020/12/14/ML-lhy/variance.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>L2正则会倾向得到更加平滑的函数。</p><p>做正则化的时候并不需要考虑bias项。</p><h3 id="三-验证集"><a class="markdownIt-Anchor" href="#三-验证集"></a> 三. 验证集</h3><p>为什么要划分出validation set？ 因为这样的话，才没有在结果中考虑到public testing set的影响，从而可以更贴近模型在private testing set上的表现。一般进行N折交叉验证（N-fold cross Validation）选择模型。注意，模型选择后还要在整个训练集上重新训练一下。</p><div align="center">  <img src="/2020/12/14/ML-lhy/validation.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h2 id="分类问题"><a class="markdownIt-Anchor" href="#分类问题"></a> 分类问题</h2><p>**为什么不可以用回归方法硬解分类问题？**比如结果大于0是一类，小于0是另一类。很大的问题你很难通过回归方法得到准确模型，比如下图右下角数据会对模型产生很大影响，最终会选择紫色线模型。而且使用回归方法解分类问题，在定义不同类别的时候，隐含着加入了大小关系。</p><div align="center">  <img src="/2020/12/14/ML-lhy/regress.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><h3 id="一-生成方法"><a class="markdownIt-Anchor" href="#一-生成方法"></a> 一. 生成方法</h3><p>生成模型的关键三步，最初思想为贝叶斯公式。</p><div align="center">  <img src="/2020/12/14/ML-lhy/steps.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>假设服从<strong>高斯分布</strong>（不同情况下可以使用不同分布，比如伯努利等），关键参数为mean和covariance matrix，即均值和协方差矩阵。使用<strong>最大似然估计</strong>算这两个参数值，即从样本中估计出最可能的参数模型。</p><p>通常不会给每个分类都有自己的mean和covariance matrix（大小与输入的属性个数成平方），一般会计算出<strong>统一的协方差矩阵</strong>，有效减少参数。而且这种情况下分类boundary变成了直线，模型也属于<strong>线性模型</strong>。数学推导如下，几经展开、消去等运算后，变成线性模型加一个sigmoid函数。另外，在高维空间，如果假设每个属性的分布是独立分布的，这就是<strong>朴素贝叶斯</strong>（Naive），这个假设还是比较强的。</p><div align="center">  <img src="/2020/12/14/ML-lhy/math1.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/12/14/ML-lhy/math2.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>在生成模型中，我们直接计算出权重w和偏移b，而在目前比较流行的判别方法中，我们使用各种优化方法逐步寻找最优的w和b。</p><h3 id="二-逻辑斯特回归"><a class="markdownIt-Anchor" href="#二-逻辑斯特回归"></a> 二. 逻辑斯特回归</h3><p>损失函数使用<strong>伯努利分布交叉熵</strong>，而不直接像线性回归一样直接使用MSE，原因是这样的损失函数在逻辑斯特回归上过于平坦。详细来说，如果使用MSE做损失函数，求偏微分之后，由结果公式看到在距离目标很远的时候，微分值也很小，更新速度非常慢。</p><div align="center">  <img src="/2020/12/14/ML-lhy/cross.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>但求偏微分之后，逻辑斯特与线性回归的参数更新表达式是一样的。</p><div align="center">  <img src="/2020/12/14/ML-lhy/logistic.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>多分类情况</strong>下，用Softmax（二分类问题下就是sigmoid）。Softmax会对比较大的结果值进行强化，使最大值辨识度更高。</p><div align="center">  <img src="/2020/12/14/ML-lhy/softmax.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>逻辑斯特模型局限，其为线性模型，无法直接处理非线性问题（比如异或问题）。一种解决方法为Feature Transformation，但恰当的方法比较难找。我们希望让机器自动找到合适的方法，可以通过级联多个逻辑斯特模型实现。我们可以给它们一个新的名字，把每一个逻辑斯特模型叫做神经元，整个模型叫做神经网络，然后搞到层数多一些，就有了深度~</p><div align="center">  <img src="/2020/12/14/ML-lhy/multi.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="三-生成方法-vs-判别方法"><a class="markdownIt-Anchor" href="#三-生成方法-vs-判别方法"></a> 三. 生成方法 vs 判别方法</h3><p>模型公式相同，但我们极大可能找不到相同的w和b参数。因为两类方法有不同的假设。比如逻辑斯特回归对数据分布没有任何假设，是直接梯度下降找最优解；朴素贝叶斯则假设了独立同分布、高斯分布等等。</p><div align="center">  <img src="/2020/12/14/ML-lhy/wb.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>常常在文献上有人讲，判别方法比生成方法效果好。但有些时候生成方法也是有优势的（因为此类方法加入了对数据分布的假设，相当于有了一些脑补），比如1）训练数据量很少的时候；2）有某些噪声；3）先验与类相关概率可以通过不同数据来源估计。</p><h2 id="作业"><a class="markdownIt-Anchor" href="#作业"></a> 作业</h2><h3 id="1hw2"><a class="markdownIt-Anchor" href="#1hw2"></a> 1.HW2</h3><p>生成模型关于协方差矩阵求逆的部分，“Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error. Via SVD decomposition, one can get matrix inverse efficiently and accurately.”</p><pre><code class="hljs python">u, s, v = np.linalg.svd(cov, full_matrices=<span class="hljs-literal">False</span>)inv = np.matmul(v.T * <span class="hljs-number">1</span> / s, u.T)</code></pre>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大规模图数据分析技术：挑战与机遇</title>
    <link href="/2020/11/16/talk-1/"/>
    <url>/2020/11/16/talk-1/</url>
    
    <content type="html"><![CDATA[<p>2020年11月16日，FIT楼多功能厅，听取樊文飞教授报告。以下为报告笔记，因为背景知识有限，听取现场报告，可能会有一些遗漏或理解错误之处。</p><p>当前图数据已成为大数据分析场景下的重要数据来源，老师在报告中就<strong>4V问题</strong>（Volume，Variety，Velocity，Veracity/value），分析图数据存储、分析中遇到的问题与部分解决思路。</p><h4 id="1volume体量"><a class="markdownIt-Anchor" href="#1volume体量"></a> 1.Volume（体量）</h4><p>大型图网络中包含百亿节点，百亿条边，如何存储处理？</p><p>DFS虽然是线性复杂度，但在大图上已经难以运行。</p><p>使用并行计算系统完成大图处理，在工业界存在很多问题：</p><p>1）已知的图算法能否并行化，学术界有很多这样的尝试比如google的pregel，CMU的Graphlab/PowerGraph，Facebook的Giraph，Berkeley的GraphX，IBM的Giraph++，但工业界未使用。樊老师团队有此类工作发表在SIGMOD，同时项目被阿里收购，今年开源为GraphScope。</p><div align="center">  <img src="/2020/11/16/talk-1/graphscope.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>2）并行计算中如何选择同步或异步，樊老师团队提出AAP工作，《Adaptive Asynchronous Parallelization of Graph Algorithms》</p><p>2）如何衡量某个并行算法是有效的？</p><p>3）传统计算复杂性理论在并行计算环境下有什么样的新表现？</p><h4 id="2velocity动态"><a class="markdownIt-Anchor" href="#2velocity动态"></a> 2.Velocity（动态）</h4><p>实际中的网络动态变化，但变化部分相对原网络占比较小，如何高效处理变化的信息？</p><p>计算output的变化。</p><p>1）很多被证明不是bounded的问题仍要解决，樊老师团队提出relative bounded，《Bounded incremental graph computations: Undoable and doable》。</p><p>2）如何提出增量式的算法，《Incrementalization of graph partitioning algorithms》VLDB 2020。</p><h4 id="3-variety异构"><a class="markdownIt-Anchor" href="#3-variety异构"></a> 3. Variety（异构）</h4><p>如何达成图数据库与原有关系型数据库的统一？比如阿里提到的数据中台概念。但这一方向距离落地还有很多工作要做。樊老师提出gSQL概念，以及联邦数据库（这里与联邦学习的概念不同，强调的是多类型数据的联邦）。</p><div align="center">  <img src="/2020/11/16/talk-1/gsql.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="4-veracity质量"><a class="markdownIt-Anchor" href="#4-veracity质量"></a> 4. Veracity（质量）</h4><p>存在过时数据、链接丢失与语义不一致等问题，如何解决？这个问题是4V中解决度最低的。</p><p>樊老师认为可以将logic规则与AI统一在一个框架下，同时还可以提高ML的可解释性。</p><div align="center">  <img src="/2020/11/16/talk-1/ml-1.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/11/16/talk-1/ml-2.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/11/16/talk-1/ml-3.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3><p>大数据场景下，图数据分析在4V中存在如下挑战：</p><ul><li>Volume，并行处理——reduction，complete problems等</li><li>Velocity，增量计算理论与实践</li><li>Variety，在SQL上做到图数据库与关系型数据库的统一</li><li>Veracity，统一规则（logic）与ML；图数据清洗；链路预测等</li></ul><h3 id="相关文献"><a class="markdownIt-Anchor" href="#相关文献"></a> 相关文献</h3><ol><li>Application driven graph partitions. SIGMOD 2020</li><li>Capturing associations in graphs VLDB 2020</li><li>Incrementalization of graph partitioning algorithms. VLDB 2020</li><li>Graph algorithms: Parallelization ans scalability. Science China Information Sciences, 2020</li><li>Adaptive asynchronous paralleization of graph algorithms, TODS2020</li><li>Deducing certain fixes to graphs, VLDB 2020</li><li>Parallelizing sequential graph computations TODS 2018</li><li>Incremental graph compulations: doable and undoable SIGMOD 2017</li></ol><h3 id="思考"><a class="markdownIt-Anchor" href="#思考"></a> 思考</h3><p>1）有关数据质量的部分，老师提到将规则与AI纳入统一框架，这个让我联想到网络安全领域目前基于规则和基于AI的方法，是否可以进行结合？比如Yara规则与CFG恶意软件检测等。这部分可以搜一下相关资料，樊老师并没有更详细地分享。</p><p>2）<strong>理论与系统</strong>是计算机专业领域的核心，要结合考虑理论研究的应用落实，同时学会如何在项目开发中发现待解决问题。</p><p>3）科研技术与产业落地之间还会有比如，政策、安全性、市场等多方面问题，需要综合考虑。比如XML并没有实现统一，比如在当前已有标准的情况下，为什么不能把关系型数据库全部转化为图数据存储。</p>]]></content>
    
    
    <categories>
      
      <category>交流报告</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
      <tag>大数据分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【小故事】无法消散</title>
    <link href="/2020/07/07/story-1/"/>
    <url>/2020/07/07/story-1/</url>
    
    <content type="html"><![CDATA[<p>下午四点半，9号楼的小洛死了。</p><p>一个女孩就这么从楼上摔下去，“肝脑涂地”，围观的人们在惋惜中带着阵阵恶心。</p><p>警队来了，120无聊地待在一边。</p><p>小洛家在十一楼，电梯坏了，郝队爬得气喘吁吁，“大夏天的，整这出？”</p><p>小洛和父母同住，郝队进门，家里一尘不染，物件规整，门口立着扫帚，卫生间里的拖布还没晾干。阳台的花鸟，墙上的字画，尤其沙发、床头随处可见的书籍，显示出这家人对文化生活的习惯，不是那种张扬在外的追捧。</p><p>“没有打斗的痕迹，看来是失足坠楼。”</p><p>小洛摔下去的地方正对着家里小阳台的窗户，窗页大敞着，郝队看了看，有齐腰高，如果不是故意爬上去，应该没可能发生意外。奇怪的是，周围窗台也一尘不染，连在附近活动过的痕迹都没有。如果是失足坠落，这窗户就显得过分冷静了。</p><p>“真会给我出难题。”</p><p>在屋内转了一圈，没获得什么实质性线索，郝队转过头来询问家人亲友。据了解，小洛是个成绩不错的学生，正在读博士，平时安安静静，不太可能涉及校园贷、勒索威胁等杂七杂八的事情。小洛有近期和同学出游的计划，前一天洛爸还听见她在电话上兴奋地讨论行程安排。听同学说，小洛最近有一篇论文在写，她还报名了半个月之后的一场线上比赛。哦，还有在小区附近的美容院里预约了两天后的祛痘清洁服务。</p><p>“你家孩子近期遇到什么事情了么？”，  “没有啊，疫情在家，每天平平淡淡的，哪有什么大事。” 洛妈已经哭晕了，都是洛爸在撑着回答。</p><p>“孩子人际交往怎么样？”</p><p>“性格有些内向，打小害羞，爱自己一个人玩，但长大就好很多，上学也结识了几个挺不错的朋友，这几天晚上还经常一起打游戏聊天呢。”</p><p>“那她平时生活状态怎么样？我看她是博士生，是不是课业压力挺大的？”</p><p>“刚读博的时候确实是，她老觉得毕业没希望，打电话回家也都挺沮丧的，后来发了两篇论文，就好很多了。我孩子不可能自杀，她最近也作息规律，偶尔锻炼，跟我俩聊天，都很好的，警官您可得仔细给查查啊，我们都配合，都配合。”</p><p>“不像啊”，郝队心想，“这感觉过得挺好。” 忽然，郝队看到垃圾桶里有个弯了的勺子，是平时做饭用的不锈钢厨具。要不是强外力，勺子不可能拧成这样。刚刚在书房里发现的塑料碎片，应该就是这勺子把手上掉下来的。郝队一阵激动，“这案子应该另有隐情。”</p><p>其实，这就是一场简单的自杀。</p><p>小洛是个理解力远超表达力几个维度的人，这样的人是孤独的，她在期盼与失落中交替，觉得没劲，就走了。</p><p>临走的几个小时前，又一次失落后，小洛很愤怒，刷碗时猛地把勺子砸向地板。看着弯折的勺子和四散的勺柄，她觉得挺好笑的。小洛有很多年，或者甚至说是从小，就不会生气，她好像总能站过去理解对面的逻辑，然后承认现实。但承认现实，安慰不了自己。因为无法被理解，所以时常失落，又因为能理解，所以她的失落没有焦点。</p><p>不如算了吧，通过模仿别人而产生的烟火气，总也不能落地，搞得大家都麻烦。后来她想到，家里人都爱干净，就一一收拾好，还彻底打扫了卫生。以往这种时候，洛妈回来都会表扬几句，小洛听着，觉得“你开心就好”。</p><p>窗台上，小洛一边擦去最后的痕迹，一边记起初中时，在思想政治书上背过，“热爱生活，珍惜生命，回报父母，贡献社会”。真没办法，对不起当时考出的98分。下坠时，她记起，之前和同学讨论起生命的意义、自杀等问题，同学说，“死不死得无所谓，走之前可以把角膜啥的这些器官，捐献给那些想活着的人”。</p><p>我走了，就任你们处置了，最好可以尽快消散掉。不过，好像没大可能，而且又要为当代青年抹黑了，真是不好意思。</p><p>“现在这些年轻人啊，生活条件那么好了，蜜罐里泡大，心理素质就是差。”</p>]]></content>
    
    
    <categories>
      
      <category>原创故事</category>
      
    </categories>
    
    
    <tags>
      
      <tag>瞎写</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>这个博客的搭建过程</title>
    <link href="/2020/06/01/build-blog/"/>
    <url>/2020/06/01/build-blog/</url>
    
    <content type="html"><![CDATA[<p>本文重点介绍基于Hexo+Github搭建个人网站流程，最初基本源自<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog" target="_blank" rel="noopener">这篇文章</a>（如何使用Github从零开始搭建一个博客），作者把步骤已经介绍得非常详细完善了。我只是补充踩到的一些坑….</p><p>后来，我背弃了极简审美，开始使用<strong>Fluid</strong>主题。</p><h3 id="一-基础搭建"><a class="markdownIt-Anchor" href="#一-基础搭建"></a> 一. 基础搭建</h3><p>搭建过程使用的两个最基本、最重要的东西是Hexo和Github，其中前者是一个轻量级博客框架，支持将Markdown编写的文章直接编译为静态网页文件并发布，省去了数据库问题。Github则用来解决域名问题，其Github Pages允许每个用户创建一个名为{username}.github.io的仓库，发布博客网页。当然也可以自己申请域名，使用CNAME跳转。</p><h4 id="1-github创建仓库"><a class="markdownIt-Anchor" href="#1-github创建仓库"></a> 1. Github创建仓库</h4><p>在Github上创建一个名为{username}.github.io的仓库，注意必须是github.io结尾。比如我的github账户为“DeepDeer”，创建仓库为“<a href="http://deepdeer.github.io" target="_blank" rel="noopener">deepdeer.github.io</a>”。另外，申请对应仓库时不要弄成private的，否则开放博客Github要收费哈。</p><h4 id="2-安装环境"><a class="markdownIt-Anchor" href="#2-安装环境"></a> 2. 安装环境</h4><p>首先在自己电脑上安装Node.js，确保环境变量配置好，可以使用npm命令；</p><p>其次使用npm命令安装Hexo，安装后确保可以使用<code>hexo</code>命令。</p><pre><code class="hljs bash">npm install -g hexo-cli</code></pre><h4 id="3-初始化项目"><a class="markdownIt-Anchor" href="#3-初始化项目"></a> 3. 初始化项目</h4><p>选定存储博客文件的位置，在此文件夹中使用如下命令创建项目及对应文件夹：</p><pre><code class="hljs bash">hexo init &#123;name&#125;</code></pre><p>命令下产生的文件夹包括themes、source等文件夹，调用如下命令，则在public文件夹中生成js、css、font等内容。</p><pre><code class="hljs verilog">hexo <span class="hljs-keyword">generate</span></code></pre><p>使用server命令在本地运行博客，可以看到类似结果：</p><pre><code class="hljs routeros">hexo<span class="hljs-built_in"> server </span> #或简写为 hexo s</code></pre><div align="center">  <img src="/2020/06/01/build-blog/hello.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/hexo.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><h4 id="4-部署至github"><a class="markdownIt-Anchor" href="#4-部署至github"></a> 4. 部署至Github</h4><p>安装一个支持Git的部署插件</p><pre><code class="hljs sql">npm <span class="hljs-keyword">install</span> hexo-deployer-git <span class="hljs-comment">--save</span></code></pre><p>修改Hexo的配置文件_config.yml，找到Deployment部分，修改为如下内容：</p><pre><code class="hljs bash"><span class="hljs-comment"># Deployment</span><span class="hljs-comment">## Docs: https://hexo.io/docs/deployment.html</span>deploy:  <span class="hljs-built_in">type</span>: git  repo: git@github.com:DeepDeer/deepdeer.github.io <span class="hljs-comment">#你自己的Github仓库地址</span>  branch: master</code></pre><p>使用deploy命令部署后，可通过域名deepdeer.github.io访问，Github上传代码如下：</p><pre><code class="hljs ebnf"><span class="hljs-attribute">hexo deploy</span></code></pre><div align="center">  <img src="/2020/06/01/build-blog/github.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p><a href="http://xn--deploy-hp7ik1vdf32vsxrqigxw5bw73eklg.sh" target="_blank" rel="noopener">可编写如下内容脚本deploy.sh</a>，此后每当有内容更新时，<code>.deploy.sh</code>运行脚本即可。</p><pre><code class="hljs bash">hexo cleanhexo generatehexo deploy</code></pre><h3 id="二-加入主题"><a class="markdownIt-Anchor" href="#二-加入主题"></a> 二.  加入主题</h3><p>目前我加入的是Fluid主题（因为看上了颜值），之前用过一段时间Next主题，也很推荐。</p><h4 id="1-next主题"><a class="markdownIt-Anchor" href="#1-next主题"></a> 1. Next主题</h4><p>有关Next主题的配置及各种插件，在<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog" target="_blank" rel="noopener">这篇文章</a>中介绍地非常详细，这里只补充有关1）添加Gitalk插件和2）修改字体部分。</p><h5 id="1-gitalk插件"><a class="markdownIt-Anchor" href="#1-gitalk插件"></a> 1&gt; Gitalk插件</h5><p>申请Gitalk就在Github个人账户的settings——&gt; Developer settings ——&gt; OAuth Apps，点击 New OAuth App，出现申请界面。其中应用名称随便写就行，Hompage URL和Authorization callback URL写博客链接。如果有自己的域名可以更改Authorization callback URL。点击注册，生成Client ID和Client Secret。</p><p>注意，如果自己配置了域名，这个callback URL要改成自定义域名</p><div align="center">  <img src="/2020/06/01/build-blog/oauth.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>在配置了_config.yml文件后，第一次进入界面会出现下图效果。如果点击Github登录后跳转到了404界面，那么, 就说明配错了。我当时是在写_config.yml忘了把client_id, client_secret字段带的{ }去掉。这给我一顿google啊…</p><div align="center">  <img src="/2020/06/01/build-blog/begin.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>最后效果就像这个博客里一样，相关评论会显示在对应仓库的issues里，记得在仓库的settings里把features—&gt;issues勾选上（貌似默认就是开启的）</p><div align="center">  <img src="/2020/06/01/build-blog/comment.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/issues.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><h5 id="2-修改字体"><a class="markdownIt-Anchor" href="#2-修改字体"></a> 2&gt; 修改字体</h5><p>Next主题默认的博文正文字体大小有点大了，可以在配置文件里改一下。相关配置在hexo\themes\next\source\css\variables路径下的base.styl文件里的Font Size部分。这里面每个变量控制某一部分的字体大小，我是挨个试出来的font-size-large是正文字体（简单粗暴，真开心…)</p><div align="center">  <img src="/2020/06/01/build-blog/font.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>另外，在Hexo配置文件和Next主题配置文件中，都有一些有关网站信息的配置选项，最终使用Next主题搭建出的网站效果如下：</p><div align="center">  <img src="/2020/06/01/build-blog/next.jpg" srcset="/img/loading.gif" width="70%" height="50%" alt="oauth"></div><h4 id="2fluid主题"><a class="markdownIt-Anchor" href="#2fluid主题"></a> 2.Fluid主题</h4><p>其实这个主题有非常好的<a href="https://fluid-dev.github.io/hexo-fluid-docs/guide/" target="_blank" rel="noopener">配置指南</a>，其配置文件_config.yml的注释也很清晰，可以从头摸索。有几点经验包括1）图片插入；2）评论插件。</p><h5 id="1-图片插入"><a class="markdownIt-Anchor" href="#1-图片插入"></a> 1&gt; 图片插入</h5><p>需要注意的一点是，在Fluid主题下有文章背景图等存在于框架中的图片，这些图片一律存放在<code>./themes/fluid/source/img</code>文件夹下。即使是某个文章的缩略图也是这样。</p><div align="center">  <img src="/2020/06/01/build-blog/pic.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>其他存在于文章中的图片，可用如下形式添加。</p><p>首先，把_config.yml文件里的post_asset_folder选项设置为true。</p><p>其次安装一个插件，据说原有插件有一些bug，下面是修改过的插件，亲测有效，感谢<a href="https://www.jianshu.com/p/3db6a61d3782" target="_blank" rel="noopener">这篇博客</a></p><pre><code class="hljs vim">npm install http<span class="hljs-variable">s:</span>//github.<span class="hljs-keyword">com</span>/<span class="hljs-number">7</span>ym0n/hexo-asset-image --<span class="hljs-keyword">sa</span></code></pre><p>有了这些配置后，再运行hexo new xxx，在/source/_posts/路径下，除了可以生成新文章xxx.md之外，还生成一个同名文件夹。插入图片时放到这个文件夹里即可，在markdown里用如下语句：</p><pre><code class="hljs routeros">&lt;img <span class="hljs-attribute">src</span>=<span class="hljs-string">"xxx/图片名称.png"</span> <span class="hljs-attribute">alt</span>=<span class="hljs-string">"图片标识"</span> <span class="hljs-attribute">style</span>=<span class="hljs-string">"zoom:30%;"</span> /&gt;</code></pre><p>但是，在Fluid主题下，这些图片并没有默认居中，可以采用如下HTML代码控制位置和大小：</p><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">align</span>=<span class="hljs-string">center</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">img</span> <span class="hljs-attr">src</span>=<span class="hljs-string">"build-blog/pic.jpg"</span> <span class="hljs-attr">width</span>=<span class="hljs-string">"30%"</span> <span class="hljs-attr">height</span>=<span class="hljs-string">"30%"</span> <span class="hljs-attr">alt</span>=<span class="hljs-string">"oauth"</span>  /&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span></code></pre><h5 id="2评论插件"><a class="markdownIt-Anchor" href="#2评论插件"></a> 2&gt;评论插件</h5><p>Fluid推荐的utteranc.es插件，经常会有加载比较慢的问题，乍一看以为不让评论…</p><p>这个配置过程也很简单。</p><p>首先在github创建一个公开的仓库，比如命名为’deepdeer_comments’。</p><p>点击<a href="https://github.com/apps/utterances" target="_blank" rel="noopener">这个链接</a>安装应用，选择“only select repositories”选项，找到刚刚建立好的仓库，点击install。</p><p>在配置中填写repo名，格式为“用户名/仓库名”，如“DeepDeer/deepdeer_comments”。Issue的命名方式建议选择第一个“Issue title contains page pathname”。</p><p>根据个人喜好选择主题之后，最后一栏会自动生成配置信息，复制这些信息。</p><p>在fluid主题的配置文件中，找到<code>comments</code>部分，将enable设置为true，并将type写成utterances。</p><p>在后面的comments具体配置部分，改成之前自动生成的配置。</p><div align="center">  <img src="/2020/06/01/build-blog/utter.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p>这个博客目前用的是Gitalk插件，配置与Next主题中提到的大致相同。Fluid代码中该插件配置有问题，评论无法分页显示，感谢<a href="https://juejin.im/post/5ed177e36fb9a047923a39fe" target="_blank" rel="noopener">这篇文章</a>。即更改fluid主题下的<code>layout/_partial/comments/gitalk.ejs</code>文件内容中的’id’一栏部分</p><pre><code class="hljs python"><span class="hljs-comment">#原有的</span>id: <span class="hljs-string">'&lt;%- md5(theme.gitalk.id) %&gt;'</span>,<span class="hljs-comment">#改正后</span>id: &lt;%- theme.gitalk.id %&gt;,</code></pre><h3 id="三-自定义域名"><a class="markdownIt-Anchor" href="#三-自定义域名"></a> 三. 自定义域名</h3><p>本博客使用了阿里云上购买的域名。</p><p>在<a href="https://wanwang.aliyun.com/domain/searchresult/?keyword=skylasun&suffix=.cn#/?keyword=skylasun&suffix=cn" target="_blank" rel="noopener">这里</a>点击“控制台”，登录后，找到边栏中的“域名”。选择“域名注册”</p><div align="center">  <img src="/2020/06/01/build-blog/domain.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/reg.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p>进入后，查询你喜欢的关键字相关的域名的注册情况，选择中意的域名就可以交钱了。最终付款之前还需要一些身份认证。</p><div align="center">  <img src="/2020/06/01/build-blog/buy.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>购买成功，认证通过后，在已有域名那里，点击“解析”，添加解析规则。这里添加的IP地址是之前deepdeer.github.io的解析情况，可以通过各类IP或域名查询网站找到，比如<a href="https://site.ip138.com/" target="_blank" rel="noopener">这里</a>。注意下面要加上一条CNMA规则。</p><div align="center">  <img src="/2020/06/01/build-blog/map.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/ip.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>另外，在Github仓库的Settings里，需要加上“Custom domain”，保存配置后，会自动生成名为CNAME的文件，内容如下。但需要注意的是，每次我们重新部署时，使用deploy clean再generate后，会清除掉这个CNAME文件。为解决这个问题，可以把CNAME文件放到博客的“source”文件夹中。</p><div align="center">  <img src="/2020/06/01/build-blog/cname.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><h3 id="四-其它小经验"><a class="markdownIt-Anchor" href="#四-其它小经验"></a> 四. 其它小经验</h3><h4 id="1markdown编辑器推荐"><a class="markdownIt-Anchor" href="#1markdown编辑器推荐"></a> 1.Markdown编辑器推荐</h4><p>这些博客都是用<a href="https://www.typora.io/" target="_blank" rel="noopener">Typora</a>写的。该软件界面简洁，即时效果，很好用，推荐~</p><p>Typora除了支持公式块之外，还支持行内公式，在偏好设置中勾选”内联公式“即可。</p><div align="center">  <img src="/2020/06/01/build-blog/gongshi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2markdown中内容折叠"><a class="markdownIt-Anchor" href="#2markdown中内容折叠"></a> 2.Markdown中内容折叠</h4><p>有时文章内容过多不便于显示，可以使用如下语法进行折叠</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">details</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">summary</span>&gt;</span>可显示的标题<span class="hljs-tag">&lt;/<span class="hljs-name">summary</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">pre</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">code</span>&gt;</span>   折叠内容  <span class="hljs-tag">&lt;/<span class="hljs-name">code</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">pre</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">details</span>&gt;</span></code></pre><p>比如</p><div align="center">  <img src="/2020/06/01/build-blog/zhedie.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>效果如下，点击后显示黑色部分</p><div align="center">  <img src="/2020/06/01/build-blog/xiaoguo.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>如果有其它的坑，欢迎大家评论补充，谢谢！</p>]]></content>
    
    
    <categories>
      
      <category>教程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>工程技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>THU研究生国际会议出行准备流程</title>
    <link href="/2020/05/30/baoxiao/"/>
    <url>/2020/05/30/baoxiao/</url>
    
    <content type="html"><![CDATA[<p>下文仅限清华大学网络科学与网络空间研究院研究生同学使用，包含护照、签证、报销等</p><blockquote><p>提示：出国手续涉及部门较多，请尽早准备提前办理。如遇假期会有所调整，要关注邮件通知~</p></blockquote><h3 id="首先"><a class="markdownIt-Anchor" href="#首先"></a> 首先</h3><p>​你要有个<strong>护照</strong>，如果没有，办理的时候把发票留好，可以报销。</p><h3 id="前期准备"><a class="markdownIt-Anchor" href="#前期准备"></a> 前期准备</h3><p>注册会议并拿到<strong>邀请函</strong></p><p>预定<strong>机票和酒店</strong>（办签证使用）。目前携程等网站貌似不再支持付款前先打印行程单。</p><ol><li>机票时间，一般可定在会议安排往前往后各一天。有特殊情况，赶在自己文章汇报前到达即可。</li><li>酒店一般订会议推荐的，预定前注意一下学校给的当地住宿报销额度。有些会议官网会贴出提前订酒店有优惠的通知，发邮件过去即可。</li></ol><h3 id="学校审批"><a class="markdownIt-Anchor" href="#学校审批"></a> 学校审批</h3><h4 id="1申请出国批件"><a class="markdownIt-Anchor" href="#1申请出国批件"></a> 1&gt;申请出国批件</h4><ol><li>首先在info上进行申请，找“出国出境申报”——&gt;“因公出国（境）申报系统（新）</li></ol><div align="center">  <img src="/2020/05/30/baoxiao/apply.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><ol start="2"><li>进入系统后，选择”新申请”，点击“我已阅读”，在因公出境申请表上填写信息，其中会<strong>比较犹豫的几个字段</strong>有：</li></ol><ul><li>出访基本信息：出入境时间大概在会议日程往前往后各一天，离境、入境时间是否需要过境等如实填写即可</li><li>出访类别：单位公派，会议</li><li>出访经费：费用来源一般选择“全部校内支付”，“纵向科研经费”，校内支付，人民币（大致写一个费用即可）</li><li>日程计划：简单填写就行，比如出发，抵达，开会，回程等等（可适当扩展）</li></ul><ol start="3"><li><p>提交之后会有一个预算表，大概可以看到给当地的住宿、日常消费额度等。这类信息也可以在边栏中“预算、外汇与报销”的“政策与标准”的表格中看到。</p><div align="center">  <img src="/2020/05/30/baoxiao/biaozhun.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div></li><li><p>“报批材料”里上传会议的邀请函和论文录用证明。</p></li><li><p>点击提交，打印申请表，这个表需要自己和导师签字。</p></li><li><p>提交完成后，返回主界面会显示出当前进度，完成后圆圈会变绿。大概两周左右“单位审核”会变绿。等到“学校审批”通过，显示批件下达之后，可以下载电子版。</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/jindu.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div><h4 id="2批件领取"><a class="markdownIt-Anchor" href="#2批件领取"></a> 2&gt;批件领取</h4><p>去国际处，在李兆基4楼（可以进楼的门有点多，但失之毫厘谬以千里，所以可以问下保安…）</p><p>去之前先准备一份**“派出证明”<strong>。还是在刚刚的出入境申请系统的边栏里面。点击进去下载对应模板，注意老师们已经用最直接醒目的方法标示出的</strong>注意事项**。</p><div align="center">  <img src="/2020/05/30/baoxiao/chat.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div><div align="center">  <img src="/2020/05/30/baoxiao/chats.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><div align="center">  <img src="/2020/05/30/baoxiao/attention.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>在国际处主要有以下<strong>几个事情</strong>：</p><ul><li>拿批件</li><li>拿外汇预算单</li><li>派出证明需要盖章</li><li>如果办签证时需要单位法人证明一类的材料，需要<strong>主动</strong>和老师提及</li></ul><h3 id="签证办理"><a class="markdownIt-Anchor" href="#签证办理"></a> 签证办理</h3><p>这个就要看去哪个国家了，我以希腊为例，需要申根签，可以先在官网上填写表格申请，然后按照里面写的去依次准备材料。去使馆办事处。一定要注意时间，选最最最是工作时间的时段过去。我第一去的时候好像是下午3点左右到的，说是刚刚停止办理…</p><p>其它细节事项：</p><ol><li><p>保险可以直接在淘宝上买，搜“申根保险”就可以，看清楚额度是否符合要求；</p></li><li><p>户口页，如果户口在学校的话，直接在info上申请，”集体户口卡借阅”，里面包括“借阅预约”和“首页打印”。预约之后直接去地图里圈出的小房子（保卫处）里拿就好了；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/hukou.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="3"><li><p>银行对账单可以直接在C楼打印；</p></li><li><p>在读证明，在info上预约然后直接与三教打印（貌似改到了六教？，反正C楼应该都是万能的）；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/zaidu.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="5"><li>办理签证最后需要交纳现金。留好发票，这个在报销范围内。</li></ol><h3 id="出行及报销"><a class="markdownIt-Anchor" href="#出行及报销"></a> 出行及报销</h3><ol><li><p>行程中尽量保存好<strong>所有票据</strong>，回来整理<strong>报销</strong>。（我都是先垫付再报销，据说还可以先去学校<strong>借款</strong>）</p><p>各类车票，登机牌，行程单，机票购买记录及发票（让网站寄过来），酒店账单/发票，会议注册费发票等</p></li><li><p>去首都机场的话，清华科技园那里有大巴，车费是30块？这种貌似属于城建交通，也可以报销，不行的话，也有日常杂费可以cover掉。</p></li><li><p>回来后的报销主要是填写一个报销表格，还是在刚刚的出入境申报系统的边栏上的“表格下载”里，选择**”报销表格下载“**，表格如下图，里面也标明了一些报销流程和注意事项；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/baoxiaochat.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><div align="center">  <img src="/2020/05/30/baoxiao/items.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="4"><li><p>其它细节事项</p><ul><li>大额机票需要发票验真，通过官网或发票自带的网站都可以，截图打印</li><li>打印护照的出入境记录页</li><li>提供交易记录截图（微信通知，短信账单，订单等均可）</li><li>在发票上签字需要用<strong>油笔</strong></li></ul><p>我们组的报销可以去对门实验室请教<strong>乔老师</strong>，老师会给予很多帮助，在此表示感谢~</p></li></ol><p>本文凭借对半年前的回忆整理，如有疏漏，欢迎大家评论指正！</p>]]></content>
    
    
    <categories>
      
      <category>办公事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
