<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>李宏毅2020——半监督、无监督</title>
    <link href="/2020/12/22/lhy-semi/"/>
    <url>/2020/12/22/lhy-semi/</url>
    
    <content type="html"><![CDATA[<h2 id="半监督"><a class="markdownIt-Anchor" href="#半监督"></a> 半监督</h2><p>semi-supervised，有另外一组无标签数据集且数量远大于带标签集合。分为Transductive learning（使用了testing set的属性/信息）和Inductive learning（训练时不考虑testing set）。</p><p>分为Transductive</p><h2 id="无监督"><a class="markdownIt-Anchor" href="#无监督"></a> 无监督</h2>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅2020——深度学习【其它】</title>
    <link href="/2020/12/21/lhy-advanced/"/>
    <url>/2020/12/21/lhy-advanced/</url>
    
    <content type="html"><![CDATA[<p>包括可解释性、对抗学习、网络压缩等深度学习相关其它方面。</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习</title>
    <link href="/2020/12/16/pytorch-book/"/>
    <url>/2020/12/16/pytorch-book/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅2020——深度学习【基础】</title>
    <link href="/2020/12/16/DL-lhy/"/>
    <url>/2020/12/16/DL-lhy/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a class="markdownIt-Anchor" href="#背景"></a> 背景</h2><h3 id="一-历史发展"><a class="markdownIt-Anchor" href="#一-历史发展"></a> 一. 历史发展</h3><p>1958年感知机模型被提出；1969年提出感知机模型无法解决非线性问题。</p><p>1980s提出多层感知机尝试解决非线性问题，其实与今天的DNN没有特别明显的差别；</p><p>1986年提出反向传播技术，但在超过三层的模型中就无法训练出好的结果；1989年有人提出只需要一层就可以模拟任何函数，加入足够神经元即可，不需要Deep；（那个年代NN非常之不吃香，人们开始尝试…改名）</p><p>2006年，RBM initialization（受限玻尔兹曼机），非常复杂，人们在一段时间里认为这是一个突破，但目前很少在有使用，因为带来的帮助有限。但它让大家再次对NN领域有了兴趣。</p><p>2009年使用GPU加速；2011年开始在语音识别领域流行；2012年赢得了图像领域的比赛。</p><h3 id="二-基础"><a class="markdownIt-Anchor" href="#二-基础"></a> 二. 基础</h3><p>给定了<strong>网络架构</strong>（连接方法，神经元数目，层数），其实就是定义了一个函数集合，我们希望通过不同的w和b找到其中最优的那个函数。</p><p>不同网络架构的区别在于神经元的不同连接方式，常用的有全连接网络。</p><p>深度神经网络它就是很深。。。。特别深的网络需要比较特殊的网络结构，全连接网络太深的话很难训练起来。</p><div align="center">  <img src="/2020/12/16/DL-lhy/deep.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>DNN其实就是一连串矩阵预算（输入向量乘权重矩阵再加上偏量），可以使用GPU进行矩阵运算的加速。DNN的隐藏层可以全部视为特征工程部分，最后输出层视为一个多分类器（加一个softmax）。</p><div align="center">  <img src="/2020/12/16/DL-lhy/matrix.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="1-是否要选择使用dnn"><a class="markdownIt-Anchor" href="#1-是否要选择使用dnn"></a> 1. 是否要选择使用DNN？</h4><p>原本不是DNN的模型，我们需要<strong>找到恰当的特征工程方法</strong>，使用DNN可以直接丢过去，但<strong>问题转化为如何设计合适的DNN网络结构</strong>。我们可以考虑这两个任务哪个更容易解决，来决定是否选择使用深度学习方法。比如在CV或语音识别方面，DNN的效果非常强，但在NLP领域优势没有那么大。</p><h4 id="2-是否可以自动决定网络架构"><a class="markdownIt-Anchor" href="#2-是否可以自动决定网络架构"></a> 2. 是否可以自动决定网络架构？</h4><p>有相关研究（Evolutionary Artificial Neural Networks），但是目前没有被广泛应用起来。</p><p>大家通常选择使用CNN，RNN等架构。</p><h4 id="3-为什么要深度学习"><a class="markdownIt-Anchor" href="#3-为什么要深度学习"></a> 3. 为什么要深度学习？</h4><p>只要有足够多的神经元，一层网络就可以近似所有的函数（宽度学习 &gt;__&lt;）</p><p>“Deeper is Better”？会存在梯度消失的问题，比如损失函数中使用Sigmoid，经过链式法则，靠近输出层的地方有比较大的梯度值更新很快，靠近输入层则相反，导致几乎在已经收敛完成时，靠近输入层还是随机参数状态，陷入局部最小值。</p><div align="center">  <img src="/2020/12/16/DL-lhy/vanish.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div>同等量级参数的情况下，矮胖model和瘦长model哪个好？实验结果表明是后者好。<p>DL更有效率，只需要比较少的神经元，意味着比较少的参数，所以用比较少的数据量就可以运算。模组化、逻辑电路、剪窗花的例子。我们没有足够的训练数据所以比较需要Deep learning（这和传统想法不同啊）。</p><p>在语音识别领域，DL逐渐取代以往《信号与系统》等领域的某些步骤（DCT, log, Filter bank等），google甚至尝试从第二步（DFT）开始就直接使用端到端的深度学习网络，最终效果是与传统方法打平。</p><div align="center">  <img src="/2020/12/16/DL-lhy/speech.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="三-反向传播"><a class="markdownIt-Anchor" href="#三-反向传播"></a> 三. 反向传播</h3><p>DNN里依然是使用<strong>梯度下降</strong>法找最优参数，很多框架都实现了<strong>反向传播</strong>技术，即一种在神经网络中更<strong>有效率</strong>计算偏微分的方法，可以直接使用。大多数深度学习“专家”并不会计算微分。</p><p>最大的问题是DNN中有过多参数，如何有效计算？反向传播就是一种比较有效率的方法，其关键点是<strong>链式法则</strong>。</p><p>两个关键步骤就是：Forward Pass 之后进行 Backword Pass。</p><div align="center">  <img src="/2020/12/16/DL-lhy/back.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h2 id="训练tips"><a class="markdownIt-Anchor" href="#训练tips"></a> 训练Tips</h2><h3 id="一-问题区分"><a class="markdownIt-Anchor" href="#一-问题区分"></a> 一. 问题区分</h3><p>首先检查<strong>在训练集</strong>上有没有得到好的结果（有没有训练起来），之后再看测试集（不要直接上）。</p><p>另外，不要看到所有不好的效果，都说是因为overfitting，你需要先检查训练集上的效果，如果测试集比训练集差才是<strong>过拟合</strong>。也不是叫欠拟合，因为有的时候模型的能力其实是够的，但是因为局部最优解等问题而没有训练好。李宏毅老师认为严格的“欠拟合”概念是模型参数不够多而导致它本身没有能力解决这个问题。</p><div align="center">  <img src="/2020/12/16/DL-lhy/recept.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>针对在<strong>训练集效果不好</strong>和在<strong>测试集效果不好</strong>这两个问题，有不同的处理方法，这个必须要清楚。</p><div align="center">  <img src="/2020/12/16/DL-lhy/recipt.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="1-新的激活函数"><a class="markdownIt-Anchor" href="#1-新的激活函数"></a> 1. 新的激活函数</h4><p>梯度消失、Sigmoid</p><p>ReLU、Leaky ReLu、Paramatic ReLu、Maxout</p><p>早年的解决方法是依次（或分批次）训练不同层。目前常用的方法是更换激活函数，由Sigmoid换成了<strong>ReLU</strong>。ReLU的优势在于：1）计算迅速；2）有生物学基础；3）等同于无穷多带有不同bias的sigmoid函数叠加的结果；4）可以弱化梯度消失问题。</p><p>使用的时候非常粗暴地解决了ReLU在0点无法微分的事情，直接忽略…也有一些变种，比如Leaky ReLU（小于零部分加入0.01的系数），Parametric ReLU（小于零部分加入可学习的系数）、<strong>Maxout</strong>（自动学习激活函数，可以学出ReLU，也可以学出其它形状的激活函数）等。</p><p>Maxout把多个神经元的参数视为一组，然后进行一个类似maxpooling的操作。max操作无法微分，那么maxout如何训练呢？其实对于被max操作选中的参数，模型和线性模型是一样的，其它参数直接拿掉就可以（类似一个开关），每一次有不同的输入时，z的大小会变化，被训练到的参数是不一样的，最终其实几乎全部都可以被训练到。</p><div align="center">  <img src="/2020/12/16/DL-lhy/train.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2-可调整的学习率"><a class="markdownIt-Anchor" href="#2-可调整的学习率"></a> 2. 可调整的学习率</h4><p>DNN的损失函数更加复杂，需要更灵活的学习率，只是用Adagrad可能无法满足。人们提出RMSProp（在Hinton的线上课程上提出来，没有paper）。RMSProp + Momentum形成Adam。</p><h4 id="3-早停"><a class="markdownIt-Anchor" href="#3-早停"></a> 3. 早停</h4><p>在DeepLearning中，正则化和早停的效果差不多，正则化的作用没有像在SVM等方法中这么厉害。</p><h4 id="4-正则化"><a class="markdownIt-Anchor" href="#4-正则化"></a> 4. 正则化</h4><p>正则化一般不考虑bias项，因为它和使函数更平滑没有关系。</p><p>L1和L2略有不同，L1每次减掉一个（固定的）值，而L2是通过乘法来做的。L1训练出来的结果比较稀疏，L2的话权重平均都比较小。</p><h4 id="5-dropout"><a class="markdownIt-Anchor" href="#5-dropout"></a> 5. Dropout</h4><p>每次都训练一个比较瘦长的网络架构，每次训练的网络是不一样的。在testing的阶段需要在参数上乘以dropout rate。</p><p>可以通过ensemble的角度理解dropout的效果。</p><p>感觉DL领域中的甚多地方都是偏工程或理解直觉的，无法通过严谨的理论推理。</p><h2 id="cnn"><a class="markdownIt-Anchor" href="#cnn"></a> ## CNN</h2><p>CNN的关键点是简化网络架构，如果用全连接网路的话，参数过多。可以做到这样的简化是因为：1）神经元为了发现图片中的某种模式并不需要整张图，只需要局部信息即可；2）同种模式可能出现在图片中的不同区域；3）subsampling（下采样）一些像素可能对整个图片没有太大影响。</p><div align="center">  <img src="/2020/12/16/DL-lhy/cnn.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>CNN可以视为将全连接层拿掉一些权重，即没有考虑全部输入信息。</p><p>图片分析中，每一个filter是同时考虑所有channel的，但也有相应的参数。CNN实现的过程中需要将vector搞成高维tensor，另外要考虑filter的大小及个数。</p><p>如何了解CNN模型学到了什么？将模型参数固定，通过梯度上升方法寻找使模型激活结果最大的输入（有一些可解释性AI的意思），如下图所示。选取<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub><mi>j</mi></mrow><annotation encoding="application/x-tex">a_ij</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span>不同的位置可以解释不同层的作用。比如，通过放在输出层有研究工作发现，CNN真正学到的东西和人类的想象不同，《Deep Neural Networks are Easily Fooled》。另外我们也可以通过调整最终的Loss函数，对输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>加入一些限制，使最终找到的CNN理想输入更符合人类的想象。</p><div align="center">  <img src="/2020/12/16/DL-lhy/what_learned.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>找理想输入/可解释性的部分，感觉可以有挺多应用，比如是否可以在恶意软件检测的部分自动抽取Yara规则。</p><p>**什么时候可以用CNN呢？**场景需要具有如下特性：1）局部信息（远小于全局信息）可以反映出某些模式（pattern），比如围棋中的落子模式；2）同样的模式会出现在不同区域，但代表了同样的意义；3）下采样并不会改变整个样本（比如图像识别），这是max pooling的感性基础，但这一条在alpho go那里为什么成立呢？看文章描述alpho go中应该是没有用到pooling。<strong>在应用时一定要考虑场景特性</strong></p><h2 id="gnn视为cnn的扩展"><a class="markdownIt-Anchor" href="#gnn视为cnn的扩展"></a> GNN（视为CNN的扩展）</h2><p>总的路线图如下，分为spatial-based和spectral-based两种，课程中首先讲了前者。GNN中常用的benchmark dataset是CORA, TU-MUTAG。常用任务为：图分类（真的是图像分类，在MNIST和CIFAR10上superpixel）；回归（ZINC）；节点分类（graph pattern recognition、semi-supervised graph clustering）；边分类（旅行商问题，TSP）；</p><div align="center">  <img src="/2020/12/16/DL-lhy/GNN.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="一-spatial-based-gnn"><a class="markdownIt-Anchor" href="#一-spatial-based-gnn"></a> 一. Spatial-based GNN</h3><p>基础操作有Aggregate（等价Convolution）和Readout（代表整个图）。</p><h3 id="二-spectral-based-gnn"><a class="markdownIt-Anchor" href="#二-spectral-based-gnn"></a> 二. Spectral-based GNN</h3><p>从信号与系统过来，经过傅里叶变换之后，卷积操作变成乘法操作即可。</p><p>合成分析，信号可以视为N维空间的向量，我们常用cos(x)或sin(x)作为bases。</p><div align="center">  <img src="/2020/12/16/DL-lhy/f.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>Graph Laplacian， L =  D - A，即度矩阵减去邻接矩阵，最后L是一个半正定矩阵，通过谱分解后，计算出特征值和特征向量。</p><div align="center">  <img src="/2020/12/16/DL-lhy/spectral.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>频率越大，相邻两点间的信号量变化越大。在图上的操作，某种程度上代表了某个节点与其相邻节点信号能量差，由此来量化频率的大小。所以特征值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span>表示频率变化。</p><p>如何在图上将信号在数值与频率意义上的相互转换。</p><div align="center">  <img src="/2020/12/16/DL-lhy/s1.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><div align="center">  <img src="/2020/12/16/DL-lhy/s2.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>在频率意义上如何进行过滤？有问题：1）滤波函数参数与输入节点数呈正比；2）学习滤波器的时候可能了一些不希望学习到的东西，并不是localized的，多次方会考虑多步邻居。</p><p>ChebNet，主打速度快而且可以局部化。选择拉普拉斯多项式函数从而限定在考虑K近邻局部信息，而且限制了参数数量与K成正比。但依然存在计算复杂度太高的问题<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。将一组多项式函数转换成为另一组多项式函数，会使计算更加简单。类比下面的高中试题例子。最后的计算时递归的形式，计算量变为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>K</mi><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(KE)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mclose">)</span></span></span></span>。</p><div align="center">  <img src="/2020/12/16/DL-lhy/chebnet.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>最后变成，需要学习一组/几组参数，处理经过k次递归计算的输入信号。</p><p><strong>GCN</strong></p><div align="center">  <img src="/2020/12/16/DL-lhy/gcn.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>MLP的意思是直接用特征来做，一般被认为是一个benchmark。GCN的效果并没有很强，一般使用GAT或GraphSAGE。</p><p>使用dropedge技巧稍稍拯救一下太深层的GCN。</p><h3 id="三-graph生成"><a class="markdownIt-Anchor" href="#三-graph生成"></a> 三. Graph生成</h3><p>通过VAE 和 GAN做。还有Auto-regressive-based model，每一步生成一个节点/边。</p><p><strong>最终总结</strong></p><div align="center">  <img src="/2020/12/16/DL-lhy/summary.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h2 id="rnn"><a class="markdownIt-Anchor" href="#rnn"></a> RNN</h2><h3 id="一-lstm基础"><a class="markdownIt-Anchor" href="#一-lstm基础"></a> 一. LSTM基础</h3><p>希望神经网络具有记忆，除了考虑输入之外还要考虑存在<strong>memory</strong>里的值，所以就考虑到了输入序列的次序。</p><p>Elman Network （memory里存的是hidden layer中的值）和 Jordan Network（memory里存的是output layer的值）。</p><p>Bidirectional RNN的好处就是上下文兼顾，而不是只处理上文。</p><p>LSTM（Long Short-term Memory）有三个gate，包括input gate（控制否是写入memory），output gate（控制是否可以读出memory）以及forget gate（控制memory要忘掉记得过去的东西）。可以认为LSTM有<strong>四个输入和一个输出</strong>。一般门里面选择sigmoid的函数，表示门被打开的程度。在神经元个数相同的情况下，LSTM是一般神经网络参数的4倍。</p><div align="center">  <img src="/2020/12/16/DL-lhy/lstm.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><div align="center">  <img src="/2020/12/16/DL-lhy/lstm_1.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>LSTM的最终形态，将前一个时间的hidden layer，memory都考虑到输入中来。然后叠加个五六层。</p><div align="center">  <img src="/2020/12/16/DL-lhy/lst_b.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>目前说在做RNN的人基本都是在用LSTM。Keras中实现了三种有，<strong>LSTM，GRU和SimpleRNN</strong>（最开始上课时引入的那个模型）。</p><p>RNN由于同样的参数在不同时间点反复使用（不是激活函数的锅），会有<strong>梯度消失问题</strong>，所以其的训练比较困难。RNN的error surface是非常陡峭的，参数比较容易飞出去，程序就NAN了。使用了非常工程化的一招clipping，即设计了一个阈值。解决的技巧就是<strong>LSTM</strong>，可以解决梯度消失问题（但不解决gradient explode问题）。**为什么LSTM可以做到呢？**因为memory和input是加在一起的，如果当前输入会影响到memory的话，那么这个影响会永远存在（传言，训练LSTM是需要确保forget gate在多数情况下都开启）。</p><p><strong>GRU</strong>只有两个门（input gate 和 forget gate合为一体），需要的参数量更少。LSTM如果过拟合比较严重，可以尝试一下GRU。</p><p>两种注意力机制网络结构，后者为Neural Turing Machine（包含了Writing Head Controller部分）。</p><div align="center">  <img src="/2020/12/16/DL-lhy/attention.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-structural-learning"><a class="markdownIt-Anchor" href="#二-structural-learning"></a> 二. Structural Learning</h3><h4 id="1-统一框架与三大问题"><a class="markdownIt-Anchor" href="#1-统一框架与三大问题"></a> 1. 统一框架与三大问题</h4><p>输入输出不是一个向量，而是一种结构化数据（sequence、list、tree、bounding box等）的时候该如何处理。应用场景包括比如：语音识别、机器翻译、语义解析、目标检测（图像到边框）、文本总结等。统一框架如下，即寻找一个可以衡量输入输出匹配性的函数F，测试阶段穷举所有可能的输出，选取在当前函数F下分值最高的y，作为最终结果。其实也可以将函数F理解为x和y一同出现的几率P。</p><div align="center">  <img src="/2020/12/16/DL-lhy/struct.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>我们需要解决如下三个问题：1）几率函数形式；2）如何找到最恰当的输出；3）如何找到合适的几率函数。可以将简单的DNN视为structural learning的子类，将DNN中的函数f和损失函数一同视为structural learning中的几率函数F。</p><div align="center">  <img src="/2020/12/16/DL-lhy/three.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-三大问题的解决"><a class="markdownIt-Anchor" href="#2-三大问题的解决"></a> 2. 三大问题的解决</h4><p>问题1：抽取features，目标检测中可以使用CNN抽取特征。</p><p>问题2：假装没有问题，已经解决；</p><p>问题3：算法步骤如下，重点在于证明这个方法最后是可以收敛/停止的。</p><div align="center">  <img src="/2020/12/16/DL-lhy/algorithm.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="3-structured-svm"><a class="markdownIt-Anchor" href="#3-structured-svm"></a> 3. Structured SVM</h4><h4 id="4-sequence-labeling"><a class="markdownIt-Anchor" href="#4-sequence-labeling"></a> 4. Sequence Labeling</h4><p>李老师认为GAN也是属于structural learning。</p><h3 id="三-deep-learning-vs-structural-learning"><a class="markdownIt-Anchor" href="#三-deep-learning-vs-structural-learning"></a> 三. Deep Learning vs Structural Learning</h3><div align="center">  <img src="/2020/12/16/DL-lhy/compare.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>现在流行把它们组装起来，未来的趋势是将Deep和Structured组装在一起。</p><div align="center">  <img src="/2020/12/16/DL-lhy/together.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅2020——机器学习</title>
    <link href="/2020/12/14/ML-lhy/"/>
    <url>/2020/12/14/ML-lhy/</url>
    
    <content type="html"><![CDATA[<p>课程视频可以在B站（<a href="https://www.bilibili.com/video/BV1JE411g7XF%EF%BC%89%E6%88%96Youtube%E6%89%BE%E5%88%B0%EF%BC%8C%E8%AF%BE%E7%A8%8B%E5%AE%98%E6%96%B9%E7%BD%91%E7%AB%99%E4%B8%BA%EF%BC%8Chttp://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html%E3%80%82" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1JE411g7XF）或Youtube找到，课程官方网站为，http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html。</a></p><div align="center">  <img src="/2020/12/14/ML-lhy/learn_map.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div>关键点：<p>老师的讲解思路以回归问题入手，使用MSE作为损失函数；之后再扩展/改进到分类问题</p><p>助教对于5大优化器方法（及各类研究型改进方法）的原理、使用场景的介绍。</p><h2 id="基础概念"><a class="markdownIt-Anchor" href="#基础概念"></a> 基础概念</h2><h3 id="一-梯度下降"><a class="markdownIt-Anchor" href="#一-梯度下降"></a> 一. 梯度下降</h3><p>线性回归问题没有局部最小解（损失函数是convex的）。</p><p>求解最优问题时“梯度越大的点距离最优点越远”这个通常只在<strong>仅有一个参数</strong>的时候成立，在<strong>跨参数的时候</strong>就无法进行这样的比对。最好的step应当把当前变量的二次微分也考虑进来。另外，在多个参数的时候也许会不降反增（我的世界视频）。</p><p>梯度下降法的局限不仅限于局部最小值，还有saddle point，或者导数很小的plateau。</p><h4 id="1-学习率"><a class="markdownIt-Anchor" href="#1-学习率"></a> 1. 学习率</h4><p>学习率，通常来讲可以随着参数的调整情况而减小学习率，但准确来讲是根据不同参数及不同更新情况调整学习率，比如<strong>Adagrad</strong>。当然这个方法到后面会计算比较慢。还有一系列表现更好的优化方法，比如常用Adam。</p><p><strong>Adagrad</strong>主要考虑梯度值的**“反差”**（How surprise it is），即当前梯度值与历史值的相对大小。从另一个角度来讲，Adagrad分母部分代表了二次微分的估算（这里是对于相对大小的近似，没有直接计算二次微分是出于复杂度的考虑）。</p><div align="center">  <img src="/2020/12/14/ML-lhy/adagrad.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>**Stochastic Gradient Descent**（SGD），每次只拿一个样本出来计算损失函数值，多次计算，速度快，不稳定。<h4 id="2-feature-scaling"><a class="markdownIt-Anchor" href="#2-feature-scaling"></a> 2. Feature Scaling</h4><p>尽量保证不同特征的取值范围相近，也是为了保证相应的权重对于损失函数的微分影响相近，最终图像更近似于圆形，更容易进行参数更新。Scaling的方法有非常多种，选一个喜欢的，常用的是减去平均值后除以标准差。</p><div align="center">  <img src="/2020/12/14/ML-lhy/scaling.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="3数学基础"><a class="markdownIt-Anchor" href="#3数学基础"></a> 3.数学基础</h4><p>泰勒级数展开，重点在于当x无限接近<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>时，只保留低阶项完成近似。所以学习率需要足够小以保证泰勒级数展开成立。另外，我们一般不会加入二次微分或更高阶的微分来增大学习率的恰当取值范围。</p><div align="center">  <img src="/2020/12/14/ML-lhy/mathbase.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="4五大常用优化方法"><a class="markdownIt-Anchor" href="#4五大常用优化方法"></a> 4.五大常用优化方法</h4><p>关注off-line类型的方法。以下五个是同时（几乎所有时候）工程上用到的优化方法，后三个是adaptive类型，比如在比较平缓的地方可以自动大步走过去，所以收敛速度比较快，但通常收敛结果要弱一些。其它研究型优化方法有非常多，但尝试起来可能比较费力。</p><ul><li>SGD</li><li>SGDM（with Momentum），计算当前参数更新值的时候考虑到过去的更新值。YOLO、ResNet模型训练时有使用。另外1983年提出了NAG方法针对动量进行了修改，look into the future，有一些数学计算保证无需维护两份参数，相当于把计算过程中的动量向前走了一个time step。这个思想用到Adam上就是Nadam方法。</li></ul><div align="center">  <img src="/2020/12/14/ML-lhy/SGDM.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><ul><li>Adagrad，考虑当前梯度相对于历史梯度值的大小。</li><li>RMSProp，是对Adagrad的改进，确保分母部分不会持续增大而导致出现计算结果过小的问题（没走几步就卡住了）。</li><li>Adam，是将SGDM与RMSProp结合；BERT、ADAM、Tacotron、Big-GAN、MEMO等模型都是使用ADAM训练出来的。</li></ul><p>为什么实际应用比较强的模型大多是使用SGDM、Adam训练出来的呢？助教认为这是因为这两个模型先抢到了两个最极端的位置。Adam比较快而SGDM比较稳。</p><div align="center">  <img src="/2020/12/14/ML-lhy/vs.png" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>有人提出SWATS，即开始的时候追求是速度使用Adam，后面使用SGDM。关键问题在于什么时候进行切换，作者并没有证明为什么要在那个点进行切换。</p><p>有一些研究旨在提升Adam，如何让Adam收敛得更稳。比如ICLR‘18论文AMSGrad，通过max操作记住过去最大时候的梯度值，降低影响较小的小梯度值带来的影响，其实有点像走回头路。ICLR’19发表AdaBound提出了两个人工定制的边界，限制动态调整的范围，但助教认为非常粗暴，有点像工程的解决方法。</p><p>另一个方向是从SGDM这里提升，是否可以帮助SGDM找到最佳学习率，提升其收敛速度。比如WACV 17’提出Cyclical LR周期性改变学习率大小（锯齿状信号）。SGDR也类似，只是变了不同的波形。One-cycle LR则是只做一个周期，周期中首先提升而变小，最后再平缓变小。</p><p><strong>Adam是否也需要warm-up？</strong>，答案是肯定的，这样会使它前面几次迭代结果不会太乱。工程上的wram-up经常是自己定义一个曲线。也有一些研究工作提出其它warm-up方法。ICLR’20 提出RAdam方法，开始的时候使用SGDM，后面用了改动后的Adam。</p><div align="center">  <img src="/2020/12/14/ML-lhy/radam.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>Geoffrey Hinton团队提出<strong>Lookahead方法</strong>，可以包装在现有各类优化算法外面，关键点是“K setp forward，1 step back”。这样做使梯度下降更稳定，better generalization，因为尽量保持在比较平坦的区域而不会进入太危险的区域。</p><p>计算SGDM和Adam时需不需要考虑L2正则项的参数呢？有研究结果表明最好是在计算动量或历史加和的时候不要加入，而在计算更新值的时候加入，提出的算法名为<strong>AdamW和SGDW</strong>，与大部分优化算法不同，AdamW是真正被工业界使用过的。</p><div align="center">  <img src="/2020/12/14/ML-lhy/all.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>通常的适用情况如下，并没有一个通用万能的优化器。更换更恰当的优化器的作用只能是在模型训练起来以后，提升一些性能；如果模型训练不起来，那可能是数据或架构本身有问题，通过更换优化器并不能解决。</p><div align="center">  <img src="/2020/12/14/ML-lhy/env.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="5-优化技巧"><a class="markdownIt-Anchor" href="#5-优化技巧"></a> 5. 优化技巧</h4><p>Shuffling、Dropout、Gradient noise（加入高斯噪声，随着时间标准差减小）</p><p>Warm up（一开始学习率定比较小，慢慢稳定之后调大）、Curriculum Learning、Fine-tuning；这几个技巧的本质是先用简单的数据或任务训练模型，之后再慢慢增加难度。</p><p>Normalization，Regularization，避免模型学到过于极端的参数。</p><h3 id="二-正则化"><a class="markdownIt-Anchor" href="#二-正则化"></a> 二. 正则化</h3><p>比较简单的model受不同数据分布的影响相对较小。如果模型的error主要来自于variance，则情况为overfitting（过拟合），需要增加训练集或加入正则项；如果主要来自bias，则情况为underfitting（欠拟合），需要重新设计模型，加入更多特征信息等。更直观的过拟合/欠拟合评价标准是观察模型在训练集和测试集上的误差表现。</p><div align="center">  <img src="/2020/12/14/ML-lhy/variance.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>L2正则会倾向得到更加平滑的函数。</p><p>做正则化的时候并不需要考虑bias项。</p><h3 id="三-验证集"><a class="markdownIt-Anchor" href="#三-验证集"></a> 三. 验证集</h3><p>为什么要划分出validation set？ 因为这样的话，才没有在结果中考虑到public testing set的影响，从而可以更贴近模型在private testing set上的表现。一般进行N折交叉验证（N-fold cross Validation）选择模型。注意，模型选择后还要在整个训练集上重新训练一下。</p><div align="center">  <img src="/2020/12/14/ML-lhy/validation.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h2 id="分类问题"><a class="markdownIt-Anchor" href="#分类问题"></a> 分类问题</h2><p>**为什么不可以用回归方法硬解分类问题？**比如结果大于0是一类，小于0是另一类。很大的问题你很难通过回归方法得到准确模型，比如下图右下角数据会对模型产生很大影响，最终会选择紫色线模型。而且使用回归方法解分类问题，在定义不同类别的时候，隐含着加入了大小关系。</p><div align="center">  <img src="/2020/12/14/ML-lhy/regress.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><h3 id="一-生成方法"><a class="markdownIt-Anchor" href="#一-生成方法"></a> 一. 生成方法</h3><p>生成模型的关键三步，最初思想为贝叶斯公式。</p><div align="center">  <img src="/2020/12/14/ML-lhy/steps.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>假设服从<strong>高斯分布</strong>（不同情况下可以使用不同分布，比如伯努利等），关键参数为mean和covariance matrix，即均值和协方差矩阵。使用<strong>最大似然估计</strong>算这两个参数值，即从样本中估计出最可能的参数模型。</p><p>通常不会给每个分类都有自己的mean和covariance matrix（大小与输入的属性个数成平方），一般会计算出<strong>统一的协方差矩阵</strong>，有效减少参数。而且这种情况下分类boundary变成了直线，模型也属于<strong>线性模型</strong>。数学推导如下，几经展开、消去等运算后，变成线性模型加一个sigmoid函数。另外，在高维空间，如果假设每个属性的分布是独立分布的，这就是<strong>朴素贝叶斯</strong>（Naive），这个假设还是比较强的。</p><div align="center">  <img src="/2020/12/14/ML-lhy/math1.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/12/14/ML-lhy/math2.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>在生成模型中，我们直接计算出权重w和偏移b，而在目前比较流行的判别方法中，我们使用各种优化方法逐步寻找最优的w和b。</p><h3 id="二-逻辑斯特回归"><a class="markdownIt-Anchor" href="#二-逻辑斯特回归"></a> 二. 逻辑斯特回归</h3><p>损失函数使用<strong>伯努利分布交叉熵</strong>，而不直接像线性回归一样直接使用MSE，原因是这样的损失函数在逻辑斯特回归上过于平坦。详细来说，如果使用MSE做损失函数，求偏微分之后，由结果公式看到在距离目标很远的时候，微分值也很小，更新速度非常慢。</p><div align="center">  <img src="/2020/12/14/ML-lhy/cross.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>但求偏微分之后，逻辑斯特与线性回归的参数更新表达式是一样的。</p><div align="center">  <img src="/2020/12/14/ML-lhy/logistic.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>多分类情况</strong>下，用Softmax（二分类问题下就是sigmoid）。Softmax会对比较大的结果值进行强化，使最大值辨识度更高。</p><div align="center">  <img src="/2020/12/14/ML-lhy/softmax.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>逻辑斯特模型局限，其为线性模型，无法直接处理非线性问题（比如异或问题）。一种解决方法为Feature Transformation，但恰当的方法比较难找。我们希望让机器自动找到合适的方法，可以通过级联多个逻辑斯特模型实现。我们可以给它们一个新的名字，把每一个逻辑斯特模型叫做神经元，整个模型叫做神经网络，然后搞到层数多一些，就有了深度~</p><div align="center">  <img src="/2020/12/14/ML-lhy/multi.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="三-生成方法-vs-判别方法"><a class="markdownIt-Anchor" href="#三-生成方法-vs-判别方法"></a> 三. 生成方法 vs 判别方法</h3><p>模型公式相同，但我们极大可能找不到相同的w和b参数。因为两类方法有不同的假设。比如逻辑斯特回归对数据分布没有任何假设，是直接梯度下降找最优解；朴素贝叶斯则假设了独立同分布、高斯分布等等。</p><div align="center">  <img src="/2020/12/14/ML-lhy/wb.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>常常在文献上有人讲，判别方法比生成方法效果好。但有些时候生成方法也是有优势的（因为此类方法加入了对数据分布的假设，相当于有了一些脑补），比如1）训练数据量很少的时候；2）有某些噪声；3）先验与类相关概率可以通过不同数据来源估计。</p><h2 id="作业"><a class="markdownIt-Anchor" href="#作业"></a> 作业</h2><h3 id="1hw2"><a class="markdownIt-Anchor" href="#1hw2"></a> 1.HW2</h3><p>生成模型关于协方差矩阵求逆的部分，“Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error. Via SVD decomposition, one can get matrix inverse efficiently and accurately.”</p><pre><code class="hljs python">u, s, v = np.linalg.svd(cov, full_matrices=<span class="hljs-literal">False</span>)inv = np.matmul(v.T * <span class="hljs-number">1</span> / s, u.T)</code></pre>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大规模图数据分析技术：挑战与机遇</title>
    <link href="/2020/11/16/talk-1/"/>
    <url>/2020/11/16/talk-1/</url>
    
    <content type="html"><![CDATA[<p>2020年11月16日，FIT楼多功能厅，听取樊文飞教授报告。以下为报告笔记，因为背景知识有限，听取现场报告，可能会有一些遗漏或理解错误之处。</p><p>当前图数据已成为大数据分析场景下的重要数据来源，老师在报告中就<strong>4V问题</strong>（Volume，Variety，Velocity，Veracity/value），分析图数据存储、分析中遇到的问题与部分解决思路。</p><h4 id="1volume体量"><a class="markdownIt-Anchor" href="#1volume体量"></a> 1.Volume（体量）</h4><p>大型图网络中包含百亿节点，百亿条边，如何存储处理？</p><p>DFS虽然是线性复杂度，但在大图上已经难以运行。</p><p>使用并行计算系统完成大图处理，在工业界存在很多问题：</p><p>1）已知的图算法能否并行化，学术界有很多这样的尝试比如google的pregel，CMU的Graphlab/PowerGraph，Facebook的Giraph，Berkeley的GraphX，IBM的Giraph++，但工业界未使用。樊老师团队有此类工作发表在SIGMOD，同时项目被阿里收购，今年开源为GraphScope。</p><div align="center">  <img src="/2020/11/16/talk-1/graphscope.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>2）并行计算中如何选择同步或异步，樊老师团队提出AAP工作，《Adaptive Asynchronous Parallelization of Graph Algorithms》</p><p>2）如何衡量某个并行算法是有效的？</p><p>3）传统计算复杂性理论在并行计算环境下有什么样的新表现？</p><h4 id="2velocity动态"><a class="markdownIt-Anchor" href="#2velocity动态"></a> 2.Velocity（动态）</h4><p>实际中的网络动态变化，但变化部分相对原网络占比较小，如何高效处理变化的信息？</p><p>计算output的变化。</p><p>1）很多被证明不是bounded的问题仍要解决，樊老师团队提出relative bounded，《Bounded incremental graph computations: Undoable and doable》。</p><p>2）如何提出增量式的算法，《Incrementalization of graph partitioning algorithms》VLDB 2020。</p><h4 id="3-variety异构"><a class="markdownIt-Anchor" href="#3-variety异构"></a> 3. Variety（异构）</h4><p>如何达成图数据库与原有关系型数据库的统一？比如阿里提到的数据中台概念。但这一方向距离落地还有很多工作要做。樊老师提出gSQL概念，以及联邦数据库（这里与联邦学习的概念不同，强调的是多类型数据的联邦）。</p><div align="center">  <img src="/2020/11/16/talk-1/gsql.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="4-veracity质量"><a class="markdownIt-Anchor" href="#4-veracity质量"></a> 4. Veracity（质量）</h4><p>存在过时数据、链接丢失与语义不一致等问题，如何解决？这个问题是4V中解决度最低的。</p><p>樊老师认为可以将logic规则与AI统一在一个框架下，同时还可以提高ML的可解释性。</p><div align="center">  <img src="/2020/11/16/talk-1/ml-1.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/11/16/talk-1/ml-2.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/11/16/talk-1/ml-3.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3><p>大数据场景下，图数据分析在4V中存在如下挑战：</p><ul><li>Volume，并行处理——reduction，complete problems等</li><li>Velocity，增量计算理论与实践</li><li>Variety，在SQL上做到图数据库与关系型数据库的统一</li><li>Veracity，统一规则（logic）与ML；图数据清洗；链路预测等</li></ul><h3 id="相关文献"><a class="markdownIt-Anchor" href="#相关文献"></a> 相关文献</h3><ol><li>Application driven graph partitions. SIGMOD 2020</li><li>Capturing associations in graphs VLDB 2020</li><li>Incrementalization of graph partitioning algorithms. VLDB 2020</li><li>Graph algorithms: Parallelization ans scalability. Science China Information Sciences, 2020</li><li>Adaptive asynchronous paralleization of graph algorithms, TODS2020</li><li>Deducing certain fixes to graphs, VLDB 2020</li><li>Parallelizing sequential graph computations TODS 2018</li><li>Incremental graph compulations: doable and undoable SIGMOD 2017</li></ol><h3 id="思考"><a class="markdownIt-Anchor" href="#思考"></a> 思考</h3><p>1）有关数据质量的部分，老师提到将规则与AI纳入统一框架，这个让我联想到网络安全领域目前基于规则和基于AI的方法，是否可以进行结合？比如Yara规则与CFG恶意软件检测等。这部分可以搜一下相关资料，樊老师并没有更详细地分享。</p><p>2）<strong>理论与系统</strong>是计算机专业领域的核心，要结合考虑理论研究的应用落实，同时学会如何在项目开发中发现待解决问题。</p><p>3）科研技术与产业落地之间还会有比如，政策、安全性、市场等多方面问题，需要综合考虑。比如XML并没有实现统一，比如在当前已有标准的情况下，为什么不能把关系型数据库全部转化为图数据存储。</p>]]></content>
    
    
    <categories>
      
      <category>交流报告</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
      <tag>大数据分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【小故事】无法消散</title>
    <link href="/2020/07/07/story-1/"/>
    <url>/2020/07/07/story-1/</url>
    
    <content type="html"><![CDATA[<p>下午四点半，9号楼的小洛死了。</p><p>一个女孩就这么从楼上摔下去，“肝脑涂地”，围观的人们在惋惜中带着阵阵恶心。</p><p>警队来了，120无聊地待在一边。</p><p>小洛家在十一楼，电梯坏了，郝队爬得气喘吁吁，“大夏天的，整这出？”</p><p>小洛和父母同住，郝队进门，家里一尘不染，物件规整，门口立着扫帚，卫生间里的拖布还没晾干。阳台的花鸟，墙上的字画，尤其沙发、床头随处可见的书籍，显示出这家人对文化生活的习惯，不是那种张扬在外的追捧。</p><p>“没有打斗的痕迹，看来是失足坠楼。”</p><p>小洛摔下去的地方正对着家里小阳台的窗户，窗页大敞着，郝队看了看，有齐腰高，如果不是故意爬上去，应该没可能发生意外。奇怪的是，周围窗台也一尘不染，连在附近活动过的痕迹都没有。如果是失足坠落，这窗户就显得过分冷静了。</p><p>“真会给我出难题。”</p><p>在屋内转了一圈，没获得什么实质性线索，郝队转过头来询问家人亲友。据了解，小洛是个成绩不错的学生，正在读博士，平时安安静静，不太可能涉及校园贷、勒索威胁等杂七杂八的事情。小洛有近期和同学出游的计划，前一天洛爸还听见她在电话上兴奋地讨论行程安排。听同学说，小洛最近有一篇论文在写，她还报名了半个月之后的一场线上比赛。哦，还有在小区附近的美容院里预约了两天后的祛痘清洁服务。</p><p>“你家孩子近期遇到什么事情了么？”，  “没有啊，疫情在家，每天平平淡淡的，哪有什么大事。” 洛妈已经哭晕了，都是洛爸在撑着回答。</p><p>“孩子人际交往怎么样？”</p><p>“性格有些内向，打小害羞，爱自己一个人玩，但长大就好很多，上学也结识了几个挺不错的朋友，这几天晚上还经常一起打游戏聊天呢。”</p><p>“那她平时生活状态怎么样？我看她是博士生，是不是课业压力挺大的？”</p><p>“刚读博的时候确实是，她老觉得毕业没希望，打电话回家也都挺沮丧的，后来发了两篇论文，就好很多了。我孩子不可能自杀，她最近也作息规律，偶尔锻炼，跟我俩聊天，都很好的，警官您可得仔细给查查啊，我们都配合，都配合。”</p><p>“不像啊”，郝队心想，“这感觉过得挺好。” 忽然，郝队看到垃圾桶里有个弯了的勺子，是平时做饭用的不锈钢厨具。要不是强外力，勺子不可能拧成这样。刚刚在书房里发现的塑料碎片，应该就是这勺子把手上掉下来的。郝队一阵激动，“这案子应该另有隐情。”</p><p>其实，这就是一场简单的自杀。</p><p>小洛是个理解力远超表达力几个维度的人，这样的人是孤独的，她在期盼与失落中交替，觉得没劲，就走了。</p><p>临走的几个小时前，又一次失落后，小洛很愤怒，刷碗时猛地把勺子砸向地板。看着弯折的勺子和四散的勺柄，她觉得挺好笑的。小洛有很多年，或者甚至说是从小，就不会生气，她好像总能站过去理解对面的逻辑，然后承认现实。但承认现实，安慰不了自己。因为无法被理解，所以时常失落，又因为能理解，所以她的失落没有焦点。</p><p>不如算了吧，通过模仿别人而产生的烟火气，总也不能落地，搞得大家都麻烦。后来她想到，家里人都爱干净，就一一收拾好，还彻底打扫了卫生。以往这种时候，洛妈回来都会表扬几句，小洛听着，觉得“你开心就好”。</p><p>窗台上，小洛一边擦去最后的痕迹，一边记起初中时，在思想政治书上背过，“热爱生活，珍惜生命，回报父母，贡献社会”。真没办法，对不起当时考出的98分。下坠时，她记起，之前和同学讨论起生命的意义、自杀等问题，同学说，“死不死得无所谓，走之前可以把角膜啥的这些器官，捐献给那些想活着的人”。</p><p>我走了，就任你们处置了，最好可以尽快消散掉。不过，好像没大可能，而且又要为当代青年抹黑了，真是不好意思。</p><p>“现在这些年轻人啊，生活条件那么好了，蜜罐里泡大，心理素质就是差。”</p>]]></content>
    
    
    <categories>
      
      <category>原创故事</category>
      
    </categories>
    
    
    <tags>
      
      <tag>瞎写</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>这个博客的搭建过程</title>
    <link href="/2020/06/01/build-blog/"/>
    <url>/2020/06/01/build-blog/</url>
    
    <content type="html"><![CDATA[<p>本文重点介绍基于Hexo+Github搭建个人网站流程，最初基本源自<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog" target="_blank" rel="noopener">这篇文章</a>（如何使用Github从零开始搭建一个博客），作者把步骤已经介绍得非常详细完善了。我只是补充踩到的一些坑….</p><p>后来，我背弃了极简审美，开始使用<strong>Fluid</strong>主题。</p><h3 id="一-基础搭建"><a class="markdownIt-Anchor" href="#一-基础搭建"></a> 一. 基础搭建</h3><p>搭建过程使用的两个最基本、最重要的东西是Hexo和Github，其中前者是一个轻量级博客框架，支持将Markdown编写的文章直接编译为静态网页文件并发布，省去了数据库问题。Github则用来解决域名问题，其Github Pages允许每个用户创建一个名为{username}.github.io的仓库，发布博客网页。当然也可以自己申请域名，使用CNAME跳转。</p><h4 id="1-github创建仓库"><a class="markdownIt-Anchor" href="#1-github创建仓库"></a> 1. Github创建仓库</h4><p>在Github上创建一个名为{username}.github.io的仓库，注意必须是github.io结尾。比如我的github账户为“DeepDeer”，创建仓库为“<a href="http://deepdeer.github.io" target="_blank" rel="noopener">deepdeer.github.io</a>”。另外，申请对应仓库时不要弄成private的，否则开放博客Github要收费哈。</p><h4 id="2-安装环境"><a class="markdownIt-Anchor" href="#2-安装环境"></a> 2. 安装环境</h4><p>首先在自己电脑上安装Node.js，确保环境变量配置好，可以使用npm命令；</p><p>其次使用npm命令安装Hexo，安装后确保可以使用<code>hexo</code>命令。</p><pre><code class="hljs bash">npm install -g hexo-cli</code></pre><h4 id="3-初始化项目"><a class="markdownIt-Anchor" href="#3-初始化项目"></a> 3. 初始化项目</h4><p>选定存储博客文件的位置，在此文件夹中使用如下命令创建项目及对应文件夹：</p><pre><code class="hljs bash">hexo init &#123;name&#125;</code></pre><p>命令下产生的文件夹包括themes、source等文件夹，调用如下命令，则在public文件夹中生成js、css、font等内容。</p><pre><code class="hljs verilog">hexo <span class="hljs-keyword">generate</span></code></pre><p>使用server命令在本地运行博客，可以看到类似结果：</p><pre><code class="hljs routeros">hexo<span class="hljs-built_in"> server </span> #或简写为 hexo s</code></pre><div align="center">  <img src="/2020/06/01/build-blog/hello.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/hexo.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><h4 id="4-部署至github"><a class="markdownIt-Anchor" href="#4-部署至github"></a> 4. 部署至Github</h4><p>安装一个支持Git的部署插件</p><pre><code class="hljs sql">npm <span class="hljs-keyword">install</span> hexo-deployer-git <span class="hljs-comment">--save</span></code></pre><p>修改Hexo的配置文件_config.yml，找到Deployment部分，修改为如下内容：</p><pre><code class="hljs bash"><span class="hljs-comment"># Deployment</span><span class="hljs-comment">## Docs: https://hexo.io/docs/deployment.html</span>deploy:  <span class="hljs-built_in">type</span>: git  repo: git@github.com:DeepDeer/deepdeer.github.io <span class="hljs-comment">#你自己的Github仓库地址</span>  branch: master</code></pre><p>使用deploy命令部署后，可通过域名deepdeer.github.io访问，Github上传代码如下：</p><pre><code class="hljs ebnf"><span class="hljs-attribute">hexo deploy</span></code></pre><div align="center">  <img src="/2020/06/01/build-blog/github.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p><a href="http://xn--deploy-hp7ik1vdf32vsxrqigxw5bw73eklg.sh" target="_blank" rel="noopener">可编写如下内容脚本deploy.sh</a>，此后每当有内容更新时，<code>.deploy.sh</code>运行脚本即可。</p><pre><code class="hljs bash">hexo cleanhexo generatehexo deploy</code></pre><h3 id="二-加入主题"><a class="markdownIt-Anchor" href="#二-加入主题"></a> 二.  加入主题</h3><p>目前我加入的是Fluid主题（因为看上了颜值），之前用过一段时间Next主题，也很推荐。</p><h4 id="1-next主题"><a class="markdownIt-Anchor" href="#1-next主题"></a> 1. Next主题</h4><p>有关Next主题的配置及各种插件，在<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog" target="_blank" rel="noopener">这篇文章</a>中介绍地非常详细，这里只补充有关1）添加Gitalk插件和2）修改字体部分。</p><h5 id="1-gitalk插件"><a class="markdownIt-Anchor" href="#1-gitalk插件"></a> 1&gt; Gitalk插件</h5><p>申请Gitalk就在Github个人账户的settings——&gt; Developer settings ——&gt; OAuth Apps，点击 New OAuth App，出现申请界面。其中应用名称随便写就行，Hompage URL和Authorization callback URL写博客链接。如果有自己的域名可以更改Authorization callback URL。点击注册，生成Client ID和Client Secret。</p><p>注意，如果自己配置了域名，这个callback URL要改成自定义域名</p><div align="center">  <img src="/2020/06/01/build-blog/oauth.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>在配置了_config.yml文件后，第一次进入界面会出现下图效果。如果点击Github登录后跳转到了404界面，那么, 就说明配错了。我当时是在写_config.yml忘了把client_id, client_secret字段带的{ }去掉。这给我一顿google啊…</p><div align="center">  <img src="/2020/06/01/build-blog/begin.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>最后效果就像这个博客里一样，相关评论会显示在对应仓库的issues里，记得在仓库的settings里把features—&gt;issues勾选上（貌似默认就是开启的）</p><div align="center">  <img src="/2020/06/01/build-blog/comment.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/issues.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><h5 id="2-修改字体"><a class="markdownIt-Anchor" href="#2-修改字体"></a> 2&gt; 修改字体</h5><p>Next主题默认的博文正文字体大小有点大了，可以在配置文件里改一下。相关配置在hexo\themes\next\source\css\variables路径下的base.styl文件里的Font Size部分。这里面每个变量控制某一部分的字体大小，我是挨个试出来的font-size-large是正文字体（简单粗暴，真开心…)</p><div align="center">  <img src="/2020/06/01/build-blog/font.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>另外，在Hexo配置文件和Next主题配置文件中，都有一些有关网站信息的配置选项，最终使用Next主题搭建出的网站效果如下：</p><div align="center">  <img src="/2020/06/01/build-blog/next.jpg" srcset="/img/loading.gif" width="70%" height="50%" alt="oauth"></div><h4 id="2fluid主题"><a class="markdownIt-Anchor" href="#2fluid主题"></a> 2.Fluid主题</h4><p>其实这个主题有非常好的<a href="https://fluid-dev.github.io/hexo-fluid-docs/guide/" target="_blank" rel="noopener">配置指南</a>，其配置文件_config.yml的注释也很清晰，可以从头摸索。有几点经验包括1）图片插入；2）评论插件。</p><h5 id="1-图片插入"><a class="markdownIt-Anchor" href="#1-图片插入"></a> 1&gt; 图片插入</h5><p>需要注意的一点是，在Fluid主题下有文章背景图等存在于框架中的图片，这些图片一律存放在<code>./themes/fluid/source/img</code>文件夹下。即使是某个文章的缩略图也是这样。</p><div align="center">  <img src="/2020/06/01/build-blog/pic.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>其他存在于文章中的图片，可用如下形式添加。</p><p>首先，把_config.yml文件里的post_asset_folder选项设置为true。</p><p>其次安装一个插件，据说原有插件有一些bug，下面是修改过的插件，亲测有效，感谢<a href="https://www.jianshu.com/p/3db6a61d3782" target="_blank" rel="noopener">这篇博客</a></p><pre><code class="hljs vim">npm install http<span class="hljs-variable">s:</span>//github.<span class="hljs-keyword">com</span>/<span class="hljs-number">7</span>ym0n/hexo-asset-image --<span class="hljs-keyword">sa</span></code></pre><p>有了这些配置后，再运行hexo new xxx，在/source/_posts/路径下，除了可以生成新文章xxx.md之外，还生成一个同名文件夹。插入图片时放到这个文件夹里即可，在markdown里用如下语句：</p><pre><code class="hljs routeros">&lt;img <span class="hljs-attribute">src</span>=<span class="hljs-string">"xxx/图片名称.png"</span> <span class="hljs-attribute">alt</span>=<span class="hljs-string">"图片标识"</span> <span class="hljs-attribute">style</span>=<span class="hljs-string">"zoom:30%;"</span> /&gt;</code></pre><p>但是，在Fluid主题下，这些图片并没有默认居中，可以采用如下HTML代码控制位置和大小：</p><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">align</span>=<span class="hljs-string">center</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">img</span> <span class="hljs-attr">src</span>=<span class="hljs-string">"build-blog/pic.jpg"</span> <span class="hljs-attr">width</span>=<span class="hljs-string">"30%"</span> <span class="hljs-attr">height</span>=<span class="hljs-string">"30%"</span> <span class="hljs-attr">alt</span>=<span class="hljs-string">"oauth"</span>  /&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span></code></pre><h5 id="2评论插件"><a class="markdownIt-Anchor" href="#2评论插件"></a> 2&gt;评论插件</h5><p>Fluid推荐的utteranc.es插件，经常会有加载比较慢的问题，乍一看以为不让评论…</p><p>这个配置过程也很简单。</p><p>首先在github创建一个公开的仓库，比如命名为’deepdeer_comments’。</p><p>点击<a href="https://github.com/apps/utterances" target="_blank" rel="noopener">这个链接</a>安装应用，选择“only select repositories”选项，找到刚刚建立好的仓库，点击install。</p><p>在配置中填写repo名，格式为“用户名/仓库名”，如“DeepDeer/deepdeer_comments”。Issue的命名方式建议选择第一个“Issue title contains page pathname”。</p><p>根据个人喜好选择主题之后，最后一栏会自动生成配置信息，复制这些信息。</p><p>在fluid主题的配置文件中，找到<code>comments</code>部分，将enable设置为true，并将type写成utterances。</p><p>在后面的comments具体配置部分，改成之前自动生成的配置。</p><div align="center">  <img src="/2020/06/01/build-blog/utter.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p>这个博客目前用的是Gitalk插件，配置与Next主题中提到的大致相同。Fluid代码中该插件配置有问题，评论无法分页显示，感谢<a href="https://juejin.im/post/5ed177e36fb9a047923a39fe" target="_blank" rel="noopener">这篇文章</a>。即更改fluid主题下的<code>layout/_partial/comments/gitalk.ejs</code>文件内容中的’id’一栏部分</p><pre><code class="hljs python"><span class="hljs-comment">#原有的</span>id: <span class="hljs-string">'&lt;%- md5(theme.gitalk.id) %&gt;'</span>,<span class="hljs-comment">#改正后</span>id: &lt;%- theme.gitalk.id %&gt;,</code></pre><h3 id="三-自定义域名"><a class="markdownIt-Anchor" href="#三-自定义域名"></a> 三. 自定义域名</h3><p>本博客使用了阿里云上购买的域名。</p><p>在<a href="https://wanwang.aliyun.com/domain/searchresult/?keyword=skylasun&suffix=.cn#/?keyword=skylasun&suffix=cn" target="_blank" rel="noopener">这里</a>点击“控制台”，登录后，找到边栏中的“域名”。选择“域名注册”</p><div align="center">  <img src="/2020/06/01/build-blog/domain.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/reg.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p>进入后，查询你喜欢的关键字相关的域名的注册情况，选择中意的域名就可以交钱了。最终付款之前还需要一些身份认证。</p><div align="center">  <img src="/2020/06/01/build-blog/buy.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>购买成功，认证通过后，在已有域名那里，点击“解析”，添加解析规则。这里添加的IP地址是之前deepdeer.github.io的解析情况，可以通过各类IP或域名查询网站找到，比如<a href="https://site.ip138.com/" target="_blank" rel="noopener">这里</a>。注意下面要加上一条CNMA规则。</p><div align="center">  <img src="/2020/06/01/build-blog/map.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/ip.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>另外，在Github仓库的Settings里，需要加上“Custom domain”，保存配置后，会自动生成名为CNAME的文件，内容如下。但需要注意的是，每次我们重新部署时，使用deploy clean再generate后，会清除掉这个CNAME文件。为解决这个问题，可以把CNAME文件放到博客的“source”文件夹中。</p><div align="center">  <img src="/2020/06/01/build-blog/cname.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><h3 id="四-其它小经验"><a class="markdownIt-Anchor" href="#四-其它小经验"></a> 四. 其它小经验</h3><h4 id="1markdown编辑器推荐"><a class="markdownIt-Anchor" href="#1markdown编辑器推荐"></a> 1.Markdown编辑器推荐</h4><p>这些博客都是用<a href="https://www.typora.io/" target="_blank" rel="noopener">Typora</a>写的。该软件界面简洁，即时效果，很好用，推荐~</p><p>Typora除了支持公式块之外，还支持行内公式，在偏好设置中勾选”内联公式“即可。</p><div align="center">  <img src="/2020/06/01/build-blog/gongshi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2markdown中内容折叠"><a class="markdownIt-Anchor" href="#2markdown中内容折叠"></a> 2.Markdown中内容折叠</h4><p>有时文章内容过多不便于显示，可以使用如下语法进行折叠</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">details</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">summary</span>&gt;</span>可显示的标题<span class="hljs-tag">&lt;/<span class="hljs-name">summary</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">pre</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">code</span>&gt;</span>   折叠内容  <span class="hljs-tag">&lt;/<span class="hljs-name">code</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">pre</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">details</span>&gt;</span></code></pre><p>比如</p><div align="center">  <img src="/2020/06/01/build-blog/zhedie.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>效果如下，点击后显示黑色部分</p><div align="center">  <img src="/2020/06/01/build-blog/xiaoguo.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>如果有其它的坑，欢迎大家评论补充，谢谢！</p>]]></content>
    
    
    <categories>
      
      <category>教程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>工程技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>THU研究生国际会议出行准备流程</title>
    <link href="/2020/05/30/baoxiao/"/>
    <url>/2020/05/30/baoxiao/</url>
    
    <content type="html"><![CDATA[<p>下文仅限清华大学网络科学与网络空间研究院研究生同学使用，包含护照、签证、报销等</p><p>（邓峰基金部分持续更新….）</p><blockquote><p>提示：出国手续涉及部门较多，请尽早准备提前办理。如遇假期会有所调整，要关注邮件通知~</p></blockquote><h3 id="首先"><a class="markdownIt-Anchor" href="#首先"></a> 首先</h3><p>​你要有个<strong>护照</strong>，如果没有，办理的时候把发票留好，可以报销。</p><h3 id="前期准备"><a class="markdownIt-Anchor" href="#前期准备"></a> 前期准备</h3><p>注册会议并拿到<strong>邀请函</strong></p><p>预定<strong>机票和酒店</strong>（办签证使用）。目前携程等网站貌似不再支持付款前先打印行程单。</p><ol><li>机票时间，一般可定在会议安排往前往后各一天。有特殊情况，赶在自己文章汇报前到达即可。</li><li>酒店一般订会议推荐的，预定前注意一下学校给的当地住宿报销额度。有些会议官网会贴出提前订酒店有优惠的通知，发邮件过去即可。</li></ol><h3 id="学校审批"><a class="markdownIt-Anchor" href="#学校审批"></a> 学校审批</h3><h4 id="1申请出国批件"><a class="markdownIt-Anchor" href="#1申请出国批件"></a> 1&gt;申请出国批件</h4><ol><li>首先在info上进行申请，找“出国出境申报”——&gt;“因公出国（境）申报系统（新）</li></ol><div align="center">  <img src="/2020/05/30/baoxiao/apply.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><ol start="2"><li>进入系统后，选择”新申请”，点击“我已阅读”，在因公出境申请表上填写信息，其中会<strong>比较犹豫的几个字段</strong>有：</li></ol><ul><li>出访基本信息：出入境时间大概在会议日程往前往后各一天，离境、入境时间是否需要过境等如实填写即可</li><li>出访类别：单位公派，会议</li><li>出访经费：费用来源一般选择“全部校内支付”，“纵向科研经费”，校内支付，人民币（大致写一个费用即可）</li><li>日程计划：简单填写就行，比如出发，抵达，开会，回程等等（可适当扩展）</li></ul><ol start="3"><li><p>提交之后会有一个预算表，大概可以看到给当地的住宿、日常消费额度等。这类信息也可以在边栏中“预算、外汇与报销”的“政策与标准”的表格中看到。</p><div align="center">  <img src="/2020/05/30/baoxiao/biaozhun.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div></li><li><p>“报批材料”里上传会议的邀请函和论文录用证明。</p></li><li><p>点击提交，打印申请表，这个表需要自己和导师签字。</p></li><li><p>提交完成后，返回主界面会显示出当前进度，完成后圆圈会变绿。大概两周左右“单位审核”会变绿。等到“学校审批”通过，显示批件下达之后，可以下载电子版。</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/jindu.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div><h4 id="2批件领取"><a class="markdownIt-Anchor" href="#2批件领取"></a> 2&gt;批件领取</h4><p>去国际处，在李兆基4楼（可以进楼的门有点多，但失之毫厘谬以千里，所以可以问下保安…）</p><p>去之前先准备一份**“派出证明”<strong>。还是在刚刚的出入境申请系统的边栏里面。点击进去下载对应模板，注意老师们已经用最直接醒目的方法标示出的</strong>注意事项**。</p><div align="center">  <img src="/2020/05/30/baoxiao/chat.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div><div align="center">  <img src="/2020/05/30/baoxiao/chats.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><div align="center">  <img src="/2020/05/30/baoxiao/attention.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>在国际处主要有以下<strong>几个事情</strong>：</p><ul><li>拿批件</li><li>拿外汇预算单</li><li>派出证明需要盖章</li><li>如果办签证时需要单位法人证明一类的材料，需要<strong>主动</strong>和老师提及</li></ul><h3 id="签证办理"><a class="markdownIt-Anchor" href="#签证办理"></a> 签证办理</h3><p>这个就要看去哪个国家了，我以希腊为例，需要申根签，可以先在官网上填写表格申请，然后按照里面写的去依次准备材料。去使馆办事处。一定要注意时间，选最最最是工作时间的时段过去。我第一去的时候好像是下午3点左右到的，说是刚刚停止办理…</p><p>其它细节事项：</p><ol><li><p>保险可以直接在淘宝上买，搜“申根保险”就可以，看清楚额度是否符合要求；</p></li><li><p>户口页，如果户口在学校的话，直接在info上申请，”集体户口卡借阅”，里面包括“借阅预约”和“首页打印”。预约之后直接去地图里圈出的小房子（保卫处）里拿就好了；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/hukou.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="3"><li><p>银行对账单可以直接在C楼打印；</p></li><li><p>在读证明，在info上预约然后直接与三教打印（貌似改到了六教？，反正C楼应该都是万能的）；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/zaidu.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="5"><li>办理签证最后需要交纳现金。留好发票，这个在报销范围内。</li></ol><h3 id="出行及报销"><a class="markdownIt-Anchor" href="#出行及报销"></a> 出行及报销</h3><ol><li><p>行程中尽量保存好<strong>所有票据</strong>，回来整理<strong>报销</strong>。（我都是先垫付再报销，据说还可以先去学校<strong>借款</strong>）</p><p>各类车票，登机牌，行程单，机票购买记录及发票（让网站寄过来），酒店账单/发票，会议注册费发票等</p></li><li><p>去首都机场的话，清华科技园那里有大巴，车费是30块？这种貌似属于城建交通，也可以报销，不行的话，也有日常杂费可以cover掉。</p></li><li><p>回来后的报销主要是填写一个报销表格，还是在刚刚的出入境申报系统的边栏上的“表格下载”里，选择**”报销表格下载“**，表格如下图，里面也标明了一些报销流程和注意事项；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/baoxiaochat.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><div align="center">  <img src="/2020/05/30/baoxiao/items.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="4"><li><p>其它细节事项</p><ul><li>大额机票需要发票验真，通过官网或发票自带的网站都可以，截图打印</li><li>打印护照的出入境记录页</li><li>提供交易记录截图（微信通知，短信账单，订单等均可）</li><li>在发票上签字需要用<strong>油笔</strong></li></ul><p>我们组的报销可以去对门实验室请教<strong>乔老师</strong>，老师会给予很多帮助，在此表示感谢~</p></li></ol><p>本文凭借对半年前的回忆整理，如有疏漏，欢迎大家评论指正！</p>]]></content>
    
    
    <categories>
      
      <category>办公事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
